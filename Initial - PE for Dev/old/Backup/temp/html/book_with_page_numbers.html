
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>Complete Book with Page Numbers</title>
    <style>

@page {
    size: letter;
    margin: 2cm;
    @bottom-center {
        content: "Page " counter(page);
    }
}

body {
    font-family: "Helvetica", "Arial", sans-serif;
    line-height: 1.6;
    margin: 0;
    padding: 0 0.5cm;
    font-size: 10.5pt;
    counter-reset: chapter;
}

/* Chapter page styling with visible page numbers */
.chapter-container {
    page-break-before: always !important;
    break-before: page !important;
    padding-top: 60px;
    min-height: 100vh;
    position: relative;
    margin-bottom: 40px;
}

.chapter-container:first-of-type {
    page-break-before: avoid !important;
    break-before: avoid !important;
}

/* Chapter header with page number display */
.chapter-header {
    position: relative;
    margin-bottom: 2em;
    padding-bottom: 1em;
    border-bottom: 2px solid #3498db;
}

.chapter-page-number {
    position: absolute;
    top: -40px;
    right: 0;
    background: #3498db;
    color: white;
    padding: 5px 15px;
    border-radius: 20px;
    font-size: 12pt;
    font-weight: bold;
}

h1 {
    font-size: 20pt;
    color: #2c3e50;
    margin-top: 0;
    margin-bottom: 0.5em;
}

h2 {
    font-size: 16pt;
    color: #3498db;
    margin-top: 1em;
    page-break-after: avoid;
}

h3 {
    font-size: 14pt;
    color: #2980b9;
    page-break-after: avoid;
}

/* Table of Contents styling */
.toc {
    page-break-after: always;
    max-width: 800px;
    margin: 0 auto 2em auto;
}

.toc h1 {
    text-align: center;
    color: #2c3e50;
    font-size: 24pt;
    margin-bottom: 2em;
    border-bottom: 2px solid #3498db;
    padding-bottom: 0.5em;
}

.toc-entry {
    display: flex;
    justify-content: space-between;
    align-items: baseline;
    margin: 1em 0;
    border-bottom: 1px dotted #ccc;
    padding-bottom: 0.5em;
}

.chapter-title {
    flex: 1;
    font-weight: bold;
    color: #2c3e50;
}

.dots {
    flex: 0 1 auto;
    border-bottom: 1px dotted #999;
    margin: 0 10px;
    height: 1px;
    align-self: flex-end;
    margin-bottom: 6px;
}

.page-number {
    font-weight: bold;
    color: #3498db;
    min-width: 40px;
    text-align: right;
}

/* Code styling */
pre {
    background-color: #f8f8f8;
    border: 1px solid #ddd;
    border-radius: 3px;
    padding: 1em;
    overflow-x: auto;
    page-break-inside: avoid;
}

code {
    font-family: "Courier New", monospace;
    background-color: #f8f8f8;
    padding: 2px 4px;
    border-radius: 3px;
}

/* Table styling */
table {
    border-collapse: collapse;
    width: 100%;
    margin: 1em 0;
    page-break-inside: avoid;
}

table, th, td {
    border: 1px solid #ddd;
}

th, td {
    padding: 0.5em;
    text-align: left;
}

th {
    background-color: #f2f2f2;
}

blockquote {
    border-left: 4px solid #ccc;
    padding-left: 1em;
    color: #555;
    margin: 1em 0;
}

img {
    max-width: 100%;
    page-break-inside: avoid;
}

    </style>
</head>
<body>
<div class="toc">
<h1>Table of Contents</h1>

<div class="toc-entry">
    <span class="chapter-title">1. Introduction to Prompt Engineering: The Developer's New Skillset</span>
    <span class="dots"></span>
    <span class="page-number">5</span>
</div>
<div class="toc-entry">
    <span class="chapter-title">2. Understanding LLMs: A Developer's Perspective</span>
    <span class="dots"></span>
    <span class="page-number">3</span>
</div>
<div class="toc-entry">
    <span class="chapter-title">3. The Art and Science of Prompt Construction</span>
    <span class="dots"></span>
    <span class="page-number">26</span>
</div>
<div class="toc-entry">
    <span class="chapter-title">4. Essential Prompting Patterns for Developers</span>
    <span class="dots"></span>
    <span class="page-number">40</span>
</div>
<div class="toc-entry">
    <span class="chapter-title">5. Advanced Prompting Techniques for Enhanced Control</span>
    <span class="dots"></span>
    <span class="page-number">6</span>
</div>
<div class="toc-entry">
    <span class="chapter-title">6. Building Effective Developer Tooling for LLM Applications</span>
    <span class="dots"></span>
    <span class="page-number">81</span>
</div>
<div class="toc-entry">
    <span class="chapter-title">7. Hands-on Project 1: Building a Smart Code Assistant</span>
    <span class="dots"></span>
    <span class="page-number">101</span>
</div>
<div class="toc-entry">
    <span class="chapter-title">8. Hands-on Project 2: LLM-Powered ML Model Explainer</span>
    <span class="dots"></span>
    <span class="page-number">128</span>
</div>
<div class="toc-entry">
    <span class="chapter-title">9. Hands-on Project 3: ML Training Debugger and Optimizer</span>
    <span class="dots"></span>
    <span class="page-number">7</span>
</div></div>

<div class="chapter-container">
    <div class="chapter-header">
        <div class="chapter-page-number">Page 5</div>
    </div>
    <h1>Chapter 1: Introduction to Prompt Engineering: The Developer's New Skillset</h1>
<h2>What is Prompt Engineering and Why It Matters for Developers</h2>
<p>Prompt engineering is the practice of crafting effective inputs (prompts) to large language models (LLMs) to obtain desired outputs. For developers, it represents a paradigm shift in how we interact with software tools. Unlike traditional programming, where we write explicit instructions in code that machines follow precisely, prompt engineering involves communicating with AI systems in natural language to achieve computational goals.</p>
<p>The importance of prompt engineering for developers cannot be overstated:</p>
<ol>
<li>
<p><strong>New Interface to Computing Resources</strong>: Prompts are becoming a universal interface to powerful computational capabilities that would otherwise require complex programming or specialized knowledge.</p>
</li>
<li>
<p><strong>Productivity Multiplier</strong>: Well-crafted prompts can dramatically accelerate development tasks like code generation, debugging, documentation, and data transformation.</p>
</li>
<li>
<p><strong>Competitive Advantage</strong>: As LLMs become integrated into development workflows, proficiency in prompt engineering provides a significant edge in the job market.</p>
</li>
<li>
<p><strong>Bridge Between Technical and Non-Technical Domains</strong>: Prompt engineering enables developers to work more effectively with non-technical stakeholders by transforming natural language requirements into working solutions more directly.</p>
</li>
</ol>
<h2>LLMs as Programmable Interfaces</h2>
<p>Large Language Models represent a fundamentally new type of programmable interface with unique characteristics:</p>
<h3>From APIs to LLMs: A Shift in Abstraction</h3>
<p>Traditional APIs require developers to:
- Learn specific endpoints and parameters
- Format data according to strict schemas
- Handle errors through defined error codes
- Work within rigid constraints of what the API can do</p>
<p>In contrast, LLMs offer:
- Natural language interaction
- Flexibility in input formatting
- Graceful handling of ambiguity
- The ability to perform a vastly wider range of tasks through a single interface</p>
<h3>The Programming Model of LLMs</h3>
<p>```python</p>
<h1>Traditional API call</h1>
<p>response = requests.post(
    "https://api.example.com/translate",
    json={"text": "Hello world", "source": "en", "target": "fr"},
    headers={"Authorization": "Bearer " + API_KEY}
)
result = response.json()["translated_text"]</p>
<h1>vs. LLM-based approach</h1>
<p>response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[
        {"role": "user", "content": "Translate 'Hello world' from English to French."}
    ]
)
result = response.choices[0].message.content
```</p>
<p>This fundamental shift means that developers need to think differently about:</p>
<ul>
<li><strong>Input Design</strong>: Crafting prompts that clearly communicate intent</li>
<li><strong>Output Parsing</strong>: Extracting structured information from natural language responses</li>
<li><strong>Error Handling</strong>: Dealing with hallucinations, misunderstandings, and irrelevant outputs</li>
<li><strong>Iteration</strong>: Refining prompts based on observed outputs</li>
</ul>
<h2>Ethical Considerations and Responsible Use for Developers</h2>
<p>As developers integrating LLMs into applications, we carry significant responsibility for the ethical use of these tools:</p>
<h3>Potential Ethical Issues</h3>
<ol>
<li>
<p><strong>Bias Amplification</strong>: LLMs may reproduce or amplify social biases present in their training data, which can manifest in generated code, documentation, or user-facing content.</p>
</li>
<li>
<p><strong>Misinformation Risk</strong>: LLMs can generate plausible but incorrect information, including invalid code, inaccurate explanations, or false claims about technical topics.</p>
</li>
<li>
<p><strong>Intellectual Property Concerns</strong>: Generated code may raise questions about originality, licensing, and attribution.</p>
</li>
<li>
<p><strong>Security Vulnerabilities</strong>: LLMs may inadvertently suggest code with security flaws or sensitive information disclosure.</p>
</li>
</ol>
<h3>Responsible Development Practices</h3>
<p>As developers working with LLMs, we should:</p>
<ul>
<li><strong>Validate Outputs</strong>: Never blindly integrate LLM-generated code without review and testing</li>
<li><strong>Set Clear Boundaries</strong>: Be explicit about what types of requests your LLM-powered application should and should not fulfill</li>
<li><strong>Provide Attribution</strong>: When appropriate, disclose the use of AI-generated content</li>
<li><strong>Design for Transparency</strong>: Make users aware when they're interacting with AI systems</li>
<li><strong>Monitor for Bias</strong>: Regularly audit system outputs for signs of harmful bias</li>
</ul>
<h2>Setting Up Your Development Environment</h2>
<p>To begin working with LLMs as a developer, you'll need to set up a proper environment:</p>
<h3>API Keys and Access</h3>
<p>Most commercial LLM providers require authentication via API keys:</p>
<ol>
<li><strong>OpenAI (GPT-3.5, GPT-4)</strong>:</li>
<li>Create an account at <a href="https://platform.openai.com">platform.openai.com</a></li>
<li>Navigate to API keys section and generate a new key</li>
<li>
<p>Set up billing (required for API access)</p>
</li>
<li>
<p><strong>Google (Gemini)</strong>:</p>
</li>
<li>Get started at <a href="https://ai.google.dev">ai.google.dev</a></li>
<li>Create a project in Google Cloud Console</li>
<li>
<p>Enable the Gemini API and generate credentials</p>
</li>
<li>
<p><strong>Anthropic (Claude)</strong>:</p>
</li>
<li>Request access at <a href="https://anthropic.com/earlyaccess">anthropic.com/earlyaccess</a></li>
<li>Once approved, generate API keys from the console</li>
</ol>
<h3>Essential Python Libraries</h3>
<p>```bash</p>
<h1>Core LLM interaction libraries</h1>
<p>pip install openai google-generativeai anthropic</p>
<h1>Utility libraries for LLM applications</h1>
<p>pip install langchain llama-index</p>
<h1>For embedding and vector operations</h1>
<p>pip install sentence-transformers numpy</p>
<h1>Environment management</h1>
<p>pip install python-dotenv
```</p>
<h3>Environment Configuration</h3>
<p>Best practice is to store API keys securely using environment variables:</p>
<p>```python</p>
<h1>.env file (add to .gitignore)</h1>
<p>OPENAI_API_KEY=sk-your-key-here
ANTHROPIC_API_KEY=sk-ant-your-key-here
GOOGLE_API_KEY=your-google-key-here</p>
<h1>In your Python code</h1>
<p>import os
from dotenv import load_dotenv</p>
<p>load_dotenv()  # Load environment variables from .env file</p>
<p>openai_api_key = os.getenv("OPENAI_API_KEY")
```</p>
<h2>Version Control for Prompts in Development Workflows</h2>
<p>As your prompt engineering practice matures, treating prompts as first-class development artifacts becomes essential:</p>
<h3>Why Version Control Prompts?</h3>
<ol>
<li><strong>Reproducibility</strong>: Ensuring consistent LLM behavior across development, testing, and production</li>
<li><strong>Collaboration</strong>: Enabling team members to review and improve prompts</li>
<li><strong>Quality Assurance</strong>: Tracking changes to identify when and how prompt modifications affect system behavior</li>
<li><strong>Auditability</strong>: Maintaining records of prompt evolution for compliance or troubleshooting</li>
</ol>
<h3>Practical Approaches to Prompt Version Control</h3>
<h4>1. Structured Prompt Storage</h4>
<p>```python</p>
<h1>prompts/code_generation.py</h1>
<p>CODE_GENERATION_PROMPT = """
You are an expert Python developer. Generate a well-documented 
function that {task_description}. Follow these guidelines:
- Use type hints
- Include docstrings in Google format
- Follow PEP 8 style conventions
- Handle edge cases appropriately
"""
```</p>
<h4>2. Prompt Templates with Variables</h4>
<p>```python</p>
<h1>Using a library like Jinja2 for template management</h1>
<p>from jinja2 import Template</p>
<p>code_gen_template = Template("""
You are an expert {{ language }} developer with {{ years_experience }}+ years of experience.
Generate a {{ purpose }} that accomplishes the following:</p>
<p>{{ task_description }}</p>
<p>Requirements:
{% for req in requirements %}
- {{ req }}
{% endfor %}
""")</p>
<h1>Generate specific prompt</h1>
<p>prompt = code_gen_template.render(
    language="Python",
    years_experience=5,
    purpose="data processing function",
    task_description="Parse CSV files and extract specific columns",
    requirements=["Handle malformed data", "Be memory efficient", "Include logging"]
)
```</p>
<h4>3. Prompt Versioning Strategies</h4>
<p>Consider creating a formal system for prompt versioning:</p>
<p>```python</p>
<h1>prompts/registry.py</h1>
<p>PROMPTS = {
    "code_generation": {
        "v1": "Original prompt focused on basic functionality",
        "v2": "Added type hints and docstring requirements",
        "v3": "Expanded to include edge case handling guidance",
        "current": "v3"  # Pointer to current version
    },
    "code_explanation": {
        "v1": "Basic explanation format",
        "v2": "Enhanced with step-by-step breakdown",
        "current": "v2"
    }
}
```</p>
<h2>Conclusion</h2>
<p>Prompt engineering represents a fundamental new skill for developers in the age of AI. By understanding the principles of effective prompt design, considering ethical implications, setting up a proper development environment, and treating prompts as versioned artifacts, you can harness the power of LLMs to transform your development workflow.</p>
<p>In the next chapter, we'll explore the technical underpinnings of LLMs and develop a deeper understanding of their capabilities and limitations from a developer's perspective.</p>
<h2>Exercises</h2>
<ol>
<li>Set up your development environment with access to at least one LLM API.</li>
<li>Write a prompt that generates a function in your preferred programming language and experiment with variations to see how the output changes.</li>
<li>Create a simple versioning scheme for your prompts and implement it in a small project.</li>
<li>Reflect on potential ethical considerations for an LLM-powered application you might want to build.</li>
</ol>
</div>

<div class="chapter-container">
    <div class="chapter-header">
        <div class="chapter-page-number">Page 3</div>
    </div>
    <h1>Chapter 2: Understanding LLMs: A Developer's Perspective</h1>
<h2>Brief Overview of LLM Capabilities and Limitations</h2>
<p>Large Language Models (LLMs) represent a revolutionary class of AI systems with capabilities that can transform development workflows. To effectively leverage these systems, developers must understand both what they excel at and where they fall short.</p>
<h3>Key Capabilities</h3>
<ol>
<li>
<p><strong>Natural Language Understanding</strong>: LLMs can parse and interpret complex instructions, requirements, and queries written in natural language.</p>
</li>
<li>
<p><strong>Code Generation</strong>: Modern LLMs can generate syntactically correct code across dozens of programming languages, from simple functions to complex classes and algorithms.</p>
</li>
<li>
<p><strong>Code Explanation</strong>: LLMs can analyze existing code and provide explanations of its purpose, logic, and implementation details.</p>
</li>
<li>
<p><strong>Translation</strong>: LLMs can translate between natural languages and between programming languages.</p>
</li>
<li>
<p><strong>Data Extraction and Transformation</strong>: LLMs can parse unstructured data and convert it into structured formats.</p>
</li>
<li>
<p><strong>Problem-Solving</strong>: LLMs can apply reasoning to debug code, optimize algorithms, and solve programming challenges.</p>
</li>
</ol>
<h3>Key Limitations</h3>
<ol>
<li><strong>Hallucinations</strong>: LLMs can generate content that appears plausible but is factually incorrect or nonsensical. This includes:</li>
<li>Inventing non-existent functions or libraries</li>
<li>Creating syntactically correct but logically flawed code</li>
<li>
<p>Confidently stating incorrect technical information</p>
</li>
<li>
<p><strong>Context Window Constraints</strong>: LLMs have fixed limits on how much text they can process in a single interaction:</p>
</li>
<li>GPT-4 (as of this writing): ~32,000 tokens (~24,000 words)</li>
<li>Claude 2: ~100,000 tokens</li>
<li>Gemini Pro: ~32,000 tokens</li>
</ol>
<p>These limitations affect your ability to provide context such as large code files or extensive requirements.</p>
<ol>
<li><strong>Knowledge Cutoffs</strong>: LLMs are trained on data up to a specific date, after which they have no knowledge:</li>
<li>GPT-4: Knowledge cutoff in early 2023</li>
<li>Other models have similar cutoffs</li>
</ol>
<p>This affects their knowledge of recent language features, libraries, and best practices.</p>
<ol>
<li>
<p><strong>Inconsistency</strong>: LLMs may provide different solutions to the same prompt when called multiple times, which can introduce unpredictability in development workflows.</p>
</li>
<li>
<p><strong>Security Risks</strong>: LLMs may inadvertently generate code with security vulnerabilities or suggest unsafe practices.</p>
</li>
</ol>
<h2>Popular LLM APIs</h2>
<p>Several providers offer LLM access through well-documented APIs. Here's a comparison of the major options:</p>
<h3>OpenAI (GPT-3.5, GPT-4)</h3>
<p><strong>Strengths:</strong>
- Industry-leading capabilities for code-related tasks
- Comprehensive documentation and community support
- Flexible API with good tooling ecosystem</p>
<p><strong>Considerations:</strong>
- Higher pricing compared to some alternatives
- Rate limits for new accounts
- Data usage policies that may affect privacy-sensitive applications</p>
<p><strong>API Basics:</strong>
```python
import openai</p>
<h1>Configure with your API key</h1>
<p>openai.api_key = "your-api-key"</p>
<p>response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a helpful assistant that generates Python code."},
        {"role": "user", "content": "Write a function that finds prime numbers up to n."}
    ]
)</p>
<p>print(response.choices[0].message.content)
```</p>
<h3>Google (Gemini)</h3>
<p><strong>Strengths:</strong>
- Strong performance on technical and scientific content
- Integration with Google Cloud ecosystem
- Competitive pricing</p>
<p><strong>Considerations:</strong>
- Newer API with evolving documentation
- May require Google Cloud account setup</p>
<p><strong>API Basics:</strong>
```python
import google.generativeai as genai</p>
<h1>Configure with your API key</h1>
<p>genai.configure(api_key="your-api-key")</p>
<p>model = genai.GenerativeModel('gemini-pro')
response = model.generate_content("Write a function that finds prime numbers up to n in Python.")</p>
<p>print(response.text)
```</p>
<h3>Anthropic (Claude)</h3>
<p><strong>Strengths:</strong>
- Very large context window (100K tokens)
- Design focus on safety and reduction of hallucinations
- Clear and thoughtful responses</p>
<p><strong>Considerations:</strong>
- May not match code generation capabilities of GPT-4 for complex tasks
- More limited developer ecosystem</p>
<p><strong>API Basics:</strong>
```python
from anthropic import Anthropic</p>
<h1>Initialize with your API key</h1>
<p>anthropic = Anthropic(api_key="your-api-key")</p>
<p>response = anthropic.completions.create(
    prompt=f"Human: Write a function that finds prime numbers up to n in Python.\n\nAssistant:",
    model="claude-2",
    max_tokens_to_sample=1000
)</p>
<p>print(response.completion)
```</p>
<h3>Open Source and Self-Hosted Options</h3>
<p>For developers with privacy requirements or cost constraints, several open-source LLMs can be self-hosted:</p>
<ul>
<li><strong>Llama 2 / Llama 3</strong>: Meta's powerful open-source models</li>
<li><strong>Mixtral</strong>: Mistral AI's mixture-of-experts model with strong performance</li>
<li><strong>Code Llama</strong>: Specialized for code generation tasks</li>
<li><strong>Falcon</strong>: Technology Innovation Institute's efficient model</li>
</ul>
<p>Self-hosting requires substantial hardware resources but enables complete control over data and usage.</p>
<h2>Basic API Calls and Handling Responses</h2>
<p>Most LLM interactions follow a similar pattern regardless of provider:</p>
<ol>
<li>Construct a prompt (input)</li>
<li>Send to the LLM API</li>
<li>Receive and process the response</li>
<li>Handle errors and edge cases</li>
</ol>
<h3>Core API Interaction Pattern</h3>
<p>```python
import os
import openai
from dotenv import load_dotenv</p>
<h1>Load API key from .env file</h1>
<p>load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")</p>
<p>def get_completion(prompt, model="gpt-3.5-turbo", temperature=0):
    """
    Get a completion from the OpenAI API.</p>
<pre class="codehilite"><code>Args:
    prompt: The prompt to send to the API
    model: The model to use (default: gpt-3.5-turbo)
    temperature: Controls randomness (0=deterministic, 1=creative)

Returns:
    The completion text
&quot;&quot;&quot;
try:
    response = openai.ChatCompletion.create(
        model=model,
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
        temperature=temperature
    )
    return response.choices[0].message.content
except openai.error.OpenAIError as e:
    print(f&quot;OpenAI API error: {e}&quot;)
    return None
except Exception as e:
    print(f&quot;Unexpected error: {e}&quot;)
    return None
</code></pre>

<h1>Example usage</h1>
<p>result = get_completion("Write a Python function to calculate the Fibonacci sequence.")
print(result)
```</p>
<h3>Structured Response Handling</h3>
<p>For development workflows, you often need structured data rather than freeform text. You can request specific JSON formats:</p>
<p>```python
def get_structured_response(prompt, format_instructions, model="gpt-4"):
    """
    Get a structured response from the LLM in JSON format.</p>
<pre class="codehilite"><code>Args:
    prompt: The main prompt/question
    format_instructions: JSON format specification
    model: The model to use

Returns:
    Parsed JSON response or None on error
&quot;&quot;&quot;
import json

full_prompt = f&quot;&quot;&quot;{prompt}

Return your response as a JSON object with the following structure:
{format_instructions}

Ensure your response is valid JSON.
&quot;&quot;&quot;

try:
    response = openai.ChatCompletion.create(
        model=model,
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: full_prompt}],
        temperature=0  # Use deterministic output for consistent JSON
    )

    response_text = response.choices[0].message.content

    # Extract JSON part (in case there's surrounding text)
    try:
        # Try to parse the entire response
        parsed = json.loads(response_text)
        return parsed
    except json.JSONDecodeError:
        # If that fails, try to find and extract a JSON block
        import re
        json_match = re.search(r'```json\n(.*?)\n```', response_text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError:
                print(&quot;JSON parsing failed even after extraction&quot;)
                return None
        else:
            print(&quot;Could not extract JSON from response&quot;)
            return None

except Exception as e:
    print(f&quot;Error: {e}&quot;)
    return None
</code></pre>

<h1>Example usage</h1>
<p>format_spec = """{
    "function_name": "string",
    "parameters": ["list of parameter names"],
    "complexity": "string (O-notation)",
    "code": "string (the Python code)"
}"""</p>
<p>result = get_structured_response(
    "Create a binary search function", 
    format_spec
)</p>
<p>if result:
    print(f"Function: {result['function_name']}")
    print(f"Complexity: {result['complexity']}")
    print("Code:")
    print(result['code'])
```</p>
<h2>Troubleshooting Common API Issues</h2>
<p>When working with LLM APIs, several common issues may arise:</p>
<h3>1. Rate Limiting and Quota Errors</h3>
<p><strong>Symptoms:</strong>
- HTTP 429 "Too Many Requests" errors
- Responses indicating rate limit exceeded</p>
<p><strong>Solutions:</strong>
```python
import time</p>
<p>def resilient_completion(prompt, max_retries=5, backoff_factor=2):
    """Make API calls with exponential backoff for rate limiting"""
    retries = 0
    while retries &lt;= max_retries:
        try:
            return openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}]
            )
        except openai.error.RateLimitError:
            wait_time = backoff_factor ** retries
            print(f"Rate limit hit. Waiting {wait_time} seconds...")
            time.sleep(wait_time)
            retries += 1</p>
<pre class="codehilite"><code>raise Exception(&quot;Max retries exceeded&quot;)
</code></pre>

<p>```</p>
<h3>2. Context Length Exceeded</h3>
<p><strong>Symptoms:</strong>
- Errors about tokens or input length being too long
- Truncated responses</p>
<p><strong>Solutions:</strong>
```python
def chunked_completion(long_text, question, chunk_size=2000, overlap=200):
    """Process long documents by chunking with overlap"""
    chunks = []
    for i in range(0, len(long_text), chunk_size - overlap):
        chunk = long_text[i:i + chunk_size]
        chunks.append(chunk)</p>
<pre class="codehilite"><code>responses = []
for i, chunk in enumerate(chunks):
    prompt = f&quot;Document chunk {i+1}/{len(chunks)}:\n\n{chunk}\n\n{question}&quot;
    response = get_completion(prompt)
    responses.append(response)

# Combine responses
combined_prompt = f&quot;Based on these analyses of different parts of a document:\n\n&quot;
for i, r in enumerate(responses):
    combined_prompt += f&quot;Analysis part {i+1}: {r}\n\n&quot;
combined_prompt += f&quot;Now provide a complete answer to: {question}&quot;

return get_completion(combined_prompt)
</code></pre>

<p>```</p>
<h3>3. Inconsistent Response Formats</h3>
<p><strong>Symptoms:</strong>
- JSON parsing errors
- Missing fields in structured outputs
- Format inconsistencies</p>
<p><strong>Solutions:</strong>
```python
def format_enforcing_prompt(prompt, expected_format):
    """Create a prompt that enforces output format"""
    formatted_prompt = f"""
    {prompt}</p>
<pre class="codehilite"><code>Your response MUST follow this exact format:
{expected_format}

If your response doesn't match this format exactly, it will break the system.
Verify your response carefully before submitting.
&quot;&quot;&quot;
return formatted_prompt
</code></pre>

<p>```</p>
<h3>4. API Connection Issues</h3>
<p><strong>Symptoms:</strong>
- Timeouts
- Connection reset errors
- Intermittent failures</p>
<p><strong>Solutions:</strong>
```python
import backoff</p>
<p>@backoff.on_exception(backoff.expo, 
                     (openai.error.APIConnectionError, openai.error.ServiceUnavailableError),
                     max_tries=5)
def reliable_completion(prompt, <strong>kwargs):
    """Make API calls with automatic retries for connection issues"""
    return openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        </strong>kwargs
    )
```</p>
<h2>Cost Considerations and Token Management</h2>
<p>LLM API costs can rapidly accumulate, especially in production systems. Understanding token usage and implementing cost controls is essential:</p>
<h3>Understanding Tokens</h3>
<p>Tokens are the fundamental unit of text processing in LLMs:
- A token is approximately 4 characters or 0.75 words in English
- Punctuation and special characters also count as tokens
- Code often uses more tokens than natural language due to special characters and indentation</p>
<h3>Token Counting</h3>
<p>```python
import tiktoken</p>
<p>def count_tokens(text, model="gpt-4"):
    """Count the number of tokens in a text string"""
    encoding = tiktoken.encoding_for_model(model)
    tokens = encoding.encode(text)
    return len(tokens)</p>
<p>def estimate_cost(prompt_tokens, completion_tokens, model="gpt-4"):
    """Estimate cost in USD for token usage with common models"""
    costs = {
        "gpt-4": {"prompt": 0.03, "completion": 0.06},  # per 1K tokens
        "gpt-3.5-turbo": {"prompt": 0.0015, "completion": 0.002},  # per 1K tokens
        "gpt-4-32k": {"prompt": 0.06, "completion": 0.12},  # per 1K tokens
    }</p>
<pre class="codehilite"><code>if model not in costs:
    raise ValueError(f&quot;Unknown model: {model}&quot;)

prompt_cost = (prompt_tokens / 1000) * costs[model][&quot;prompt&quot;]
completion_cost = (completion_tokens / 1000) * costs[model][&quot;completion&quot;]

return prompt_cost + completion_cost
</code></pre>

<h1>Example usage</h1>
<p>text = "This is a sample prompt that will be sent to the LLM."
token_count = count_tokens(text)
print(f"Token count: {token_count}")
print(f"Estimated cost: ${estimate_cost(token_count, 150):.6f}")
```</p>
<h3>Cost Optimization Strategies</h3>
<ol>
<li>
<p><strong>Use the Right Model for the Task</strong>
   <code>python
   def choose_optimal_model(task_complexity, input_length, budget_sensitivity):
       """Select the most cost-effective model for a given task"""
       if task_complexity == "low" and budget_sensitivity == "high":
           return "gpt-3.5-turbo"  # Cheaper, good for simpler tasks
       elif input_length &gt; 30000:
           return "gpt-4-32k"  # For very long contexts
       else:
           return "gpt-4"  # For complex reasoning tasks</code></p>
</li>
<li>
<p><strong>Prompt Optimization</strong>
   ```python
   # Before: Verbose prompt
   verbose_prompt = """
   I need you to carefully analyze this code and provide a detailed explanation
   of what it does, how it works, and identify any potential bugs or performance
   issues that might exist in the implementation. Please be thorough in your
   analysis and cover all aspects of the code.</p>
</li>
</ol>
<p>def factorial(n):
       if n == 0:
           return 1
       return n * factorial(n-1)
   """</p>
<p># After: Concise prompt
   concise_prompt = """
   Explain this code and identify any issues:</p>
<p>def factorial(n):
       if n == 0:
           return 1
       return n * factorial(n-1)
   """</p>
<p># Token reduction of ~50%
   ```</p>
<ol>
<li><strong>Response Caching</strong>
   ```python
   import hashlib
   import json
   import os
   import pickle</li>
</ol>
<p>class LLMCache:
       """Simple cache for LLM responses to avoid redundant API calls"""</p>
<pre class="codehilite"><code>   def __init__(self, cache_dir=&quot;.llm_cache&quot;):
       os.makedirs(cache_dir, exist_ok=True)
       self.cache_dir = cache_dir

   def _get_cache_key(self, prompt, model, temperature):
       &quot;&quot;&quot;Generate a unique cache key&quot;&quot;&quot;
       key_data = {
           &quot;prompt&quot;: prompt,
           &quot;model&quot;: model,
           &quot;temperature&quot;: temperature
       }
       key_str = json.dumps(key_data, sort_keys=True)
       return hashlib.md5(key_str.encode()).hexdigest()

   def get(self, prompt, model, temperature):
       &quot;&quot;&quot;Retrieve cached response if available&quot;&quot;&quot;
       cache_key = self._get_cache_key(prompt, model, temperature)
       cache_path = os.path.join(self.cache_dir, cache_key)

       if os.path.exists(cache_path):
           try:
               with open(cache_path, 'rb') as f:
                   return pickle.load(f)
           except Exception:
               return None
       return None

   def set(self, prompt, model, temperature, response):
       &quot;&quot;&quot;Cache a response&quot;&quot;&quot;
       cache_key = self._get_cache_key(prompt, model, temperature)
       cache_path = os.path.join(self.cache_dir, cache_key)

       with open(cache_path, 'wb') as f:
           pickle.dump(response, f)
</code></pre>

<p># Usage
   cache = LLMCache()</p>
<p>def cached_completion(prompt, model="gpt-3.5-turbo", temperature=0):
       """Get completion with caching"""
       # Check cache first
       cached = cache.get(prompt, model, temperature)
       if cached:
           print("Cache hit!")
           return cached</p>
<pre class="codehilite"><code>   # If not in cache, call the API
   print(&quot;Cache miss, calling API...&quot;)
   response = openai.ChatCompletion.create(
       model=model,
       messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
       temperature=temperature
   )

   result = response.choices[0].message.content

   # Cache the result
   cache.set(prompt, model, temperature, result)

   return result
</code></pre>

<p>```</p>
<ol>
<li>
<p><strong>Budget Management</strong>
   ```python
   class LLMBudgetManager:
       """Track and limit LLM API spending"""</p>
<p>def <strong>init</strong>(self, daily_budget=1.0):  # Default $1/day
       self.daily_budget = daily_budget
       self.today_spend = 0
       self.today_date = datetime.date.today()</p>
<p>def track_request(self, prompt_tokens, completion_tokens, model):
       """Track cost of a request and check against budget"""
       # Reset counter if it's a new day
       current_date = datetime.date.today()
       if current_date &gt; self.today_date:
           self.today_date = current_date
           self.today_spend = 0</p>
<pre class="codehilite"><code>   # Calculate cost
   cost = estimate_cost(prompt_tokens, completion_tokens, model)
   self.today_spend += cost

   # Check if over budget
   if self.today_spend &gt; self.daily_budget:
       return False, cost, self.today_spend

   return True, cost, self.today_spend
</code></pre>

</li>
</ol>
<p># Usage
   budget_mgr = LLMBudgetManager(daily_budget=5.0)  # $5/day limit</p>
<p>def budget_aware_completion(prompt, model="gpt-3.5-turbo"):
       """Make API calls with budget awareness"""
       # Count tokens in prompt
       prompt_tokens = count_tokens(prompt, model)</p>
<pre class="codehilite"><code>   # Estimate completion tokens (rough estimate)
   est_completion_tokens = prompt_tokens * 1.5

   # Check budget before making call
   allowed, est_cost, total_spend = budget_mgr.track_request(
       prompt_tokens, est_completion_tokens, model
   )

   if not allowed:
       print(f&quot;Request would exceed daily budget. Today's spend: ${total_spend:.2f}&quot;)
       return None

   # Make the API call
   response = openai.ChatCompletion.create(
       model=model,
       messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}]
   )

   # Update with actual tokens used
   actual_completion_tokens = response.usage.completion_tokens
   budget_mgr.track_request(prompt_tokens, actual_completion_tokens, model)

   return response.choices[0].message.content
</code></pre>

<p>```</p>
<h2>Conclusion</h2>
<p>Understanding LLMs from a developer's perspective involves recognizing both their transformative capabilities and inherent limitations. By mastering the APIs, troubleshooting common issues, and implementing cost-effective practices, you can effectively integrate these powerful tools into your development workflow.</p>
<p>In the next chapter, we'll explore the art and science of prompt construction, focusing on how to craft effective instructions that yield the best possible results from LLMs.</p>
<h2>Exercises</h2>
<ol>
<li>Set up API access to at least two different LLM providers and compare their responses to the same coding challenge.</li>
<li>Create a utility function that handles token counting, cost estimation, and response caching for LLM API calls.</li>
<li>Experiment with different error handling strategies to make your LLM interactions more resilient.</li>
<li>Compare the token usage and cost between verbose and concise versions of the same prompt.</li>
<li>Implement a simple command-line tool that uses an LLM API to help with a specific development task of your choice.</li>
</ol>
</div>

<div class="chapter-container">
    <div class="chapter-header">
        <div class="chapter-page-number">Page 26</div>
    </div>
    <h1>Chapter 3: The Art and Science of Prompt Construction</h1>
<h2>Anatomy of a Prompt: Instructions, Context, Input Data, Output Format</h2>
<p>A well-constructed prompt is the foundation of effective interaction with Large Language Models. Understanding the key components of prompts helps developers craft instructions that yield predictable, high-quality results.</p>
<h3>The Four Core Components</h3>
<ol>
<li><strong>Instructions</strong>: Clear directives that tell the model what to do</li>
<li><strong>Context</strong>: Background information that helps the model understand the task</li>
<li><strong>Input Data</strong>: The specific content the model should work with</li>
<li><strong>Output Format</strong>: Specifications for how the response should be structured</li>
</ol>
<p>Let's examine each component in detail:</p>
<h3>1. Instructions</h3>
<p>Instructions are explicit directives that guide the model's behavior. They should be:
- Clear and specific
- Action-oriented
- Focused on a single task or a well-defined sequence of tasks</p>
<p><strong>Examples:</strong>
```
Poor instruction: "Help me with this code."
Better instruction: "Debug this Python function that should calculate factorial but is producing incorrect results."</p>
<p>Poor instruction: "Write some documentation."
Better instruction: "Generate comprehensive JSDoc comments for this JavaScript utility function."
```</p>
<h3>2. Context</h3>
<p>Context provides the background information that helps the model understand the scope, purpose, and constraints of the task. Effective context includes:
- Relevant background information
- Project-specific considerations
- Technical requirements or constraints
- Target audience information</p>
<p><strong>Examples:</strong>
<code>Limited context: "We need API documentation."
Better context: "We're building a REST API for an e-commerce platform using Express.js. The API will be used by frontend developers who are familiar with React but have limited backend experience. Documentation should be comprehensive yet accessible."</code></p>
<h3>3. Input Data</h3>
<p>Input data is the specific content the model needs to process. This might be:
- Code to analyze or modify
- Text to transform
- Data to structure or extract information from
- Problems to solve</p>
<p><strong>Examples:</strong>
```
Vague input: "Fix my sorting function."
Better input: 
"Fix the following sorting function that should sort an array of objects by their 'priority' property in descending order:</p>
<p>function sortByPriority(items) {
    return items.sort((a, b) =&gt; a.priority - b.priority);
}
"
```</p>
<h3>4. Output Format</h3>
<p>Output format specifies how the response should be structured. Clear formatting instructions help ensure the model's response is immediately usable. This might include:
- Specific structural requirements (JSON, XML, etc.)
- Formatting conventions (markdown, HTML, etc.)
- Response sections or components
- Length constraints</p>
<p><strong>Examples:</strong>
<code>Unspecified format: "Give me information about common sorting algorithms."
Better format specification: "Compare quick sort, merge sort, and bubble sort. Format your response as a markdown table with columns for: Algorithm Name, Average Time Complexity, Space Complexity, Stability, and Best Use Case."</code></p>
<h3>Putting It All Together</h3>
<p>Here's an example of a well-constructed prompt that incorporates all four components:</p>
<p>```</p>
<h1>INSTRUCTIONS</h1>
<p>Review the following Python function that calculates Fibonacci numbers and identify any performance issues or bugs. Then provide an optimized version.</p>
<h1>CONTEXT</h1>
<p>This function will be used in a web application that needs to calculate Fibonacci numbers up to the 50th number. Performance is critical as this will be called frequently.</p>
<h1>INPUT DATA</h1>
<p><code>python
def fibonacci(n):
    if n &lt;= 0:
        return 0
    elif n == 1:
        return 1
    else:
        return fibonacci(n-1) + fibonacci(n-2)</code></p>
<h1>OUTPUT FORMAT</h1>
<p>Provide your response in the following structure:
1. Issues Identified (bullet points)
2. Optimized Solution (code block with comments)
3. Complexity Analysis (time and space)
```</p>
<h2>Core Principles: Clarity, Specificity, Conciseness, Role-playing, Constraints</h2>
<p>Effective prompts adhere to several key principles that enhance the quality and reliability of LLM outputs:</p>
<h3>1. Clarity</h3>
<p>Clarity ensures the model understands exactly what is being asked. Unclear prompts lead to misinterpretations and irrelevant responses.</p>
<p><strong>Key practices:</strong>
- Use simple, direct language
- Avoid ambiguity
- Define technical terms when necessary
- State the objective up front</p>
<p><strong>Example:</strong>
<code>Unclear prompt: "Make this code better."
Clear prompt: "Refactor this Python function to improve its readability and efficiency. Specifically, reduce nested conditionals and optimize the loop structure."</code></p>
<h3>2. Specificity</h3>
<p>Specificity narrows the scope of the model's response, leading to more focused and relevant outputs.</p>
<p><strong>Key practices:</strong>
- Be explicit about requirements
- Specify the exact problem to solve
- Indicate desired approaches or techniques
- Mention constraints or limitations</p>
<p><strong>Example:</strong>
<code>General prompt: "Write a function to process data."
Specific prompt: "Write a Python function that takes a CSV string containing user records (fields: id, name, email, signup_date) and returns a list of dictionaries, with dates converted to datetime objects and emails validated for correct format."</code></p>
<h3>3. Conciseness</h3>
<p>Conciseness focuses on brevity without sacrificing necessary information. While context is important, excessive verbosity can dilute the core request.</p>
<p><strong>Key practices:</strong>
- Remove unnecessary details
- Use direct, active language
- Focus on essential requirements
- Structure information logically</p>
<p><strong>Example:</strong>
```
Verbose prompt: "I'm working on a project where I need to have a function that can take a string and then I need it to count how many times each word appears in the string because I want to analyze text frequency and I'm not sure how to approach this problem efficiently so I need a solution that works well for large texts too."</p>
<p>Concise prompt: "Create an efficient function that counts word frequency in a string, optimized for large texts."
```</p>
<h3>4. Role-playing</h3>
<p>Role-playing instructs the LLM to adopt a specific persona with relevant expertise, leading to more appropriate responses.</p>
<p><strong>Key practices:</strong>
- Define a specific role with relevant expertise
- Specify the role's perspective or approach
- Set the relationship between the role and the audience
- Provide context for why this role is appropriate</p>
<p><strong>Example:</strong>
<code>Basic prompt: "Explain how to structure a microservice architecture."
Role-based prompt: "As an experienced system architect who has designed microservice systems for large-scale e-commerce platforms, explain the key considerations when structuring a microservice architecture for a startup that expects rapid growth."</code></p>
<h3>5. Constraints</h3>
<p>Constraints provide boundaries that help guide the model's response in terms of scope, format, or approach.</p>
<p><strong>Key practices:</strong>
- Set explicit limitations
- Define what should be excluded
- Specify resource constraints
- Indicate priority criteria</p>
<p><strong>Example:</strong>
<code>Unconstrained prompt: "Write a function to validate email addresses."
Constrained prompt: "Write a JavaScript function to validate email addresses with these constraints:
- No external libraries or dependencies
- Must handle international domains
- Maximum 30 lines of code
- Prioritize readability over perfect validation"</code></p>
<h2>Basic Techniques: Zero-shot, Few-shot, Instruction-based</h2>
<p>Different prompting techniques provide varied approaches to guiding LLM behavior, each with specific advantages for different situations:</p>
<h3>1. Zero-shot Prompting</h3>
<p>Zero-shot prompting involves asking the model to perform a task without any examples. This relies on the model's pre-trained knowledge.</p>
<p><strong>Best for:</strong>
- Simple, common tasks
- When examples might bias the output
- Tasks the model is likely familiar with</p>
<p><strong>Example:</strong>
<code>Create a function in Python that validates whether a string is a valid IPv4 address.</code></p>
<p><strong>Advantages:</strong>
- Simple and direct
- Requires minimal prompt engineering
- Tests the model's inherent capabilities</p>
<p><strong>Disadvantages:</strong>
- May produce inconsistent results
- Less control over output format
- May fail for complex or uncommon tasks</p>
<h3>2. Few-shot Prompting</h3>
<p>Few-shot prompting provides one or more examples of the desired input-output pattern before asking the model to perform a similar task.</p>
<p><strong>Best for:</strong>
- Tasks with specific output formats
- Establishing patterns the model should follow
- Guiding the model toward a particular approach</p>
<p><strong>Example:</strong>
```
Convert the following function signatures from JavaScript to TypeScript:</p>
<p>Example 1:
JavaScript: function calculateTotal(prices, discount) { ... }
TypeScript: function calculateTotal(prices: number[], discount: number): number { ... }</p>
<p>Example 2:
JavaScript: function processUser(user, options) { ... }
TypeScript: function processUser(user: UserType, options: ProcessOptions): UserResult { ... }</p>
<p>Now convert this one:
JavaScript: function sortProducts(products, criteria, ascending) { ... }
```</p>
<p><strong>Advantages:</strong>
- Provides clear guidance through examples
- Reduces ambiguity
- Works well for pattern-following tasks</p>
<p><strong>Disadvantages:</strong>
- Takes up more context window space
- May limit creativity
- Can bias the model toward specific approaches</p>
<h3>3. Instruction-based Prompting</h3>
<p>Instruction-based prompting provides detailed, step-by-step directions on how the model should approach a task, often with explicit formatting requirements.</p>
<p><strong>Best for:</strong>
- Complex multi-step tasks
- Tasks requiring specific methodologies
- Outputs needing standardized formatting</p>
<p><strong>Example:</strong>
```
Analyze the security vulnerabilities in the following Node.js code:</p>
<p>```javascript
const express = require('express');
const app = express();</p>
<p>app.get('/user/:id', (req, res) =&gt; {
  const userId = req.params.id;
  const query = <code>SELECT * FROM users WHERE id = ${userId}</code>;
  db.execute(query).then(result =&gt; {
    res.json(result);
  });
});
```</p>
<p>Follow these steps in your analysis:
1. Identify each vulnerability and its type (e.g., SQL injection, XSS)
2. Explain why it's a vulnerability and potential exploit scenarios
3. Rate the severity (Low, Medium, High, Critical)
4. Provide a secure code alternative for each issue found
5. Suggest additional security best practices relevant to this code</p>
<p>Format your response as a structured report with clear headings for each vulnerability.
```</p>
<p><strong>Advantages:</strong>
- Provides detailed guidance
- Ensures comprehensive outputs
- Creates structured, predictable responses</p>
<p><strong>Disadvantages:</strong>
- Can be lengthy and consume tokens
- May over-constrain the model
- Requires careful crafting to avoid confusion</p>
<h3>Hybrid Approaches</h3>
<p>Often, the most effective prompts combine elements from multiple techniques:</p>
<p>```</p>
<h1>INSTRUCTION</h1>
<p>Implement a memory-efficient data structure for a least-recently-used (LRU) cache in Python.</p>
<h1>EXAMPLES</h1>
<p>Here's an example of how a similar data structure (Stack) might be implemented:</p>
<p>```python
class Stack:
    def <strong>init</strong>(self, capacity):
        self.capacity = capacity
        self.items = []</p>
<pre class="codehilite"><code>def push(self, item):
    if len(self.items) &gt;= self.capacity:
        raise OverflowError(&quot;Stack is full&quot;)
    self.items.append(item)

def pop(self):
    if not self.items:
        raise IndexError(&quot;Pop from empty stack&quot;)
    return self.items.pop()
</code></pre>

<p>```</p>
<h1>REQUIREMENTS</h1>
<p>Your LRU cache implementation should:
1. Have O(1) time complexity for lookups, insertions, and deletions
2. Support a configurable maximum size
3. Automatically remove least recently used items when full
4. Include methods: get(key), put(key, value), and remove(key)
5. Include proper docstrings and type hints</p>
<h1>OUTPUT FORMAT</h1>
<p>Provide your solution as a complete Python class with inline comments explaining key design decisions.
```</p>
<h2>Testing and Evaluating Prompt Effectiveness</h2>
<p>The effectiveness of a prompt can be assessed across several dimensions:</p>
<h3>1. Response Relevance</h3>
<p>How well does the output address the actual request?</p>
<p><strong>Evaluation method:</strong>
```python
def evaluate_relevance(prompt, response, criteria):
    """
    Evaluate the relevance of an LLM response against specific criteria</p>
<pre class="codehilite"><code>Args:
    prompt: The original prompt
    response: The LLM's response
    criteria: List of required topics/elements

Returns:
    Score and missing elements
&quot;&quot;&quot;
score = 0
missing = []

for criterion in criteria:
    if criterion.lower() in response.lower():
        score += 1
    else:
        missing.append(criterion)

relevance_score = score / len(criteria)
return relevance_score, missing
</code></pre>

<h1>Example usage</h1>
<p>prompt = "Explain the differences between REST and GraphQL APIs."
response = "REST APIs use standard HTTP methods and typically return fixed data structures. They may require multiple requests to fetch related data. GraphQL allows clients to specify exactly what data they need in a single request, reducing over-fetching."
criteria = ["HTTP methods", "endpoint structure", "data fetching", "versioning", "caching"]</p>
<p>score, missing = evaluate_relevance(prompt, response, criteria)
print(f"Relevance score: {score:.2f}")
print(f"Missing elements: {missing}")
```</p>
<h3>2. Output Format Compliance</h3>
<p>Does the response follow the requested format?</p>
<p><strong>Evaluation method:</strong>
```python
import re
import json</p>
<p>def evaluate_format_compliance(response, format_type):
    """
    Check if response complies with requested format</p>
<pre class="codehilite"><code>Args:
    response: The LLM response text
    format_type: Type of format expected ('json', 'markdown_table', 'bullet_list', etc.)

Returns:
    Boolean indicating compliance and reason if non-compliant
&quot;&quot;&quot;
if format_type == 'json':
    try:
        # Check if there's a code block with JSON
        json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
            json.loads(json_str)  # Test if valid JSON
            return True, &quot;Valid JSON found in code block&quot;

        # Try to find a JSON object even without code block
        json_pattern = re.search(r'(\{[^{]*&quot;.*&quot;[^}]*\})', response, re.DOTALL)
        if json_pattern:
            json_str = json_pattern.group(1)
            json.loads(json_str)  # Test if valid JSON
            return True, &quot;Valid JSON found&quot;

        return False, &quot;No valid JSON found in response&quot;
    except json.JSONDecodeError:
        return False, &quot;JSON parsing failed&quot;

elif format_type == 'markdown_table':
    # Check for markdown table pattern
    has_table = bool(re.search(r'\|[\s\w]+\|[\s\w]+\|', response) and 
                     re.search(r'\|[-:]+\|[-:]+\|', response))
    return has_table, &quot;Markdown table not found&quot; if not has_table else &quot;Markdown table found&quot;

elif format_type == 'bullet_list':
    # Check for bulleted list pattern
    has_bullets = bool(re.findall(r'^\s*[-*]\s+\w+', response, re.MULTILINE))
    return has_bullets, &quot;Bullet list not found&quot; if not has_bullets else &quot;Bullet list found&quot;

return False, &quot;Format type not supported for evaluation&quot;
</code></pre>

<h1>Example usage</h1>
<p>response = """
Here's the data you requested:</p>
<p><code>json
{
    "name": "API Comparison",
    "technologies": ["REST", "GraphQL", "gRPC"],
    "metrics": {
        "performance": [85, 92, 97],
        "learning_curve": [75, 68, 45]
    }
}</code>
"""</p>
<p>is_compliant, reason = evaluate_format_compliance(response, 'json')
print(f"Format compliant: {is_compliant}, Reason: {reason}")
```</p>
<h3>3. Factual Accuracy</h3>
<p>Does the response contain correct information?</p>
<p><strong>Evaluation method:</strong>
```python
def evaluate_factual_accuracy(response, fact_checks):
    """
    Check response for factual accuracy against known truth</p>
<pre class="codehilite"><code>Args:
    response: The LLM response
    fact_checks: Dictionary of facts to check {fact_description: truth_value}

Returns:
    Accuracy score and incorrect facts
&quot;&quot;&quot;
correct = 0
incorrect = []

for fact, truth in fact_checks.items():
    # Simple presence check - could be enhanced with NLP techniques
    fact_present = fact.lower() in response.lower()

    if fact_present == truth:
        correct += 1
    else:
        incorrect.append(fact)

accuracy = correct / len(fact_checks) if fact_checks else 0
return accuracy, incorrect
</code></pre>

<h1>Example usage</h1>
<p>response = "JavaScript is a dynamically typed language that supports first-class functions and prototypal inheritance. It was created in 1995 by Brendan Eich."</p>
<p>facts = {
    "JavaScript is dynamically typed": True,
    "JavaScript uses classical inheritance": False,
    "JavaScript was created by Brendan Eich": True,
    "JavaScript was created in 1990": False,
    "JavaScript supports first-class functions": True
}</p>
<p>accuracy, incorrect = evaluate_factual_accuracy(response, facts)
print(f"Factual accuracy: {accuracy:.2f}")
print(f"Incorrect facts: {incorrect}")
```</p>
<h3>4. A/B Testing Prompts</h3>
<p>Systematically compare different prompt variations to find the most effective approach.</p>
<p><strong>Evaluation method:</strong>
```python
import random</p>
<p>class PromptABTester:
    """A simple tool for A/B testing different prompt formulations"""</p>
<pre class="codehilite"><code>def __init__(self, llm_function, evaluation_function):
    &quot;&quot;&quot;
    Args:
        llm_function: Function that sends prompt to LLM and returns response
        evaluation_function: Function that scores response quality (0-1)
    &quot;&quot;&quot;
    self.llm_function = llm_function
    self.evaluation_function = evaluation_function
    self.results = {}

def test_prompt_variations(self, prompt_variations, trials=3):
    &quot;&quot;&quot;
    Test multiple prompt variations with repeated trials

    Args:
        prompt_variations: Dict of {variation_name: prompt_text}
        trials: Number of times to test each variation

    Returns:
        DataFrame with results
    &quot;&quot;&quot;
    import pandas as pd

    all_results = []

    for name, prompt in prompt_variations.items():
        self.results[name] = []

        for i in range(trials):
            response = self.llm_function(prompt)
            score = self.evaluation_function(response)

            self.results[name].append(score)
            all_results.append({
                'variation': name,
                'trial': i+1,
                'score': score,
                'prompt': prompt,
                'response': response
            })

    results_df = pd.DataFrame(all_results)

    # Calculate aggregate statistics
    summary = results_df.groupby('variation')['score'].agg(['mean', 'std', 'min', 'max'])

    return results_df, summary

def get_best_prompt(self):
    &quot;&quot;&quot;Return the prompt variation with highest average score&quot;&quot;&quot;
    avg_scores = {name: sum(scores)/len(scores) for name, scores in self.results.items()}
    best_variation = max(avg_scores, key=avg_scores.get)
    return best_variation, avg_scores[best_variation]
</code></pre>

<h1>Example usage</h1>
<p>def mock_llm(prompt):
    """Mock LLM function for demonstration"""
    # This would be replaced by actual LLM API call
    responses = [
        "This is a detailed response that covers all requirements.",
        "This is a partial response that misses some key points.",
        "This response is thorough and well-structured."
    ]
    return random.choice(responses)</p>
<p>def mock_evaluator(response):
    """Mock evaluation function for demonstration"""
    # This would be replaced by actual evaluation logic
    if "detailed" in response or "thorough" in response:
        return 0.9
    return 0.6</p>
<h1>Create test variations</h1>
<p>prompt_variations = {
    "basic": "Explain how virtual memory works in operating systems.",
    "detailed": "Explain how virtual memory works in operating systems. Include paging, segmentation, and address translation.",
    "role_based": "As an OS kernel engineer, explain how virtual memory works to a junior developer."
}</p>
<h1>Run the test</h1>
<p>tester = PromptABTester(mock_llm, mock_evaluator)
results, summary = tester.test_prompt_variations(prompt_variations, trials=5)
best_prompt, best_score = tester.get_best_prompt()</p>
<p>print(f"Best prompt variation: {best_prompt} (Score: {best_score:.2f})")
print("\nSummary statistics:")
print(summary)
```</p>
<h2>Conclusion</h2>
<p>Prompt construction is both an art and a science. By understanding the anatomy of effective prompts, applying core principles, and leveraging appropriate prompting techniques, developers can achieve consistently high-quality results from LLMs. Regular testing and evaluation help refine prompts over time, leading to increasingly reliable and useful outputs.</p>
<p>In the next chapter, we'll explore essential prompting patterns specifically tailored for common developer tasks, from code generation to documentation and debugging.</p>
<h2>Exercises</h2>
<ol>
<li>
<p>Take a simple prompt you've used before and improve it by explicitly incorporating the four components: instructions, context, input data, and output format.</p>
</li>
<li>
<p>Compare zero-shot, few-shot, and instruction-based approaches on the same task (e.g., generating a function to validate email addresses). Which performed best and why?</p>
</li>
<li>
<p>Create an A/B testing framework to evaluate prompt effectiveness for a specific use case (e.g., code explanation, bug finding).</p>
</li>
<li>
<p>Design a prompt that demonstrates all five core principles: clarity, specificity, conciseness, role-playing, and constraints.</p>
</li>
<li>
<p>Create a systematic evaluation method for one type of LLM task you commonly use (e.g., code generation, text summarization) with at least three different evaluation criteria.</p>
</li>
</ol>
</div>

<div class="chapter-container">
    <div class="chapter-header">
        <div class="chapter-page-number">Page 40</div>
    </div>
    <h1>Chapter 4: Essential Prompting Patterns for Developers</h1>
<p>In this chapter, we'll explore practical prompting patterns that developers can immediately apply to common tasks. These patterns serve as templates that you can adapt to your specific needs, saving time and improving consistency in your interactions with LLMs.</p>
<h2>Code Generation: Generating Functions, Classes, Scripts</h2>
<h3>Function Generation Pattern</h3>
<p>Use this pattern to generate well-structured, documented functions for specific programming tasks.</p>
<p>```
Create a [language] function named [function_name] that [purpose].</p>
<p>Input parameters:
- [param1_name] ([type]): [description]
- [param2_name] ([type]): [description]</p>
<p>Requirements:
- [requirement1]
- [requirement2]</p>
<p>Include appropriate error handling, type hints, and documentation.
```</p>
<p><strong>Example - Python Data Processing Function:</strong></p>
<p>```
Create a Python function named 'parse_log_entries' that extracts structured data from application log files.</p>
<p>Input parameters:
- log_path (str): Path to the log file
- error_levels (list, optional): Specific error levels to filter for (e.g., ["ERROR", "WARNING"])
- start_date (datetime, optional): Only process entries after this date</p>
<p>Requirements:
- Return a list of dictionaries with keys: timestamp, level, message, service
- Handle malformed log lines gracefully
- Support both plain text and gzip compressed logs
- Add type hints and comprehensive docstrings</p>
<p>Include appropriate error handling and performance optimization for large files.
```</p>
<h3>Class Generation Pattern</h3>
<p>Use this pattern to generate complete classes with properties, methods, and appropriate design patterns.</p>
<p>```
Design a [language] class named [ClassName] that [purpose].</p>
<p>Properties:
- [property1_name] ([type]): [description]
- [property2_name] ([type]): [description]</p>
<p>Methods:
- <a href="[params]">method1_name</a>: [description]
- <a href="[params]">method2_name</a>: [description]</p>
<p>Requirements:
- [requirement1]
- [requirement2]</p>
<p>Additional context:
[any relevant information about usage, environment, etc.]
```</p>
<p><strong>Example - TypeScript Component Class:</strong></p>
<p>```
Design a TypeScript class named 'DataTable' that implements a reusable, sortable table component for a React application.</p>
<p>Properties:
- data (Array<Record\<string, any>>): Source data to display
- columns (ColumnConfig[]): Column configuration including headers, field mappings and formatting
- sortState (SortConfig): Current sort column and direction
- pageSize (number): Number of records per page
- currentPage (number): Current page index</p>
<p>Methods:
- render(): JSX.Element - Render the table with current configuration
- sort(columnId: string, direction: 'asc'|'desc'): void - Sort table data
- nextPage(): void - Navigate to next page
- previousPage(): void - Navigate to previous page
- onRowClick(handler: (row: Record<string, any>) =&gt; void): void - Set row click handler</p>
<p>Requirements:
- Implement pagination with customizable page size
- Support client-side sorting for all common data types
- Allow custom cell renderers for complex data
- Follow React best practices for performance optimization
- Include proper TypeScript interfaces and types</p>
<p>Additional context:
The component will be used in a dashboard application that displays various data views to users with different permission levels.
```</p>
<h3>Script Generation Pattern</h3>
<p>Use this pattern to generate complete scripts for automation tasks, data processing, or system operations.</p>
<p>```
Create a [language] script that [purpose].</p>
<p>Inputs:
- [input1]: [description]
- [input2]: [description]</p>
<p>Expected output:
[description of what the script should produce]</p>
<p>Requirements:
- [requirement1]
- [requirement2]</p>
<p>Environment context:
[relevant environment information]
```</p>
<p><strong>Example - Python ETL Script:</strong></p>
<p>```
Create a Python script that performs ETL (Extract, Transform, Load) operations on website analytics data.</p>
<p>Inputs:
- analytics.csv: Daily website traffic data (columns: date, page_path, visitors, bounce_rate, avg_time_on_page)
- product_mapping.json: JSON file mapping page paths to product categories</p>
<p>Expected output:
- A PostgreSQL database table with aggregated metrics by product category and date
- A summary report in CSV format showing week-over-week changes</p>
<p>Requirements:
- Handle missing or malformed data gracefully
- Implement logging for the ETL process
- Support incremental loads (only process new data)
- Include command-line arguments for configuration
- Optimize for memory efficiency with large input files</p>
<p>Environment context:
- Python 3.9+
- Will run as a scheduled task in Linux environment
- PostgreSQL 13 database
```</p>
<h2>Code Explanation &amp; Documentation</h2>
<h3>Code Comprehension Pattern</h3>
<p>Use this pattern when you need to understand unfamiliar or complex code.</p>
<p>```
Explain the following [language] code, focusing on:
1. Overall purpose
2. Key algorithms or data structures used
3. Control flow and execution path
4. Any potential edge cases or bugs
5. Performance characteristics</p>
<p><code>[code block]</code>
```</p>
<p><strong>Example:</strong></p>
<p>```
Explain the following JavaScript code, focusing on:
1. Overall purpose
2. Key algorithms or data structures used
3. Control flow and execution path
4. Any potential edge cases or bugs
5. Performance characteristics</p>
<p>```javascript
function findDuplicateTransactions(transactions) {
  const sorted = [...transactions].sort((a, b) =&gt; {
    return new Date(a.time) - new Date(b.time);
  });</p>
<p>const potential = {};
  sorted.forEach(t =&gt; {
    const key = <code>${t.sourceAccount}_${t.targetAccount}_${t.category}_${t.amount}</code>;
    if (!potential[key]) potential[key] = [];
    potential[key].push(t);
  });</p>
<p>const duplicates = [];
  Object.values(potential).forEach(group =&gt; {
    if (group.length &lt;= 1) return;</p>
<pre class="codehilite"><code>const result = [];
for (let i = 0; i &lt; group.length; i++) {
  if (result.length === 0) {
    result.push(group[i]);
    continue;
  }

  const last = result[result.length - 1];
  const current = group[i];
  const timeDiff = Math.abs(new Date(current.time) - new Date(last.time));
  const minutesDiff = timeDiff / (1000 * 60);

  if (minutesDiff &lt;= 5) {
    result.push(current);
  } else if (result.length &gt; 1) {
    duplicates.push([...result]);
    result.length = 0;
    result.push(current);
  } else {
    result.length = 0;
    result.push(current);
  }
}

if (result.length &gt; 1) {
  duplicates.push(result);
}
</code></pre>

<p>});</p>
<p>return duplicates;
}
<code></code></p>
<h3>Documentation Generation Pattern</h3>
<p>Use this pattern to generate comprehensive documentation for existing code.</p>
<p>```
Generate [format] documentation for the following [language] [code_type].
Include:
- Purpose and functionality
- Parameter descriptions
- Return value details
- Usage examples
- Edge cases and exceptions</p>
<p><code>[code block]</code>
```</p>
<p><strong>Example:</strong></p>
<p>```
Generate JSDoc documentation for the following JavaScript function.
Include:
- Purpose and functionality
- Parameter descriptions
- Return value details
- Usage examples
- Edge cases and exceptions</p>
<p>```javascript
function throttle(fn, delay) {
  let lastCall = 0;
  let timeoutId = null;</p>
<p>return function(...args) {
    const now = Date.now();
    const remaining = delay - (now - lastCall);</p>
<pre class="codehilite"><code>if (remaining &lt;= 0) {
  if (timeoutId) {
    clearTimeout(timeoutId);
    timeoutId = null;
  }

  lastCall = now;
  return fn.apply(this, args);
} else if (!timeoutId) {
  timeoutId = setTimeout(() =&gt; {
    lastCall = Date.now();
    timeoutId = null;
    fn.apply(this, args);
  }, remaining);
}
</code></pre>

<p>};
}
<code></code></p>
<h2>Debugging &amp; Error Resolution</h2>
<h3>Error Diagnosis Pattern</h3>
<p>Use this pattern when you encounter error messages and need help troubleshooting.</p>
<p>```
I'm getting the following error when running my [language] code:</p>
<p><code>[error message]</code></p>
<p>Here's the relevant code:</p>
<p><code>[code block]</code></p>
<p>Please:
1. Explain what's causing this error
2. Suggest a solution with code examples
3. Explain how to prevent similar errors in the future
```</p>
<p><strong>Example:</strong></p>
<p>```
I'm getting the following error when running my Python code:</p>
<p><code>TypeError: unsupported operand type(s) for +: 'int' and 'str'</code></p>
<p>Here's the relevant code:</p>
<p>```python
def calculate_total_cost(items):
    total = 0
    for item in items:
        total = total + item['price']
    return total</p>
<p>inventory = [
    {'id': 1, 'name': 'Widget A', 'price': 10},
    {'id': 2, 'name': 'Widget B', 'price': '15'},
    {'id': 3, 'name': 'Widget C', 'price': 20},
]</p>
<p>print(calculate_total_cost(inventory))
```</p>
<p>Please:
1. Explain what's causing this error
2. Suggest a solution with code examples
3. Explain how to prevent similar errors in the future
```</p>
<h3>Code Review Pattern</h3>
<p>Use this pattern to identify potential issues before they cause runtime errors.</p>
<p>```
Review the following [language] code for:
1. Bugs or logic errors
2. Performance issues
3. Security vulnerabilities 
4. Style or best practice violations
5. Edge cases that aren't handled</p>
<p><code>[code block]</code></p>
<p>Suggest improvements with code examples.
```</p>
<p><strong>Example:</strong></p>
<p>```
Review the following Python code for:
1. Bugs or logic errors
2. Performance issues
3. Security vulnerabilities 
4. Style or best practice violations
5. Edge cases that aren't handled</p>
<p>```python
def authenticate_user():
    username = request.form.get('username')
    password = request.form.get('password')</p>
<pre class="codehilite"><code>query = &quot;SELECT * FROM users WHERE username='{}' AND password='{}'&quot;.format(
    username, password
)

result = db.execute(query)
user = result.fetchone()

if user:
    session['user_id'] = user[0]
    session['is_admin'] = user[3]
    return redirect('/dashboard')
else:
    attempts = session.get('login_attempts', 0) + 1
    session['login_attempts'] = attempts

    if attempts &gt; 3:
        time.sleep(3)  # Add delay after multiple failures

    return render_template('login.html', error=&quot;Invalid credentials&quot;)
</code></pre>

<p>```</p>
<p>Suggest improvements with code examples.
```</p>
<h2>Text Transformation</h2>
<h3>Text Summarization Pattern</h3>
<p>Use this pattern to condense documentation, comments, or other text while preserving key information.</p>
<p>```
Summarize the following [text_type] in [target_length] while preserving the key technical details:</p>
<p>[text to summarize]
```</p>
<p><strong>Example:</strong></p>
<p>```
Summarize the following API documentation in 3-4 paragraphs while preserving the key technical details:</p>
<p>The User Authentication API provides endpoints for registering, authenticating, and managing user accounts. It supports both traditional username/password authentication as well as OAuth2 integrations with popular providers.</p>
<p>The main endpoint /api/v1/auth/login accepts POST requests with either username/password combinations or OAuth tokens. For username/password authentication, the request body must include "username" and "password" fields in JSON format. For OAuth, include "provider" and "access_token" fields. The API returns a JWT token with a configurable expiration (default: 24 hours).</p>
<p>Token validation can be performed against the /api/v1/auth/validate endpoint, which accepts GET requests with the Authorization header set to "Bearer {token}". This endpoint returns user details if the token is valid or a 401 status if invalid or expired.</p>
<p>Account registration is handled through the /api/v1/auth/register endpoint, accepting POST requests with required fields "username", "email", and "password". Additional optional fields include "full_name", "phone", and "preferences" (as a JSON object). Password requirements can be configured but default to minimum 8 characters with at least one number, one uppercase letter, and one special character.</p>
<p>Password reset functionality is provided via two endpoints: /api/v1/auth/request-reset (POST with "email" field) sends a time-limited reset token to the user's email, and /api/v1/auth/reset-password (POST with "token" and "new_password" fields) performs the actual password change.</p>
<p>Rate limiting is applied to all authentication endpoints, with default limits of 5 attempts per minute for login and 3 attempts per hour for password reset requests from the same IP address. These limits are configurable through the server's environment variables RATE_LIMIT_LOGIN and RATE_LIMIT_RESET.</p>
<p>All authentication attempts, successful or failed, are logged with timestamp, IP address, and user agent information. These logs can be accessed through the admin dashboard or directly from the database's auth_logs table.
```</p>
<h3>Text Translation Pattern</h3>
<p>Use this pattern to translate between technical terminology, languages, or convert between technologies.</p>
<p>```
Translate the following [source_format] to [target_format], maintaining all functionality:</p>
<p><code>[source code or text]</code>
```</p>
<p><strong>Example:</strong></p>
<p>```
Translate the following JavaScript React component to TypeScript, maintaining all functionality:</p>
<p>```javascript
import React, { useState, useEffect } from 'react';
import axios from 'axios';</p>
<p>function UserDashboard({ userId }) {
  const [userData, setUserData] = useState(null);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState(null);</p>
<p>useEffect(() =&gt; {
    async function fetchUserData() {
      try {
        const response = await axios.get(<code>/api/users/${userId}</code>);
        setUserData(response.data);
        setLoading(false);
      } catch (err) {
        setError('Failed to load user data');
        setLoading(false);
      }
    }</p>
<pre class="codehilite"><code>fetchUserData();
</code></pre>

<p>}, [userId]);</p>
<p>function handleRefresh() {
    setLoading(true);
    fetchUserData();
  }</p>
<p>if (loading) return <div className="loading">Loading...</div>;
  if (error) return <div className="error">{error}</div>;</p>
<p>return (
    <div className="dashboard">
      <h1>Welcome, {userData.name}</h1>
      <div className="stats">
        <div className="stat-item">
          <span className="stat-label">Projects</span>
          <span className="stat-value">{userData.projects.length}</span>
        </div>
        <div className="stat-item">
          <span className="stat-label">Tasks</span>
          <span className="stat-value">{userData.tasks.filter(t =&gt; !t.completed).length}</span>
        </div>
      </div>
      <button onClick={handleRefresh}>Refresh Data</button>
    </div>
  );
}</p>
<p>export default UserDashboard;
<code></code></p>
<h3>Format Conversion Pattern</h3>
<p>Use this pattern to transform between data formats (JSON, XML, CSV, etc.) or to structure text in specific formats.</p>
<p>```
Convert the following [source_format] to [target_format]:</p>
<p><code>[source data]</code></p>
<p>Requirements:
- [requirement1]
- [requirement2]
```</p>
<p><strong>Example:</strong></p>
<p>```
Convert the following JSON data to a CSV format:</p>
<p><code>json
{
  "employees": [
    {
      "id": 101,
      "name": "John Smith",
      "department": "Engineering",
      "title": "Senior Developer",
      "skills": ["Python", "React", "MongoDB"],
      "projects": [
        {"id": "P-100", "name": "API Gateway"},
        {"id": "P-201", "name": "Data Pipeline"}
      ]
    },
    {
      "id": 102,
      "name": "Alice Johnson",
      "department": "Product",
      "title": "Product Manager",
      "skills": ["Agile", "Roadmapping", "User Research"],
      "projects": [
        {"id": "P-100", "name": "API Gateway"},
        {"id": "P-105", "name": "Mobile App"}
      ]
    }
  ]
}</code></p>
<p>Requirements:
- Include headers in the first row
- Format skills as comma-separated values within a single field
- Include project IDs and names in separate columns
- Create one row per employee-project combination
```</p>
<h2>Data Extraction: Pulling Structured Data from Unstructured Text</h2>
<h3>Entity Extraction Pattern</h3>
<p>Use this pattern to identify and extract specific entities from text.</p>
<p>```
Extract the following entities from this [text_type]:
- [entity_type1]
- [entity_type2]
- [entity_type3]</p>
<p>Return the results as [format].</p>
<p>[text]
```</p>
<p><strong>Example:</strong></p>
<p>```
Extract the following entities from this error log:
- Error codes
- Timestamps
- File paths
- IP addresses</p>
<p>Return the results as a JSON object with arrays of each entity type.</p>
<p>[2023-05-15T09:23:45.123Z] ERROR [server.js:125] Failed to authenticate user from 192.168.1.105 - Error code AUTH-401
[2023-05-15T09:25:12.456Z] WARN [auth/middleware.js:85] Rate limit exceeded for IP 192.168.1.105
[2023-05-15T09:30:22.789Z] ERROR [database/connection.js:209] Failed to connect to database after 5 retries - Error code DB-503
[2023-05-15T09:31:01.234Z] INFO [server.js:50] Server restarting with updated configuration from /etc/app/config.json
[2023-05-15T09:32:45.678Z] ERROR [api/users.js:75] Invalid request parameters from 192.168.1.210 - Error code API-400
```</p>
<h3>Tabular Data Extraction Pattern</h3>
<p>Use this pattern to convert text descriptions or semi-structured information into structured tables.</p>
<p>```
Extract the following information from the text into a [table_format] with columns [column1, column2, ...]:</p>
<p>[text]
```</p>
<p><strong>Example:</strong></p>
<p>```
Extract the following information from the text into a markdown table with columns Method, Endpoint, Required Parameters, and Description:</p>
<p>Our REST API provides the following endpoints for managing user profiles:</p>
<p>GET /api/users - Returns a list of all users. Supports optional query parameters 'limit' (default 20) and 'offset' (default 0) for pagination.</p>
<p>GET /api/users/{id} - Returns details for a specific user. The 'id' parameter is required and must be a valid user identifier.</p>
<p>POST /api/users - Creates a new user. Requires 'email', 'username', and 'password' in the request body. Optional fields include 'fullName', 'role', and 'preferences'.</p>
<p>PUT /api/users/{id} - Updates an existing user. The 'id' parameter is required. At least one update field must be provided in the request body.</p>
<p>DELETE /api/users/{id} - Removes a user. The 'id' parameter is required. This operation cannot be undone.</p>
<p>PATCH /api/users/{id}/status - Updates only the user's status. Requires 'id' parameter and 'status' field in the request body with value 'active' or 'inactive'.
```</p>
<h3>Parsing Unstructured Data Pattern</h3>
<p>Use this pattern to extract structured information from free-form text like emails, documents, or requirements.</p>
<p>```
Parse the following [text_type] and extract:
- [information1]
- [information2]
- [information3]</p>
<p>Format the output as [format]:</p>
<p>[text]
```</p>
<p><strong>Example:</strong></p>
<p>```
Parse the following customer support email and extract:
- Product name and version
- Issue category
- Steps to reproduce
- Customer contact information
- Priority level (if mentioned)</p>
<p>Format the output as JSON:</p>
<p>Subject: Urgent: Dashboard crashes when filtering by date range in Analytics Pro 4.2</p>
<p>Hello Support Team,</p>
<p>I'm experiencing a critical issue with Analytics Pro version 4.2.1 on Windows 10. 
Whenever I try to filter dashboard data using a date range, the application completely
crashes and I have to restart it.</p>
<p>Steps to reproduce:
1. Open the main dashboard
2. Click on "Date Filter" in the top right
3. Select "Custom Range" from the dropdown
4. Choose any start and end dates
5. Click "Apply Filter"</p>
<p>After clicking "Apply Filter," the screen freezes for about 5 seconds, then the
application crashes with no error message.</p>
<p>This is blocking our monthly reporting process which we need to complete by end of day
tomorrow, so I'd consider this a high priority issue.</p>
<p>Please contact me at john.smith@example.com or 555-123-4567 if you need more information.</p>
<p>Thanks,
John Smith
Senior Data Analyst
Acme Corporation
```</p>
<h2>Examples in Multiple Programming Languages</h2>
<h3>Python - Function Generation</h3>
<p>```
Create a Python function named 'parse_log_file' that extracts and analyzes error messages from a log file.</p>
<p>Input parameters:
- file_path (str): Path to the log file
- error_types (list, optional): List of error types to focus on (e.g., ['ERROR', 'CRITICAL'])
- start_date (str, optional): Only parse logs after this date (format: 'YYYY-MM-DD')</p>
<p>Requirements:
- Return a dictionary with error types as keys and lists of error messages as values
- Include timestamps and context information with each error
- Handle compressed (.gz) log files automatically
- Use generators for memory efficiency with large files
- Include proper type hints and docstrings
```</p>
<h3>JavaScript - Debugging</h3>
<p>```
I'm getting the following error when running my JavaScript React application:</p>
<p><code>TypeError: Cannot read properties of undefined (reading 'map')</code></p>
<p>Here's the relevant code:</p>
<p>```javascript
function ProductList({ categoryId }) {
  const [products, setProducts] = useState(null);
  const [loading, setLoading] = useState(true);</p>
<p>useEffect(() =&gt; {
    async function fetchProducts() {
      try {
        const response = await api.getProductsByCategory(categoryId);
        setProducts(response.data);
      } catch (err) {
        console.error('Failed to fetch products:', err);
      } finally {
        setLoading(false);
      }
    }</p>
<pre class="codehilite"><code>fetchProducts();
</code></pre>

<p>}, [categoryId]);</p>
<p>if (loading) return <LoadingSpinner />;</p>
<p>return (
    <div className="product-grid">
      {products.map(product =&gt; (
        <ProductCard key={product.id} product={product} />
      ))}
    </div>
  );
}
```</p>
<p>Please:
1. Explain what's causing this error
2. Suggest a solution with code examples
3. Explain how to prevent similar errors in the future
```</p>
<h3>Java - Code Explanation</h3>
<p>```
Explain the following Java code, focusing on:
1. Overall purpose
2. Key algorithms or design patterns used
3. Thread safety considerations
4. Potential performance bottlenecks</p>
<p>```java
public class ConnectionPool {
    private static ConnectionPool instance;
    private final List<Connection> availableConnections = new ArrayList&lt;&gt;();
    private final List<Connection> usedConnections = new ArrayList&lt;&gt;();
    private final int MAX_CONNECTIONS;</p>
<pre class="codehilite"><code>private ConnectionPool(String url, String user, String password, int maxConnections) {
    this.MAX_CONNECTIONS = maxConnections;
    try {
        for (int i = 0; i &lt; maxConnections; i++) {
            availableConnections.add(
                DriverManager.getConnection(url, user, password)
            );
        }
    } catch (SQLException e) {
        logger.severe(&quot;Failed to initialize connection pool: &quot; + e.getMessage());
    }
}

public static synchronized ConnectionPool getInstance(String url, String user, 
                                                   String password, int maxConnections) {
    if (instance == null) {
        instance = new ConnectionPool(url, user, password, maxConnections);
    }
    return instance;
}

public synchronized Connection getConnection() throws SQLException {
    if (availableConnections.isEmpty()) {
        if (usedConnections.size() &lt; MAX_CONNECTIONS) {
            String url = usedConnections.get(0).getMetaData().getURL();
            availableConnections.add(DriverManager.getConnection(url, &quot;&quot;, &quot;&quot;));
        } else {
            throw new SQLException(&quot;Connection limit reached, no available connections!&quot;);
        }
    }

    Connection connection = availableConnections.remove(availableConnections.size() - 1);
    usedConnections.add(connection);
    return connection;
}

public synchronized void releaseConnection(Connection connection) {
    usedConnections.remove(connection);
    availableConnections.add(connection);
}
</code></pre>

<p>}
<code></code></p>
<h3>C# - Class Generation</h3>
<p>```
Design a C# class named 'FileProcessor' that handles batch processing of documents in different formats.</p>
<p>Properties:
- ProcessedCount (int): Number of files successfully processed
- FailedCount (int): Number of files that failed processing
- ProcessingOptions (ProcessingOptions): Configuration settings for processing
- OnProgressUpdate (event): Event that fires when progress changes</p>
<p>Methods:
- ProcessDirectory(string path, bool recursive): Process all compatible files in a directory
- ProcessFile(string filePath): Process a single file
- GetSupportedFormats(): List<string> - Return list of supported file formats
- CancelProcessing(): Cancel any ongoing processing operation
- GenerateReport(): ProcessingSummary - Create summary of processing operations</p>
<p>Requirements:
- Support PDF, DOCX, and TXT files with different processing strategies
- Implement proper error handling and logging
- Use async/await for file operations
- Follow C# naming conventions and best practices
- Make the class extensible for adding new file format handlers
```</p>
<h2>Conclusion</h2>
<p>These essential prompting patterns form the foundation of effective LLM use in development workflows. By adapting these patterns to your specific needs, you can quickly generate reliable, high-quality outputs for a wide range of development tasks across different programming languages and environments.</p>
<p>In the next chapter, we'll explore advanced prompting techniques that provide even greater control over LLM responses, including Chain-of-Thought reasoning, self-correction strategies, and parameter tuning.</p>
<h2>Exercises</h2>
<ol>
<li>
<p>Create a function generation prompt for a programming language of your choice that implements a data validation utility.</p>
</li>
<li>
<p>Take a complex function or class from your codebase and use the code explanation pattern to generate documentation for it.</p>
</li>
<li>
<p>Find a bug in your code and use the error diagnosis pattern to get help fixing it.</p>
</li>
<li>
<p>Use the format conversion pattern to transform a dataset between two different formats (e.g., JSON to CSV or XML to JSON).</p>
</li>
<li>
<p>Create a prompt that extracts structured information from unstructured API documentation or release notes.</p>
</li>
</ol>
</div>

<div class="chapter-container">
    <div class="chapter-header">
        <div class="chapter-page-number">Page 6</div>
    </div>
    <h1>Chapter 5: Advanced Prompting Techniques for Enhanced Control</h1>
<p>As you grow more experienced with prompt engineering, you'll want to move beyond basic prompting patterns to achieve more precise and reliable results. This chapter explores advanced techniques that give you greater control over LLM outputs, especially for complex development tasks.</p>
<h2>Chain-of-Thought (CoT): Guiding LLMs through Multi-Step Reasoning</h2>
<p>Chain-of-Thought prompting encourages LLMs to break down complex problems into logical steps before reaching a conclusion. This technique is particularly valuable for debugging, algorithm design, and other tasks that require structured reasoning.</p>
<h3>The CoT Principle</h3>
<p>The core idea behind CoT is to instruct the LLM to:
1. Decompose a complex problem into distinct steps
2. Reason through each step explicitly
3. Build toward the final solution incrementally</p>
<p>This mirrors how expert programmers approach difficult problems, leading to more accurate and explainable results.</p>
<h3>Basic CoT Template</h3>
<p>```
I need to solve the following problem: [problem description]</p>
<p>Please think through this step-by-step:
1. First, analyze the problem and clarify what we need to accomplish
2. Identify the key components or subproblems
3. Solve each subproblem
4. Combine the solutions
5. Verify the result</p>
<p>For each step, explain your reasoning before moving to the next step.
```</p>
<h3>Example: Debugging Complex Logic with CoT</h3>
<p>```
I need to find the bug in this function that's supposed to find the longest palindromic substring in a string:</p>
<p>```python
def longest_palindrome(s):
    if not s:
        return ""</p>
<pre class="codehilite"><code>longest = s[0]

for i in range(len(s)):
    # Check odd-length palindromes
    temp = expand_from_center(s, i, i)
    if len(temp) &gt; len(longest):
        longest = temp

    # Check even-length palindromes
    temp = expand_from_center(s, i, i+1)
    if len(temp) &gt; len(longest):
        longest = temp

return longest
</code></pre>

<p>def expand_from_center(s, left, right):
    while left &gt;= 0 and right &lt; len(s) and s[left] == s[right]:
        left += 1
        right -= 1</p>
<pre class="codehilite"><code>return s[left+1:right]
</code></pre>

<p>```</p>
<p>Please think through this step-by-step:
1. First, understand what the function should do and how it's supposed to work
2. Trace through the logic of both functions
3. Identify any logical errors
4. Explain the bug(s)
5. Provide a corrected implementation</p>
<p>For each step, explain your reasoning before moving to the next step.
```</p>
<h3>Advanced CoT: Managing Complex Development Tasks</h3>
<p>For more complex development tasks, structure your CoT to mirror software development best practices:</p>
<p>```
I need to develop a solution for [complex task]. </p>
<p>Please approach this step-by-step:</p>
<ol>
<li>Requirements analysis:</li>
<li>Clarify the exact requirements</li>
<li>
<p>Identify edge cases and constraints</p>
</li>
<li>
<p>System design:</p>
</li>
<li>Propose an overall architecture</li>
<li>Identify key components and their interactions</li>
<li>
<p>Choose appropriate data structures and algorithms</p>
</li>
<li>
<p>Implementation planning:</p>
</li>
<li>Break down the solution into implementable units</li>
<li>
<p>Determine the sequence of implementation</p>
</li>
<li>
<p>Implementation:</p>
</li>
<li>Write the code with clear comments</li>
<li>
<p>Explain design choices</p>
</li>
<li>
<p>Testing strategy:</p>
</li>
<li>Outline test cases covering normal operation and edge cases</li>
<li>Consider potential failure points</li>
</ol>
<p>For each step, provide your reasoning before moving to the next step.
```</p>
<h3>Implementing CoT in Code Generation Workflows</h3>
<p>While CoT prompting can be used directly in conversation with an LLM, you can also embed it in automated code generation workflows:</p>
<p>```python
def generate_complex_solution(problem_description, language="python"):
    """Generate solution for complex programming problems using CoT."""</p>
<pre class="codehilite"><code>cot_prompt = f&quot;&quot;&quot;
I need a solution for the following problem in {language}: 
{problem_description}

Please solve this step-by-step:
1. Analyze the problem requirements
2. Identify the key algorithms or data structures needed
3. Design the solution approach
4. Implement the code with appropriate comments
5. Analyze the time and space complexity

For each step, explain your reasoning before moving to the next step.
&quot;&quot;&quot;

response = llm_client.generate(cot_prompt)

# Extract the final code solution from the response
# (Implementation depends on your specific LLM and response format)
solution_code = extract_code_from_response(response)

return {
    &quot;full_reasoning&quot;: response,
    &quot;code_solution&quot;: solution_code
}
</code></pre>

<p>```</p>
<h2>Self-Correction &amp; Iterative Prompting</h2>
<p>Self-correction techniques encourage LLMs to review and refine their own outputs, mimicking the way developers iterate on their code through debugging and refactoring.</p>
<h3>Basic Self-Correction Template</h3>
<p>```
Please solve the following problem: [problem description]</p>
<p>After providing your solution, critically evaluate it for:
1. Correctness
2. Edge cases
3. Efficiency
4. Readability</p>
<p>Then provide an improved version based on your evaluation.
```</p>
<h3>Example: Self-Correcting Code Implementation</h3>
<p>```
Implement a function in Python that finds all anagrams of a given word in a list of words.</p>
<p>After providing your solution, critically evaluate it for:
1. Correctness
2. Edge cases (empty strings, different letter cases, etc.)
3. Time and space complexity
4. Readability and Pythonic style</p>
<p>Then provide an improved version based on your evaluation.
```</p>
<h3>Multi-Round Iterative Refinement</h3>
<p>For complex problems, we can use multi-round refinement where each iteration focuses on a specific aspect of improvement:</p>
<p>```python
def iterative_code_refinement(initial_prompt, iterations=3):
    """Generate and iteratively refine code through multiple LLM interactions."""</p>
<pre class="codehilite"><code># Initial solution
current_solution = llm_client.generate(initial_prompt)
code = extract_code(current_solution)

refinement_aspects = [
    &quot;correctness and edge cases&quot;,
    &quot;performance optimization&quot;,
    &quot;code readability and best practices&quot;
]

for i, aspect in enumerate(refinement_aspects[:iterations]):
    refinement_prompt = f&quot;&quot;&quot;
    Here is a code solution:

    ```
    {code}
    ```

    Please review this code focusing specifically on {aspect}.
    Identify any issues, explain them, and provide an improved version of the code.
    &quot;&quot;&quot;

    refinement_response = llm_client.generate(refinement_prompt)
    improved_code = extract_code(refinement_response)

    # Update current solution if improved code was provided
    if improved_code:
        code = improved_code

    print(f&quot;Completed refinement round {i+1}/{iterations}: {aspect}&quot;)

return code
</code></pre>

<p>```</p>
<h3>Self-Debug Pattern</h3>
<p>The self-debug pattern specifically targets error correction by having the LLM analyze and fix issues in its own output:</p>
<p>```
Generate a [language] function that [task description].</p>
<p>Then act as a code reviewer and:
1. Test the function with various inputs
2. Identify any bugs or edge cases that aren't handled
3. Fix the identified issues
4. Explain the bugs and your fixes
```</p>
<h3>Example of Self-Debug Pattern</h3>
<p>```
Generate a JavaScript function that parses a URL string and returns an object containing its components (protocol, host, path, query parameters, etc.).</p>
<p>Then act as a code reviewer and:
1. Test the function with various inputs (including URLs with and without protocols, query parameters, fragments, etc.)
2. Identify any bugs or edge cases that aren't handled
3. Fix the identified issues
4. Explain the bugs and your fixes
```</p>
<h2>Controlling Output: Parameter Tuning</h2>
<p>LLM APIs provide various parameters to control the nature of the generated outputs. Understanding these parameters is crucial for fine-tuning responses to specific development needs.</p>
<h3>Temperature: Controlling Randomness vs. Determinism</h3>
<p>Temperature (typically 0-1) controls the randomness of predictions:</p>
<ul>
<li><strong>Temperature = 0</strong>: More deterministic, focused responses</li>
<li><strong>Temperature = 0.7</strong>: Balanced creativity and coherence</li>
<li><strong>Temperature = 1</strong>: More random, diverse, and creative outputs</li>
</ul>
<h4>When to Use Different Temperature Settings</h4>
<table>
<thead>
<tr>
<th>Temperature</th>
<th>Best For</th>
<th>Development Use Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.0 - 0.1</td>
<td>Deterministic outputs, factual responses</td>
<td>Code generation, debugging, technical explanations</td>
</tr>
<tr>
<td>0.2 - 0.5</td>
<td>Slightly varied but focused responses</td>
<td>Documentation generation, code comments, refactoring suggestions</td>
</tr>
<tr>
<td>0.6 - 0.8</td>
<td>Creative but coherent responses</td>
<td>Generating alternative approaches, brainstorming solutions</td>
</tr>
<tr>
<td>0.9 - 1.0</td>
<td>Highly diverse and unexpected outputs</td>
<td>Creative problem solving, generating test cases, finding edge cases</td>
</tr>
</tbody>
</table>
<p>```python
def generate_code(prompt, creativity_level="low"):
    """Generate code with appropriate temperature based on creativity needs."""</p>
<pre class="codehilite"><code># Map creativity levels to temperature values
temp_mapping = {
    &quot;none&quot;: 0.0,     # Purely deterministic
    &quot;low&quot;: 0.2,      # Slight variations
    &quot;medium&quot;: 0.5,   # Balanced
    &quot;high&quot;: 0.8      # Creative approaches
}

temperature = temp_mapping.get(creativity_level, 0.0)

response = llm_client.generate(
    prompt,
    temperature=temperature
)

return response
</code></pre>

<p>```</p>
<h3>Top-P (Nucleus Sampling)</h3>
<p>Top-P (typically 0-1) controls how the model selects tokens from the probability distribution:</p>
<ul>
<li><strong>Top-P = 0.1</strong>: Only the most likely tokens (more focused)</li>
<li><strong>Top-P = 0.5</strong>: More variety but still relatively constrained</li>
<li><strong>Top-P = 0.9</strong>: Wider range of possible outputs</li>
</ul>
<p>For most code-related tasks, a lower Top-P (0.1-0.3) is preferable as it leads to more precise outputs.</p>
<h3>Frequency and Presence Penalties</h3>
<p>These parameters discourage repetition and can be useful in longer generations:</p>
<ul>
<li><strong>Frequency penalty</strong>: Reduces likelihood of repeating the same tokens</li>
<li><strong>Presence penalty</strong>: Reduces likelihood of repeating topics or themes</li>
</ul>
<p>For code generation, moderate frequency penalties (0.1-0.3) can help avoid redundant code structures.</p>
<h3>Max Tokens and Stopping Sequences</h3>
<ul>
<li><strong>Max tokens</strong>: Limits the length of the response</li>
<li><strong>Stopping sequences</strong>: Specific strings that tell the model to stop generating</li>
</ul>
<p>```python
def generate_function(function_spec):
    """Generate just a function without additional explanation."""</p>
<pre class="codehilite"><code>prompt = f&quot;Write a function that {function_spec}. Provide only the code without explanation.&quot;

response = llm_client.generate(
    prompt,
    max_tokens=500,
    stop=[&quot;\n\n&quot;, &quot;```&quot;, &quot;def &quot;, &quot;function &quot;]  # Stop after function definition
)

return response
</code></pre>

<p>```</p>
<h3>Parameter Selection Framework</h3>
<p>```python
def select_optimal_parameters(task_type, complexity):
    """Select optimal LLM parameters based on task requirements."""</p>
<pre class="codehilite"><code>params = {
    &quot;temperature&quot;: 0.0,
    &quot;top_p&quot;: 1.0,
    &quot;frequency_penalty&quot;: 0.0,
    &quot;presence_penalty&quot;: 0.0
}

# Adjust based on task type
if task_type == &quot;code_generation&quot;:
    params[&quot;temperature&quot;] = 0.0
    params[&quot;top_p&quot;] = 0.1
elif task_type == &quot;code_explanation&quot;:
    params[&quot;temperature&quot;] = 0.1
    params[&quot;top_p&quot;] = 0.3
elif task_type == &quot;refactoring&quot;:
    params[&quot;temperature&quot;] = 0.2
    params[&quot;frequency_penalty&quot;] = 0.3
elif task_type == &quot;creative_solution&quot;:
    params[&quot;temperature&quot;] = 0.7
    params[&quot;top_p&quot;] = 0.9

# Adjust based on complexity
if complexity == &quot;high&quot;:
    params[&quot;temperature&quot;] = min(params[&quot;temperature&quot;] + 0.1, 1.0)
    params[&quot;top_p&quot;] = min(params[&quot;top_p&quot;] + 0.1, 1.0)
elif complexity == &quot;low&quot;:
    params[&quot;temperature&quot;] = max(params[&quot;temperature&quot;] - 0.1, 0.0)
    params[&quot;top_p&quot;] = max(params[&quot;top_p&quot;] - 0.1, 0.0)

return params
</code></pre>

<p>```</p>
<h2>Persona-Based Prompting</h2>
<p>Persona-based prompts instruct the LLM to adopt a specific expertise or perspective, leading to more specialized and appropriate outputs.</p>
<h3>The Persona Template</h3>
<p>```
Act as a [role/persona] with expertise in [specific skills/domains]. Your task is to [specific task].</p>
<p>Key characteristics of this role:
- [characteristic 1]
- [characteristic 2]
- [characteristic 3]</p>
<p>Now, please [specific request].
```</p>
<h3>Developer Personas for Different Tasks</h3>
<h4>Senior Developer Persona</h4>
<p>```
Act as a senior software developer with 15+ years of experience in production environments and expertise in system design, performance optimization, and best practices. Your task is to review the following code.</p>
<p>Key characteristics of your role:
- Focus on maintainability and scalability
- Attention to edge cases and error handling
- Awareness of performance implications
- Experience with enterprise coding standards</p>
<p>Now, please review this code and suggest improvements:</p>
<p><code>[code block]</code>
```</p>
<h4>Security Expert Persona</h4>
<p>```
Act as a cybersecurity expert specializing in application security with experience performing security audits and penetration testing. Your task is to identify security vulnerabilities in the following code.</p>
<p>Key characteristics of your role:
- Deep knowledge of OWASP Top 10 vulnerabilities
- Experience with secure coding practices
- Understanding of common attack vectors
- Ability to suggest practical security mitigations</p>
<p>Now, please review this code for security vulnerabilities:</p>
<p><code>[code block]</code>
```</p>
<h4>Code Optimization Persona</h4>
<p>```
Act as a performance optimization specialist who focuses on making code run efficiently. Your expertise includes algorithmic optimization, memory management, and profiling techniques. Your task is to optimize the following function.</p>
<p>Key characteristics of your role:
- Deep understanding of time and space complexity
- Experience with profiling tools and bottleneck identification
- Knowledge of language-specific optimization techniques
- Focus on measurable performance improvements</p>
<p>Now, please optimize this code:</p>
<p><code>[code block]</code>
```</p>
<h3>Creating Composite Personas</h3>
<p>For complex tasks, you can create composite personas that combine multiple areas of expertise:</p>
<p>```
Act as a technical lead at a financial technology company who has expertise in both secure coding practices and high-performance systems. You specialize in designing backend systems that handle financial transactions and must balance security, compliance, and performance. Your task is to review and improve the following payment processing code.</p>
<p>Key characteristics of your role:
- Knowledge of financial compliance requirements (PCI DSS)
- Experience with secure transaction processing
- Expertise in optimizing high-throughput systems
- Background in designing fault-tolerant architectures</p>
<p>Now, please review and improve this payment processing code:</p>
<p><code>[code block]</code>
```</p>
<h3>Implementing Persona Selection in Applications</h3>
<p>For practical applications, you can create a library of personas and select the appropriate one based on the task:</p>
<p>```python
PERSONA_LIBRARY = {
    "senior_dev": {
        "intro": "Act as a senior software developer with 15+ years of experience...",
        "characteristics": [
            "Focus on maintainability and scalability",
            "Attention to edge cases and error handling",
            "Awareness of performance implications"
        ]
    },
    "security_expert": {
        "intro": "Act as a cybersecurity expert specializing in application security...",
        "characteristics": [
            "Deep knowledge of OWASP Top 10 vulnerabilities",
            "Experience with secure coding practices",
            "Understanding of common attack vectors"
        ]
    },
    # Additional personas...
}</p>
<p>def generate_with_persona(persona_key, task, content=None):
    """Generate content using a specific persona."""</p>
<pre class="codehilite"><code>if persona_key not in PERSONA_LIBRARY:
    raise ValueError(f&quot;Unknown persona: {persona_key}&quot;)

persona = PERSONA_LIBRARY[persona_key]

prompt = f&quot;{persona['intro']}\n\nKey characteristics of your role:&quot;

for characteristic in persona[&quot;characteristics&quot;]:
    prompt += f&quot;\n- {characteristic}&quot;

prompt += f&quot;\n\nNow, please {task}&quot;

if content:
    prompt += f&quot;:\n\n```\n{content}\n```&quot;

response = llm_client.generate(prompt)
return response
</code></pre>

<p>```</p>
<h2>Prompt Chaining and Orchestration Techniques</h2>
<p>Complex development tasks often require multiple LLM calls, with the output of one prompt feeding into another. Prompt chaining creates sophisticated workflows that combine multiple prompting techniques.</p>
<h3>Basic Prompt Chain Pattern</h3>
<p>```python
def multi_stage_code_development(task_description):
    """Generate code through multiple stages of refinement."""</p>
<pre class="codehilite"><code># Stage 1: Design the solution approach
design_prompt = f&quot;&quot;&quot;
I need to develop a solution for: {task_description}

Please provide a high-level design with:
1. Key components/functions needed
2. Data structures to use
3. Overall algorithm or approach
4. Potential edge cases to handle

Just focus on the design, not the implementation.
&quot;&quot;&quot;

design = llm_client.generate(design_prompt)

# Stage 2: Implement based on the design
implementation_prompt = f&quot;&quot;&quot;
Based on the following design:

{design}

Implement the complete solution in code. Include comments explaining key parts.
&quot;&quot;&quot;

implementation = llm_client.generate(implementation_prompt)
code = extract_code(implementation)

# Stage 3: Test case generation
test_prompt = f&quot;&quot;&quot;
For the following code:

```
{code}
```

Generate comprehensive test cases that cover:
1. Normal operation
2. Edge cases
3. Error conditions

Provide the test cases as executable code.
&quot;&quot;&quot;

test_cases = llm_client.generate(test_prompt)

return {
    &quot;design&quot;: design,
    &quot;implementation&quot;: code,
    &quot;tests&quot;: extract_code(test_cases)
}
</code></pre>

<p>```</p>
<h3>Advanced Orchestration: The Specialist Pattern</h3>
<p>The specialist pattern uses different prompts/personas for different aspects of a complex task:</p>
<p>```python
def develop_feature_with_specialists(feature_spec):
    """Develop a complete feature using specialist personas for each aspect."""</p>
<pre class="codehilite"><code>specialists = {
    &quot;architect&quot;: &quot;system design and architecture&quot;,
    &quot;implementer&quot;: &quot;clean, efficient implementation&quot;,
    &quot;security_expert&quot;: &quot;security best practices&quot;,
    &quot;tester&quot;: &quot;comprehensive testing&quot;,
    &quot;documenter&quot;: &quot;clear documentation&quot;
}

results = {}
accumulated_context = feature_spec

# Step through specialist chain
for role, expertise in specialists.items():
    prompt = f&quot;&quot;&quot;
    Act as a specialist in {expertise}.

    Project context so far:
    {accumulated_context}

    Your task is to contribute the {role} perspective to this feature.
    &quot;&quot;&quot;

    response = llm_client.generate(prompt)
    results[role] = response

    # Add this specialist's contribution to the accumulated context
    accumulated_context += f&quot;\n\n{role.upper()} CONTRIBUTION:\n{response}&quot;

return results
</code></pre>

<p>```</p>
<h3>Parallel Prompting with Aggregation</h3>
<p>For some tasks, you can get multiple independent perspectives and then combine them:</p>
<p>```python
import asyncio</p>
<p>async def get_multiple_perspectives(code_to_review, perspectives=None):
    """Get multiple review perspectives on the same code and aggregate results."""</p>
<pre class="codehilite"><code>if perspectives is None:
    perspectives = [&quot;readability&quot;, &quot;performance&quot;, &quot;security&quot;, &quot;maintainability&quot;]

async def get_perspective(aspect):
    prompt = f&quot;&quot;&quot;
    Review the following code focusing ONLY on {aspect}:

    ```
    {code_to_review}
    ```

    Provide specific issues and recommendations related to {aspect}.
    &quot;&quot;&quot;

    return {
        &quot;aspect&quot;: aspect,
        &quot;review&quot;: await llm_client.generate_async(prompt)
    }

# Get all perspectives in parallel
review_tasks = [get_perspective(aspect) for aspect in perspectives]
reviews = await asyncio.gather(*review_tasks)

# Create aggregation prompt
aggregation_prompt = f&quot;&quot;&quot;
I have received the following code reviews from different perspectives:

&quot;&quot;&quot;

for review in reviews:
    aggregation_prompt += f&quot;&quot;&quot;
    {review['aspect'].upper()} REVIEW:
    {review['review']}

    &quot;&quot;&quot;

aggregation_prompt += &quot;&quot;&quot;
Synthesize these reviews into a consolidated summary of:
1. The most critical issues to address
2. Recommended improvements in priority order
3. Positive aspects of the code worth preserving
&quot;&quot;&quot;

consolidated_review = await llm_client.generate_async(aggregation_prompt)

return {
    &quot;individual_reviews&quot;: reviews,
    &quot;consolidated_review&quot;: consolidated_review
}
</code></pre>

<p>```</p>
<h2>Error Handling Strategies for Inadequate LLM Responses</h2>
<p>When working with LLMs, you'll inevitably encounter responses that don't meet your requirements. Implementing robust error handling is crucial for production applications.</p>
<h3>Response Validation Pattern</h3>
<p>Always validate LLM outputs before using them in your application:</p>
<p>```python
def validate_code_response(code, language="python"):
    """Validate that an LLM-generated code snippet is valid."""</p>
<pre class="codehilite"><code># Basic syntactic validation
if language == &quot;python&quot;:
    try:
        ast.parse(code)
        return True, &quot;Valid Python syntax&quot;
    except SyntaxError as e:
        return False, f&quot;Python syntax error: {str(e)}&quot;
elif language == &quot;javascript&quot;:
    # Use appropriate JS parser here
    pass

# You could add additional validation like:
# - Checking that specific functions exist
# - Verifying that requirements are met
# - Testing with example inputs

return True, &quot;Passed validation&quot;
</code></pre>

<p>```</p>
<h3>Retry with Enhanced Context</h3>
<p>When an LLM response is inadequate, retry with additional context:</p>
<p>```python
def get_working_solution(problem_statement, max_attempts=3):
    """Get a working solution by retrying with improved context."""</p>
<pre class="codehilite"><code>prompt = f&quot;Write a function that {problem_statement}. Include proper error handling.&quot;

for attempt in range(1, max_attempts + 1):
    print(f&quot;Attempt {attempt}/{max_attempts}&quot;)

    response = llm_client.generate(prompt)
    code = extract_code(response)

    valid, message = validate_code_response(code)
    if valid:
        return code

    # If invalid, enhance the prompt with error information
    prompt = f&quot;&quot;&quot;
    You provided the following solution:

    ```
    {code}
    ```

    However, there was an issue: {message}

    Please provide a corrected solution that addresses this problem.
    Original task: Write a function that {problem_statement}
    &quot;&quot;&quot;

# If we exhaust attempts, raise an exception
raise Exception(f&quot;Failed to generate valid code after {max_attempts} attempts&quot;)
</code></pre>

<p>```</p>
<h3>Fallback Chain Strategy</h3>
<p>Implement a chain of fallbacks when an LLM response doesn't meet requirements:</p>
<p>```python
def generate_with_fallbacks(prompt, validators=None):
    """Generate content with a series of fallback strategies."""</p>
<pre class="codehilite"><code>if validators is None:
    validators = [basic_validator]

# First attempt: Standard generation
response = llm_client.generate(prompt)

for validator in validators:
    valid, message = validator(response)
    if valid:
        return response

# Fallback 1: Retry with more specific instructions
enhanced_prompt = f&quot;&quot;&quot;
Previous attempt did not meet the requirements because: {message}

Let me clarify what's needed:
{prompt}

Please ensure your response meets all requirements.
&quot;&quot;&quot;

response = llm_client.generate(enhanced_prompt)

for validator in validators:
    valid, message = validator(response)
    if valid:
        return response

# Fallback 2: Try with different parameters
response = llm_client.generate(
    enhanced_prompt,
    temperature=0.0,  # Switch to deterministic mode
    max_tokens=2000   # Allow more space for complete response
)

for validator in validators:
    valid, message = validator(response)
    if valid:
        return response

# Fallback 3: Try a different model (e.g., more capable)
response = llm_client.generate(
    enhanced_prompt,
    model=&quot;more-capable-model&quot;  # e.g., gpt-4 instead of gpt-3.5-turbo
)

for validator in validators:
    valid, message = validator(response)
    if valid:
        return response

# If all fallbacks fail, return the best effort with a warning
return {
    &quot;response&quot;: response,
    &quot;warning&quot;: &quot;Response may not meet all requirements&quot;,
    &quot;validation_message&quot;: message
}
</code></pre>

<p>```</p>
<h2>Evaluating LLM Output Quality Programmatically</h2>
<p>Systematic evaluation of LLM outputs is essential for maintaining quality and improving prompt design over time.</p>
<h3>Code Execution Evaluation</h3>
<p>For code generation tasks, the ultimate test is whether the code executes correctly:</p>
<p>```python
import subprocess
import tempfile
import os</p>
<p>def evaluate_code_execution(code, test_input=None, expected_output=None, timeout=5):
    """Evaluate code by executing it and checking the output."""</p>
<pre class="codehilite"><code>with tempfile.NamedTemporaryFile(suffix='.py', delete=False) as temp:
    temp_name = temp.name
    temp.write(code.encode('utf-8'))

try:
    # Execute the code
    if test_input:
        # If we have test input, provide it via stdin
        process = subprocess.run(
            ['python', temp_name],
            input=test_input.encode('utf-8'),
            capture_output=True,
            timeout=timeout
        )
    else:
        process = subprocess.run(
            ['python', temp_name],
            capture_output=True,
            timeout=timeout
        )

    stdout = process.stdout.decode('utf-8')
    stderr = process.stderr.decode('utf-8')

    # Check if execution was successful
    if process.returncode != 0:
        return False, f&quot;Execution failed with error: {stderr}&quot;

    # If expected output is provided, check against it
    if expected_output is not None:
        if stdout.strip() == expected_output.strip():
            return True, &quot;Output matches expected result&quot;
        else:
            return False, f&quot;Output doesn't match expected result.\nExpected: {expected_output}\nActual: {stdout}&quot;

    return True, stdout

except subprocess.TimeoutExpired:
    return False, f&quot;Execution timed out after {timeout} seconds&quot;

finally:
    # Clean up the temporary file
    if os.path.exists(temp_name):
        os.unlink(temp_name)
</code></pre>

<p>```</p>
<h3>Unit Test Generation and Execution</h3>
<p>Generate unit tests and use them to validate LLM-generated code:</p>
<p>```python
def evaluate_with_generated_tests(code_solution, problem_description):
    """Evaluate code by generating and running tests for it."""</p>
<pre class="codehilite"><code># Generate test cases based on problem description
test_generation_prompt = f&quot;&quot;&quot;
For the following problem:
{problem_description}

Generate comprehensive pytest unit tests that cover normal cases, edge cases, and error cases.
Focus only on the tests, assuming the solution is implemented in a function called 'solution'.
&quot;&quot;&quot;

test_code = llm_client.generate(test_generation_prompt)
test_code = extract_code(test_code)

# Combine solution and tests in a temporary file
full_code = f&quot;&quot;&quot;
</code></pre>

<p>{code_solution}</p>
<h1>Generated tests</h1>
<p>{test_code}
    """</p>
<pre class="codehilite"><code># Execute the tests
with tempfile.NamedTemporaryFile(suffix='.py', delete=False) as temp:
    temp_name = temp.name
    temp.write(full_code.encode('utf-8'))

try:
    process = subprocess.run(
        ['pytest', temp_name, '-v'],
        capture_output=True
    )

    stdout = process.stdout.decode('utf-8')
    stderr = process.stderr.decode('utf-8')

    # Parse test results
    if process.returncode == 0:
        return True, &quot;All tests passed&quot;, stdout
    else:
        return False, &quot;Some tests failed&quot;, stdout + &quot;\n&quot; + stderr

finally:
    if os.path.exists(temp_name):
        os.unlink(temp_name)
</code></pre>

<p>```</p>
<h3>Functional Requirements Verification</h3>
<p>Verify that the generated solution meets all functional requirements:</p>
<p>```python
def verify_requirements_coverage(code, requirements):
    """Check if code likely addresses all specified requirements."""</p>
<pre class="codehilite"><code>evaluation_prompt = f&quot;&quot;&quot;
Given the following code:

```
{code}
```

And these requirements:

&quot;&quot;&quot;

for i, req in enumerate(requirements, 1):
    evaluation_prompt += f&quot;{i}. {req}\n&quot;

evaluation_prompt += &quot;&quot;&quot;
For each requirement, determine if the code addresses it:
1. Respond with YES if the requirement is clearly addressed
2. Respond with PARTIAL if the requirement is partly addressed
3. Respond with NO if the requirement is not addressed

Format your response as a JSON object with requirement numbers as keys and assessment as values,
with an additional 'explanation' field for each requirement.
&quot;&quot;&quot;

response = llm_client.generate(evaluation_prompt)

try:
    # Extract JSON from the response
    import re
    import json

    json_match = re.search(r'{.*}', response, re.DOTALL)
    if json_match:
        assessment = json.loads(json_match.group(0))

        # Calculate coverage percentage
        covered = sum(1 for v in assessment.values() if isinstance(v, dict) and v.get('assessment') == 'YES')
        partial = sum(0.5 for v in assessment.values() if isinstance(v, dict) and v.get('assessment') == 'PARTIAL')
        total_requirements = len(requirements)

        coverage_pct = (covered + partial) / total_requirements * 100 if total_requirements &gt; 0 else 0

        return {
            &quot;coverage_percentage&quot;: coverage_pct,
            &quot;detailed_assessment&quot;: assessment,
            &quot;missing_requirements&quot;: [
                req for i, req in enumerate(requirements, 1) 
                if str(i) in assessment and assessment[str(i)].get('assessment') == 'NO'
            ]
        }
except Exception as e:
    return {
        &quot;error&quot;: f&quot;Failed to parse assessment: {str(e)}&quot;,
        &quot;raw_response&quot;: response
    }
</code></pre>

<p>```</p>
<h3>Comprehensive Evaluation Framework</h3>
<p>For production applications, implement a comprehensive evaluation framework:</p>
<p>```python
class LLMCodeEvaluator:
    """Framework for evaluating LLM-generated code quality."""</p>
<pre class="codehilite"><code>def __init__(self, code, language=&quot;python&quot;):
    self.code = code
    self.language = language
    self.evaluation_results = {}

def run_all_evaluations(self):
    &quot;&quot;&quot;Run all available evaluations.&quot;&quot;&quot;
    self.evaluate_syntax()
    self.evaluate_style()
    self.evaluate_complexity()
    self.evaluate_security()

    if self.language == &quot;python&quot;:
        self.run_python_specific_evaluations()

    return self.get_summary()

def evaluate_syntax(self):
    &quot;&quot;&quot;Check for syntax errors.&quot;&quot;&quot;
    if self.language == &quot;python&quot;:
        try:
            ast.parse(self.code)
            self.evaluation_results[&quot;syntax&quot;] = {
                &quot;pass&quot;: True,
                &quot;message&quot;: &quot;No syntax errors detected&quot;
            }
        except SyntaxError as e:
            self.evaluation_results[&quot;syntax&quot;] = {
                &quot;pass&quot;: False,
                &quot;message&quot;: f&quot;Syntax error: {str(e)}&quot;
            }
    # Add handlers for other languages

def evaluate_style(self):
    &quot;&quot;&quot;Evaluate code style compliance.&quot;&quot;&quot;
    # For Python, could use tools like flake8, black
    # For JS, could use ESLint
    # Here's a simplified example using an LLM for style evaluation:

    style_prompt = f&quot;&quot;&quot;
    Review this {self.language} code for style issues:

    ```
    {self.code}
    ```

    Identify any style issues according to common {self.language} conventions.
    Return your response as a JSON with 'issues' (array) and 'score' (0-10).
    &quot;&quot;&quot;

    response = llm_client.generate(style_prompt)
    # Parse response and extract style evaluation
    # (Implementation details omitted)

    self.evaluation_results[&quot;style&quot;] = {
        &quot;pass&quot;: style_score &gt; 7,
        &quot;score&quot;: style_score,
        &quot;issues&quot;: style_issues
    }

# Additional evaluation methods...

def get_summary(self):
    &quot;&quot;&quot;Generate overall evaluation summary.&quot;&quot;&quot;
    total_checks = len(self.evaluation_results)
    passed_checks = sum(1 for result in self.evaluation_results.values() if result.get(&quot;pass&quot;))

    return {
        &quot;overall_score&quot;: passed_checks / total_checks if total_checks &gt; 0 else 0,
        &quot;passed_checks&quot;: passed_checks,
        &quot;total_checks&quot;: total_checks,
        &quot;detailed_results&quot;: self.evaluation_results
    }
</code></pre>

<p>```</p>
<h2>Conclusion</h2>
<p>Advanced prompting techniques give developers unprecedented control over LLM outputs. By mastering Chain-of-Thought reasoning, self-correction, parameter tuning, persona-based prompting, and effective error handling strategies, you can create more reliable, higher-quality solutions for complex development tasks.</p>
<p>The techniques in this chapter build upon the foundation established earlier and represent the current state of the art in prompt engineering for software development. As you apply these methods in your work, you'll develop an intuitive sense for which techniques work best for different types of tasks.</p>
<p>In the next chapter, we'll put these advanced techniques into practice by building a complete Smart Code Assistant that can help with a wide range of development tasks.</p>
<h2>Exercises</h2>
<ol>
<li>
<p>Create a Chain-of-Thought prompt for solving a complex algorithmic problem, and compare the results with a simple prompt for the same problem.</p>
</li>
<li>
<p>Implement a self-correction workflow for a code generation task that includes multiple rounds of refinement.</p>
</li>
<li>
<p>Experiment with different temperature settings for the same code generation task, and document how the outputs differ.</p>
</li>
<li>
<p>Design three different personas for code review, focusing on different aspects (e.g., security, performance, readability), and compare their feedback on the same piece of code.</p>
</li>
<li>
<p>Build a simple prompt chaining system that generates code, tests, and documentation for a specific function in sequence.</p>
</li>
</ol>
</div>

<div class="chapter-container">
    <div class="chapter-header">
        <div class="chapter-page-number">Page 81</div>
    </div>
    <h1>Chapter 6: Building Effective Developer Tooling for LLM Applications</h1>
<p>In the previous chapters, we've explored the fundamentals of prompt engineering and various techniques to create effective prompts. Now, it's time to take our skills to the next level by implementing robust developer tooling for LLM applications. As LLMs become integral parts of modern software systems, proper tooling becomes essential for maintainability, scalability, and reliability.</p>
<h2>6.1 Prompt Libraries and Reuse Patterns</h2>
<h3>6.1.1 The Need for Prompt Management</h3>
<p>As your project grows, managing prompts becomes increasingly challenging. Without proper organization, you might face:</p>
<ul>
<li>Duplicate prompts across different parts of your application</li>
<li>Inconsistent prompting styles and formats</li>
<li>Difficulty in tracking which prompts work best for specific tasks</li>
<li>Challenges in version control and prompt evolution</li>
</ul>
<h3>6.1.2 Building a Prompt Library</h3>
<p>Let's create a simple yet effective prompt library in Python:</p>
<p>```python</p>
<h1>A basic prompt library implementation</h1>
<p>class PromptTemplate:
    def <strong>init</strong>(self, template, required_variables=None):
        self.template = template
        self.required_variables = required_variables or []</p>
<pre class="codehilite"><code>def format(self, **kwargs):
    # Check if all required variables are provided
    missing_vars = [var for var in self.required_variables if var not in kwargs]
    if missing_vars:
        raise ValueError(f&quot;Missing required variables: {', '.join(missing_vars)}&quot;)

    # Format the template with the provided variables
    return self.template.format(**kwargs)
</code></pre>

<p>class PromptLibrary:
    def <strong>init</strong>(self):
        self.prompts = {}</p>
<pre class="codehilite"><code>def add_prompt(self, name, template, required_variables=None):
    self.prompts[name] = PromptTemplate(template, required_variables)

def get_prompt(self, name, **kwargs):
    if name not in self.prompts:
        raise KeyError(f&quot;Prompt '{name}' not found in the library&quot;)
    return self.prompts[name].format(**kwargs)
</code></pre>

<p>```</p>
<p>Usage example:</p>
<p>```python</p>
<h1>Initialize the library</h1>
<p>prompt_lib = PromptLibrary()</p>
<h1>Add prompts with templates</h1>
<p>prompt_lib.add_prompt(
    "code_explanation",
    "Explain the following {language} code:\n\n<code>{language}\n{code}\n</code>\n\nProvide a detailed explanation including:",
    ["language", "code"]
)</p>
<p>prompt_lib.add_prompt(
    "bug_fix",
    "Fix the following {language} code that has a bug:\n\n<code>{language}\n{code}\n</code>\n\nError message: {error}\n\nProvide the corrected code and explain the fix.",
    ["language", "code", "error"]
)</p>
<h1>Use the prompt template</h1>
<p>python_code = "def factorial(n):\n    if n == 0:\n        return 1\n    return n * factorial(n-1)"
formatted_prompt = prompt_lib.get_prompt("code_explanation", language="python", code=python_code)
```</p>
<h3>6.1.3 Advanced Prompt Organization Patterns</h3>
<p>For larger projects, consider organizing prompts hierarchically:</p>
<p>```python</p>
<h1>Domain-specific prompt libraries</h1>
<p>class CodePromptLibrary(PromptLibrary):
    def <strong>init</strong>(self):
        super().<strong>init</strong>()
        self._initialize_code_prompts()</p>
<pre class="codehilite"><code>def _initialize_code_prompts(self):
    self.add_prompt(&quot;generate_function&quot;, &quot;Write a {language} function that {requirement}.&quot;, [&quot;language&quot;, &quot;requirement&quot;])
    self.add_prompt(&quot;optimize_code&quot;, &quot;Optimize the following {language} code for {optimization_goal}:\n\n```{language}\n{code}\n```&quot;, [&quot;language&quot;, &quot;optimization_goal&quot;, &quot;code&quot;])
    # More code-related prompts...
</code></pre>

<p>```</p>
<h2>6.2 Debugging Tools for LLM Applications</h2>
<h3>6.2.1 Prompt Debugging</h3>
<p>Debugging LLM applications presents unique challenges compared to traditional software. Let's implement a simple prompt debugger:</p>
<p>```python
import json
from datetime import datetime</p>
<p>class PromptDebugger:
    def <strong>init</strong>(self, log_file=None):
        self.log_file = log_file
        self.history = []</p>
<pre class="codehilite"><code>def log_interaction(self, prompt, response, metadata=None):
    interaction = {
        &quot;timestamp&quot;: datetime.now().isoformat(),
        &quot;prompt&quot;: prompt,
        &quot;response&quot;: response,
        &quot;metadata&quot;: metadata or {}
    }
    self.history.append(interaction)

    if self.log_file:
        with open(self.log_file, 'a') as f:
            f.write(json.dumps(interaction) + &quot;\n&quot;)

    return interaction

def analyze_token_usage(self, interaction):
    if 'token_usage' in interaction['metadata']:
        usage = interaction['metadata']['token_usage']
        return f&quot;Prompt tokens: {usage.get('prompt_tokens', 'N/A')}, &quot; \
               f&quot;Completion tokens: {usage.get('completion_tokens', 'N/A')}, &quot; \
               f&quot;Total tokens: {usage.get('total_tokens', 'N/A')}&quot;
    return &quot;Token usage data not available&quot;

def compare_interactions(self, interaction1_idx, interaction2_idx):
    if interaction1_idx &gt;= len(self.history) or interaction2_idx &gt;= len(self.history):
        return &quot;Invalid interaction indices&quot;

    int1 = self.history[interaction1_idx]
    int2 = self.history[interaction2_idx]

    # Compare prompts
    prompt_diff = self._simple_diff(int1['prompt'], int2['prompt'])

    # Compare responses (simplified)
    response_similarity = self._calculate_similarity(int1['response'], int2['response'])

    return {
        &quot;prompt_differences&quot;: prompt_diff,
        &quot;response_similarity&quot;: f&quot;{response_similarity:.2f}%&quot;
    }

def _simple_diff(self, text1, text2):
    # A very simple diff implementation
    # In a real application, use a proper diff library
    if text1 == text2:
        return &quot;No differences&quot;

    # Basic character-by-character comparison
    diffs = []
    for i, (c1, c2) in enumerate(zip(text1, text2)):
        if c1 != c2:
            diffs.append(f&quot;Pos {i}: '{c1}' vs '{c2}'&quot;)

    if len(text1) != len(text2):
        diffs.append(f&quot;Length difference: {len(text1)} vs {len(text2)}&quot;)

    return diffs[:10]  # Only show first 10 differences

def _calculate_similarity(self, text1, text2):
    # Simple similarity calculation
    # In a real application, use a more sophisticated algorithm
    common_chars = sum(1 for c1, c2 in zip(text1, text2) if c1 == c2)
    total_length = max(len(text1), len(text2))
    return (common_chars / total_length) * 100 if total_length &gt; 0 else 100
</code></pre>

<p>```</p>
<p>Example usage with OpenAI's API:</p>
<p>```python
import openai</p>
<p>debugger = PromptDebugger(log_file="llm_debug_log.jsonl")</p>
<p>def query_llm(prompt, model="gpt-3.5-turbo"):
    response = openai.ChatCompletion.create(
        model=model,
        messages=[{"role": "user", "content": prompt}]
    )</p>
<pre class="codehilite"><code>content = response.choices[0].message.content

# Log the interaction with metadata
debugger.log_interaction(
    prompt=prompt,
    response=content,
    metadata={
        &quot;model&quot;: model,
        &quot;token_usage&quot;: response.usage._asdict() if hasattr(response, &quot;usage&quot;) else None,
        &quot;finish_reason&quot;: response.choices[0].finish_reason
    }
)

return content
</code></pre>

<p>```</p>
<h3>6.2.2 Visualizing LLM Behavior</h3>
<p>To understand how changes in prompts affect LLM responses, visualization tools can be invaluable:</p>
<p>```python
import matplotlib.pyplot as plt
import numpy as np</p>
<p>def visualize_token_usage(debugger, last_n=10):
    """Visualize token usage for the last N interactions"""
    if len(debugger.history) == 0:
        return "No history available"</p>
<pre class="codehilite"><code># Get data for the last n interactions
history = debugger.history[-last_n:]

prompt_tokens = []
completion_tokens = []
labels = []

for i, interaction in enumerate(history):
    metadata = interaction.get('metadata', {})
    usage = metadata.get('token_usage', {})

    prompt_tokens.append(usage.get('prompt_tokens', 0))
    completion_tokens.append(usage.get('completion_tokens', 0))
    labels.append(f&quot;Query {i+1}&quot;)

# Create stacked bar chart
width = 0.35
fig, ax = plt.subplots(figsize=(12, 7))

ax.bar(labels, prompt_tokens, width, label='Prompt Tokens')
ax.bar(labels, completion_tokens, width, bottom=prompt_tokens, label='Completion Tokens')

ax.set_ylabel('Token Count')
ax.set_title('Token Usage by Query')
ax.legend()

plt.tight_layout()
plt.xticks(rotation=45)
plt.savefig('token_usage.png')
plt.close()

return &quot;Token usage visualization saved as 'token_usage.png'&quot;
</code></pre>

<p>```</p>
<h2>6.3 Performance Profiling and Optimization</h2>
<h3>6.3.1 Measuring LLM Application Performance</h3>
<p>Performance in LLM applications involves several metrics:</p>
<p>```python
import time
import statistics
from functools import wraps</p>
<p>class LLMProfiler:
    def <strong>init</strong>(self):
        self.metrics = {
            "latency": [],
            "token_throughput": [],
            "success_rate": {"success": 0, "failure": 0},
            "cost": []
        }</p>
<pre class="codehilite"><code>def profile_request(self, func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        error = None
        response = None

        try:
            response = func(*args, **kwargs)
            self.metrics[&quot;success_rate&quot;][&quot;success&quot;] += 1
        except Exception as e:
            error = e
            self.metrics[&quot;success_rate&quot;][&quot;failure&quot;] += 1

        end_time = time.time()
        latency = end_time - start_time
        self.metrics[&quot;latency&quot;].append(latency)

        # Calculate token throughput if possible
        if response and hasattr(response, &quot;usage&quot;):
            total_tokens = response.usage.total_tokens
            tokens_per_second = total_tokens / latency if latency &gt; 0 else 0
            self.metrics[&quot;token_throughput&quot;].append(tokens_per_second)

            # Calculate approximate cost (example for GPT-3.5-turbo)
            # Rates as of 2023, adjust as needed
            prompt_cost = response.usage.prompt_tokens * 0.0015 / 1000  # $0.0015 per 1K tokens
            completion_cost = response.usage.completion_tokens * 0.002 / 1000  # $0.002 per 1K tokens
            total_cost = prompt_cost + completion_cost
            self.metrics[&quot;cost&quot;].append(total_cost)

        if error:
            raise error

        return response

    return wrapper

def get_summary(self):
    latency_stats = {
        &quot;min&quot;: min(self.metrics[&quot;latency&quot;]) if self.metrics[&quot;latency&quot;] else None,
        &quot;max&quot;: max(self.metrics[&quot;latency&quot;]) if self.metrics[&quot;latency&quot;] else None,
        &quot;avg&quot;: statistics.mean(self.metrics[&quot;latency&quot;]) if self.metrics[&quot;latency&quot;] else None,
        &quot;p95&quot;: self._percentile(self.metrics[&quot;latency&quot;], 95),
        &quot;p99&quot;: self._percentile(self.metrics[&quot;latency&quot;], 99)
    }

    throughput_stats = {
        &quot;avg&quot;: statistics.mean(self.metrics[&quot;token_throughput&quot;]) if self.metrics[&quot;token_throughput&quot;] else None
    }

    success_rate = (
        self.metrics[&quot;success_rate&quot;][&quot;success&quot;] / 
        (self.metrics[&quot;success_rate&quot;][&quot;success&quot;] + self.metrics[&quot;success_rate&quot;][&quot;failure&quot;])
        if (self.metrics[&quot;success_rate&quot;][&quot;success&quot;] + self.metrics[&quot;success_rate&quot;][&quot;failure&quot;]) &gt; 0
        else 0
    ) * 100

    total_cost = sum(self.metrics[&quot;cost&quot;])

    return {
        &quot;latency_ms&quot;: {k: v*1000 if v is not None else None for k, v in latency_stats.items()},
        &quot;throughput_tokens_per_sec&quot;: throughput_stats,
        &quot;success_rate_percent&quot;: success_rate,
        &quot;total_cost_usd&quot;: total_cost,
        &quot;request_count&quot;: len(self.metrics[&quot;latency&quot;])
    }

def _percentile(self, data, percentile):
    if not data:
        return None
    sorted_data = sorted(data)
    index = int(len(sorted_data) * (percentile / 100))
    return sorted_data[index]
</code></pre>

<p>```</p>
<h3>6.3.2 Optimizing Prompt Performance</h3>
<p>We can optimize prompts in several ways:</p>
<ol>
<li><strong>Prompt Compression Techniques:</strong></li>
</ol>
<p>```python
def compress_prompt(prompt, max_length=None):
    """Compress a prompt by removing redundancies while preserving meaning"""
    # Simple compression techniques
    compressed = prompt</p>
<pre class="codehilite"><code># Remove redundant whitespace
compressed = &quot; &quot;.join(compressed.split())

# Replace common verbose phrases
replacements = {
    &quot;Please provide a detailed explanation of&quot;: &quot;Explain&quot;,
    &quot;I would like you to&quot;: &quot;&quot;,
    &quot;It would be great if you could&quot;: &quot;&quot;,
    &quot;Can you please&quot;: &quot;&quot;,
}

for verbose, concise in replacements.items():
    compressed = compressed.replace(verbose, concise)

# If a maximum length is specified, truncate while preserving key instructions
if max_length and len(compressed) &gt; max_length:
    # This is a simplistic approach - a real implementation would be more sophisticated
    lines = compressed.split('. ')
    result = []
    current_length = 0

    # Always include the first line (assumed to contain the main instruction)
    result.append(lines[0])
    current_length += len(lines[0])

    # Add as many additional lines as fit within max_length
    for line in lines[1:]:
        if current_length + len(line) + 2 &lt;= max_length:  # +2 for the '. '
            result.append(line)
            current_length += len(line) + 2
        else:
            break

    compressed = '. '.join(result)
    if not compressed.endswith('.'):
        compressed += '.'

return compressed
</code></pre>

<p>```</p>
<ol>
<li><strong>Caching LLM Responses:</strong></li>
</ol>
<p>```python
import hashlib
import json
import os
import pickle</p>
<p>class LLMResponseCache:
    def <strong>init</strong>(self, cache_dir="llm_cache", ttl_seconds=86400):
        """Initialize the cache with a directory and time-to-live"""
        self.cache_dir = cache_dir
        self.ttl_seconds = ttl_seconds
        os.makedirs(cache_dir, exist_ok=True)</p>
<pre class="codehilite"><code>def _get_cache_key(self, prompt, model, temperature):
    &quot;&quot;&quot;Create a unique cache key from the request parameters&quot;&quot;&quot;
    key_data = {
        &quot;prompt&quot;: prompt,
        &quot;model&quot;: model,
        &quot;temperature&quot;: temperature
    }
    key_string = json.dumps(key_data, sort_keys=True)
    return hashlib.md5(key_string.encode()).hexdigest()

def _get_cache_path(self, key):
    &quot;&quot;&quot;Get the file path for a cache key&quot;&quot;&quot;
    return os.path.join(self.cache_dir, f&quot;{key}.pkl&quot;)

def get(self, prompt, model, temperature=0):
    &quot;&quot;&quot;Retrieve a response from the cache if it exists and is still valid&quot;&quot;&quot;
    key = self._get_cache_key(prompt, model, temperature)
    cache_path = self._get_cache_path(key)

    if not os.path.exists(cache_path):
        return None

    # Check if cache has expired
    cache_age = time.time() - os.path.getmtime(cache_path)
    if cache_age &gt; self.ttl_seconds:
        os.remove(cache_path)  # Remove expired cache
        return None

    try:
        with open(cache_path, 'rb') as f:
            return pickle.load(f)
    except:
        return None

def set(self, prompt, model, temperature, response):
    &quot;&quot;&quot;Store a response in the cache&quot;&quot;&quot;
    key = self._get_cache_key(prompt, model, temperature)
    cache_path = self._get_cache_path(key)

    with open(cache_path, 'wb') as f:
        pickle.dump(response, f)
</code></pre>

<p>```</p>
<p>Example usage of caching:</p>
<p>```python
cache = LLMResponseCache()</p>
<p>def query_llm_with_cache(prompt, model="gpt-3.5-turbo", temperature=0):
    # Try to get from cache first
    cached_response = cache.get(prompt, model, temperature)
    if cached_response:
        print("Cache hit!")
        return cached_response</p>
<pre class="codehilite"><code># If not in cache, make the API call
print(&quot;Cache miss, calling API...&quot;)
response = openai.ChatCompletion.create(
    model=model,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
    temperature=temperature
)

# Store in cache for future use
cache.set(prompt, model, temperature, response)

return response
</code></pre>

<p>```</p>
<h2>6.4 Integration with Existing Development Workflows</h2>
<h3>6.4.1 Command Line Tools for Prompt Engineering</h3>
<p>Creating a simple CLI tool for prompt engineering:</p>
<p>```python</p>
<h1>!/usr/bin/env python</h1>
<p>import argparse
import sys
import json
import openai
from pathlib import Path</p>
<p>def setup_argparser():
    parser = argparse.ArgumentParser(description="LLM Prompt Engineering CLI Tool")
    subparsers = parser.add_subparsers(dest="command", help="Commands")</p>
<pre class="codehilite"><code># Query LLM command
query_parser = subparsers.add_parser(&quot;query&quot;, help=&quot;Query an LLM with a prompt&quot;)
query_parser.add_argument(&quot;--prompt&quot;, &quot;-p&quot;, help=&quot;The prompt to send&quot;)
query_parser.add_argument(&quot;--prompt-file&quot;, &quot;-f&quot;, help=&quot;File containing the prompt&quot;)
query_parser.add_argument(&quot;--model&quot;, &quot;-m&quot;, default=&quot;gpt-3.5-turbo&quot;, help=&quot;LLM model to use&quot;)
query_parser.add_argument(&quot;--output&quot;, &quot;-o&quot;, help=&quot;Save output to file&quot;)
query_parser.add_argument(&quot;--temperature&quot;, &quot;-t&quot;, type=float, default=0, help=&quot;Temperature setting&quot;)

# Test prompt variations command
test_parser = subparsers.add_parser(&quot;test-variations&quot;, help=&quot;Test different prompt variations&quot;)
test_parser.add_argument(&quot;--variations-file&quot;, required=True, help=&quot;JSON file with prompt variations&quot;)
test_parser.add_argument(&quot;--model&quot;, &quot;-m&quot;, default=&quot;gpt-3.5-turbo&quot;, help=&quot;LLM model to use&quot;)
test_parser.add_argument(&quot;--output-dir&quot;, &quot;-o&quot;, default=&quot;./results&quot;, help=&quot;Directory to save results&quot;)

# Batch processing command
batch_parser = subparsers.add_parser(&quot;batch&quot;, help=&quot;Process a batch of prompts&quot;)
batch_parser.add_argument(&quot;--batch-file&quot;, required=True, help=&quot;JSON file with prompts to process&quot;)
batch_parser.add_argument(&quot;--model&quot;, &quot;-m&quot;, default=&quot;gpt-3.5-turbo&quot;, help=&quot;LLM model to use&quot;)
batch_parser.add_argument(&quot;--output-dir&quot;, &quot;-o&quot;, default=&quot;./results&quot;, help=&quot;Directory to save results&quot;)

return parser
</code></pre>

<p>def main():
    parser = setup_argparser()
    args = parser.parse_args()</p>
<pre class="codehilite"><code>if args.command is None:
    parser.print_help()
    return

# Initialize OpenAI API (assuming OPENAI_API_KEY environment variable is set)
if not openai.api_key:
    print(&quot;Error: OpenAI API key not found. Set the OPENAI_API_KEY environment variable.&quot;)
    sys.exit(1)

if args.command == &quot;query&quot;:
    handle_query_command(args)
elif args.command == &quot;test-variations&quot;:
    handle_test_variations_command(args)
elif args.command == &quot;batch&quot;:
    handle_batch_command(args)
</code></pre>

<p>def handle_query_command(args):
    # Get prompt from arguments or file
    if args.prompt:
        prompt = args.prompt
    elif args.prompt_file:
        with open(args.prompt_file, 'r') as f:
            prompt = f.read()
    else:
        print("Error: Either --prompt or --prompt-file must be specified")
        sys.exit(1)</p>
<pre class="codehilite"><code># Query the LLM
response = openai.ChatCompletion.create(
    model=args.model,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
    temperature=args.temperature
)

# Process the response
content = response.choices[0].message.content

# Output handling
if args.output:
    with open(args.output, 'w') as f:
        f.write(content)
    print(f&quot;Response saved to {args.output}&quot;)
else:
    print(&quot;\n--- Response ---\n&quot;)
    print(content)
    print(&quot;\n---------------\n&quot;)

# Print usage statistics
print(f&quot;Token usage: {response.usage.total_tokens} tokens&quot;)
print(f&quot;  - Prompt: {response.usage.prompt_tokens} tokens&quot;)
print(f&quot;  - Completion: {response.usage.completion_tokens} tokens&quot;)
</code></pre>

<h1>Additional handler functions omitted for brevity</h1>
<p>if <strong>name</strong> == "<strong>main</strong>":
    main()
```</p>
<h3>6.4.2 Integrating with VS Code Extensions</h3>
<p>For VS Code integration, consider creating a simple extension that enables developers to interact with LLMs directly from their editor.</p>
<p>Key features might include:
- Prompt templates accessible via snippets
- Highlighted code selection to LLM processing
- Preview window for LLM responses
- Context-aware suggestions based on the current file</p>
<h2>6.5 Testing Frameworks for LLM-Powered Features</h2>
<h3>6.5.1 Unit Testing LLM Prompts</h3>
<p>Creating a testing framework for prompts:</p>
<p>```python
import unittest
from unittest.mock import patch
import json</p>
<p>class PromptTest(unittest.TestCase):
    """Base class for testing prompt templates and their expected responses"""</p>
<pre class="codehilite"><code>def setUp(self):
    # Set up mock for OpenAI API
    self.openai_patcher = patch('openai.ChatCompletion.create')
    self.mock_openai = self.openai_patcher.start()

def tearDown(self):
    self.openai_patcher.stop()

def assert_prompt_contains(self, prompt, required_elements):
    &quot;&quot;&quot;Assert that a prompt contains all required elements&quot;&quot;&quot;
    for element in required_elements:
        self.assertIn(element, prompt, f&quot;Prompt should contain '{element}'&quot;)

def assert_prompt_format(self, prompt, expected_format):
    &quot;&quot;&quot;Assert that a prompt follows the expected format structure&quot;&quot;&quot;
    # This is a simplified check - real implementation would be more sophisticated
    sections = expected_format.split(&quot;[section]&quot;)
    last_pos = 0

    for section in sections[1:]:  # Skip the first empty section
        section = section.strip()
        pos = prompt.find(section, last_pos)
        self.assertGreater(pos, -1, f&quot;Prompt missing expected section: '{section}'&quot;)
        last_pos = pos + len(section)

def mock_llm_response(self, response_content, usage=None):
    &quot;&quot;&quot;Helper to set up a mock LLM response&quot;&quot;&quot;
    if usage is None:
        usage = {&quot;prompt_tokens&quot;: 10, &quot;completion_tokens&quot;: 20, &quot;total_tokens&quot;: 30}

    # Create a response object structure similar to OpenAI's
    response = type('obj', (object,), {
        'choices': [
            type('obj', (object,), {
                'message': type('obj', (object,), {'content': response_content}),
                'finish_reason': 'stop'
            })
        ],
        'usage': type('obj', (object,), usage),
        'model': 'gpt-3.5-turbo'
    })

    self.mock_openai.return_value = response
</code></pre>

<h1>Example test class</h1>
<p>class TestCodeGenerationPrompts(PromptTest):
    def test_python_function_prompt(self):
        from my_prompt_lib import get_function_generation_prompt</p>
<pre class="codehilite"><code>    # Test specific prompt generation
    prompt = get_function_generation_prompt(
        language=&quot;python&quot;,
        function_name=&quot;calculate_discount&quot;,
        description=&quot;Calculate the final price after applying a discount percentage&quot;,
        parameters=[&quot;price&quot;, &quot;discount_percentage&quot;]
    )

    # Verify prompt structure
    self.assert_prompt_contains(prompt, [&quot;python&quot;, &quot;calculate_discount&quot;, &quot;price&quot;, &quot;discount_percentage&quot;])
    self.assert_prompt_format(prompt, &quot;[section]Task[section]Parameters[section]Requirements&quot;)

    # Mock the LLM response
    expected_code = &quot;def calculate_discount(price, discount_percentage):\n    return price * (1 - discount_percentage / 100)&quot;
    self.mock_llm_response(expected_code)

    # Test the full flow from prompt to response
    from my_llm_service import generate_code
    response = generate_code(prompt)

    # Verify the response handling
    self.assertEqual(response, expected_code)
</code></pre>

<p>```</p>
<h3>6.5.2 Integration Testing for LLM Applications</h3>
<p>For integration tests:</p>
<p>```python
class LLMIntegrationTest(unittest.TestCase):
    """Base class for integration testing of LLM-powered features"""</p>
<pre class="codehilite"><code>def setUp(self):
    # Real API calls but with a special test API key
    # Could use a staging/test environment for the API
    import os
    os.environ[&quot;OPENAI_API_KEY&quot;] = os.environ.get(&quot;OPENAI_TEST_API_KEY&quot;)

    # Set lower temperature for more consistent results in tests
    self.default_test_params = {
        &quot;temperature&quot;: 0.0,
        &quot;max_tokens&quot;: 100  # Limit tokens for faster tests
    }

def assert_response_matches_criteria(self, response, criteria):
    &quot;&quot;&quot;Assert that an LLM response meets a set of criteria&quot;&quot;&quot;
    for criterion, expected in criteria.items():
        if criterion == &quot;contains&quot;:
            for phrase in expected:
                self.assertIn(phrase, response, f&quot;Response should contain '{phrase}'&quot;)
        elif criterion == &quot;excludes&quot;:
            for phrase in expected:
                self.assertNotIn(phrase, response, f&quot;Response should not contain '{phrase}'&quot;)
        elif criterion == &quot;length_range&quot;:
            min_len, max_len = expected
            self.assertTrue(min_len &lt;= len(response) &lt;= max_len, 
                           f&quot;Response length {len(response)} outside range {min_len}-{max_len}&quot;)
        # Add more criteria types as needed
</code></pre>

<p>```</p>
<h2>6.6 Cost Optimization Techniques</h2>
<h3>6.6.1 Token Counting and Budget Management</h3>
<p>Implement a token budget manager:</p>
<p>```python
import tiktoken</p>
<p>class TokenBudgetManager:
    """Manages token usage and budgets for LLM applications"""</p>
<pre class="codehilite"><code>def __init__(self, model_name=&quot;gpt-3.5-turbo&quot;, monthly_budget=None):
    self.model_name = model_name
    self.monthly_budget = monthly_budget
    self.encoding = tiktoken.encoding_for_model(model_name)

    # Cost per 1K tokens (adjust based on current pricing)
    self.cost_per_1k_tokens = {
        &quot;gpt-3.5-turbo&quot;: {&quot;input&quot;: 0.0015, &quot;output&quot;: 0.002},
        &quot;gpt-4&quot;: {&quot;input&quot;: 0.03, &quot;output&quot;: 0.06}
    }.get(model_name, {&quot;input&quot;: 0.0015, &quot;output&quot;: 0.002})

    self.current_usage = {
        &quot;input_tokens&quot;: 0,
        &quot;output_tokens&quot;: 0,
        &quot;total_cost&quot;: 0.0
    }

def count_tokens(self, text):
    &quot;&quot;&quot;Count the number of tokens in a text string&quot;&quot;&quot;
    if not text:
        return 0
    token_ids = self.encoding.encode(text)
    return len(token_ids)

def estimate_cost(self, input_text, estimated_output_length=None):
    &quot;&quot;&quot;Estimate the cost of an LLM request&quot;&quot;&quot;
    input_tokens = self.count_tokens(input_text)

    # If output length not provided, estimate based on input length
    if estimated_output_length is None:
        estimated_output_tokens = input_tokens * 1.5  # Simple heuristic
    else:
        estimated_output_tokens = self.count_tokens(estimated_output_length)

    input_cost = (input_tokens / 1000) * self.cost_per_1k_tokens[&quot;input&quot;]
    output_cost = (estimated_output_tokens / 1000) * self.cost_per_1k_tokens[&quot;output&quot;]

    return {
        &quot;input_tokens&quot;: input_tokens,
        &quot;estimated_output_tokens&quot;: estimated_output_tokens,
        &quot;input_cost&quot;: input_cost,
        &quot;estimated_output_cost&quot;: output_cost,
        &quot;total_estimated_cost&quot;: input_cost + output_cost
    }

def track_usage(self, input_text, output_text):
    &quot;&quot;&quot;Track actual token usage and cost&quot;&quot;&quot;
    input_tokens = self.count_tokens(input_text)
    output_tokens = self.count_tokens(output_text)

    input_cost = (input_tokens / 1000) * self.cost_per_1k_tokens[&quot;input&quot;]
    output_cost = (output_tokens / 1000) * self.cost_per_1k_tokens[&quot;output&quot;]
    total_cost = input_cost + output_cost

    # Update running totals
    self.current_usage[&quot;input_tokens&quot;] += input_tokens
    self.current_usage[&quot;output_tokens&quot;] += output_tokens
    self.current_usage[&quot;total_cost&quot;] += total_cost

    return {
        &quot;input_tokens&quot;: input_tokens,
        &quot;output_tokens&quot;: output_tokens,
        &quot;input_cost&quot;: input_cost,
        &quot;output_cost&quot;: output_cost,
        &quot;total_cost&quot;: total_cost,
        &quot;running_totals&quot;: self.current_usage.copy()
    }

def check_budget_status(self):
    &quot;&quot;&quot;Check status against monthly budget&quot;&quot;&quot;
    if self.monthly_budget is None:
        return {&quot;has_budget&quot;: False}

    remaining_budget = self.monthly_budget - self.current_usage[&quot;total_cost&quot;]
    usage_percentage = (self.current_usage[&quot;total_cost&quot;] / self.monthly_budget) * 100

    return {
        &quot;has_budget&quot;: True,
        &quot;monthly_budget&quot;: self.monthly_budget,
        &quot;current_usage&quot;: self.current_usage[&quot;total_cost&quot;],
        &quot;remaining_budget&quot;: remaining_budget,
        &quot;usage_percentage&quot;: usage_percentage,
        &quot;status&quot;: &quot;OK&quot; if usage_percentage &lt; 90 else &quot;WARNING&quot; if usage_percentage &lt; 100 else &quot;EXCEEDED&quot;
    }
</code></pre>

<p>```</p>
<h3>6.6.2 Implementing Smart Caching</h3>
<p>We've already covered a basic caching implementation earlier. Here's an enhancement with smarter invalidation strategies:</p>
<p>```python
class SemanticCache:
    """A cache that uses semantic similarity to match similar prompts"""</p>
<pre class="codehilite"><code>def __init__(self, embedding_model=&quot;text-embedding-ada-002&quot;, similarity_threshold=0.95):
    self.cache = {}  # Maps embedding hash to (response, original_prompt)
    self.embedding_model = embedding_model
    self.similarity_threshold = similarity_threshold

def _get_embedding(self, text):
    &quot;&quot;&quot;Get embedding vector for text&quot;&quot;&quot;
    response = openai.Embedding.create(
        model=self.embedding_model,
        input=text
    )
    return response[&quot;data&quot;][0][&quot;embedding&quot;]

def _compute_similarity(self, embedding1, embedding2):
    &quot;&quot;&quot;Compute cosine similarity between embeddings&quot;&quot;&quot;
    import numpy as np

    # Convert to numpy arrays for vector operations
    vec1 = np.array(embedding1)
    vec2 = np.array(embedding2)

    # Compute cosine similarity
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)

    return dot_product / (norm1 * norm2)

def get(self, prompt, model):
    &quot;&quot;&quot;Try to retrieve a cached response based on semantic similarity&quot;&quot;&quot;
    try:
        prompt_embedding = self._get_embedding(prompt)

        best_match = None
        highest_similarity = 0

        for cache_key, (cached_response, original_prompt, cached_model) in self.cache.items():
            # Skip if models don't match
            if model != cached_model:
                continue

            # Calculate similarity with the cached prompt
            similarity = self._compute_similarity(prompt_embedding, cache_key)

            if similarity &gt; highest_similarity:
                highest_similarity = similarity
                best_match = (cached_response, original_prompt, similarity)

        # Return the best match if it meets the threshold
        if best_match and highest_similarity &gt;= self.similarity_threshold:
            return {&quot;response&quot;: best_match[0], 
                    &quot;original_prompt&quot;: best_match[1],
                    &quot;similarity&quot;: highest_similarity}

        return None

    except Exception as e:
        print(f&quot;Error in semantic cache: {e}&quot;)
        return None

def set(self, prompt, response, model):
    &quot;&quot;&quot;Store a response in the cache&quot;&quot;&quot;
    try:
        prompt_embedding = self._get_embedding(prompt)
        # Use the embedding vector as key
        embedding_key = tuple(prompt_embedding)  # Convert to tuple so it's hashable
        self.cache[embedding_key] = (response, prompt, model)
    except Exception as e:
        print(f&quot;Error setting semantic cache: {e}&quot;)
</code></pre>

<p>```</p>
<h2>6.7 Conclusion</h2>
<p>In this chapter, we've explored various tools and techniques for building effective developer tooling for LLM applications. From prompt libraries and reuse patterns to debugging tools, performance optimization, testing frameworks, and cost management, we've covered the essential components needed to develop robust LLM-powered applications.</p>
<p>As you implement these tools in your projects, remember that the field of prompt engineering is rapidly evolving. Stay flexible and be prepared to adapt your tools and approaches as new best practices emerge. In the next chapter, we'll put these tools into practice with a hands-on project building a Smart Code Assistant.</p>
<h2>6.8 Further Reading</h2>
<ul>
<li>"Design Patterns for LLM Applications" by Various Authors</li>
<li>"Efficient Natural Language Processing" by Various Authors</li>
<li>"Software Engineering for AI-Powered Systems" by Various Authors</li>
<li>OpenAI API Documentation</li>
<li>LangChain and LlamaIndex Documentation for advanced tooling options</li>
</ul>
</div>

<div class="chapter-container">
    <div class="chapter-header">
        <div class="chapter-page-number">Page 101</div>
    </div>
    <h1>Chapter 7: Hands-on Project 1: Building a Smart Code Assistant</h1>
<p>In the previous chapters, we've explored various prompt engineering techniques, patterns, and tools for developing LLM-powered applications. Now it's time to put that knowledge into practice by building a practical tool that can help streamline your daily coding tasks. In this chapter, we'll create a Smart Code Assistant that leverages LLMs to automate common coding tasks.</p>
<h2>7.1 Project Overview</h2>
<h3>7.1.1 Problem Statement</h3>
<p>Software development involves numerous repetitive tasks that consume valuable time and mental energy:</p>
<ul>
<li>Writing boilerplate code for new classes, functions, or modules</li>
<li>Creating comprehensive documentation for existing code</li>
<li>Refactoring code for better readability or performance</li>
<li>Understanding unfamiliar code or complex algorithms</li>
<li>Writing unit tests for existing code</li>
<li>Converting code between different programming languages</li>
</ul>
<p>While integrated development environments (IDEs) offer some assistance, they often lack the flexibility and contextual understanding that LLMs can provide.</p>
<h3>7.1.2 Solution: The Smart Code Assistant</h3>
<p>We'll build a Python-based Smart Code Assistant that leverages the power of LLMs to:</p>
<ol>
<li>Generate boilerplate code based on natural language specifications</li>
<li>Analyze and explain existing code</li>
<li>Suggest refactoring improvements for small functions</li>
<li>Generate unit tests for given functions</li>
<li>Assist with code documentation</li>
<li>Provide language conversion between Python, JavaScript, and Java</li>
</ol>
<h3>7.1.3 Technical Requirements</h3>
<ul>
<li>Python 3.8+ environment</li>
<li>OpenAI API key (or equivalent for another LLM provider)</li>
<li>Command-line interface for easy integration with existing workflows</li>
<li>Simple, modular architecture for future extensions</li>
<li>Option to save results to files or copy to clipboard</li>
</ul>
<h2>7.2 Setting Up the Project</h2>
<p>Let's begin by setting up our project structure and installing the necessary dependencies.</p>
<h3>7.2.1 Project Structure</h3>
<p><code>smart_code_assistant/
 __init__.py
 main.py              # Entry point for the command-line interface
 code_assistant.py    # Core functionality
 prompt_library.py    # Prompt templates
 utils/
    __init__.py
    clipboard.py     # Clipboard utilities
    file_utils.py    # File handling utilities
    token_counter.py # Token counting utilities
 config.py            # Configuration handling
 requirements.txt     # Project dependencies
 README.md            # Project documentation</code></p>
<h3>7.2.2 Installing Dependencies</h3>
<p>Create a <code>requirements.txt</code> file with the following dependencies:</p>
<p><code>openai&gt;=1.0.0
typer&gt;=0.9.0
rich&gt;=13.5.0
pyperclip&gt;=1.8.2
tiktoken&gt;=0.5.0
python-dotenv&gt;=1.0.0</code></p>
<p>Install these dependencies using pip:</p>
<p><code>bash
pip install -r requirements.txt</code></p>
<h3>7.2.3 Configuration Setup</h3>
<p>Create a <code>config.py</code> file to handle API keys and other settings:</p>
<p>```python
import os
import dotenv
from pathlib import Path</p>
<h1>Load environment variables from .env file</h1>
<p>dotenv.load_dotenv()</p>
<h1>API configuration</h1>
<p>OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
DEFAULT_MODEL = os.getenv("DEFAULT_MODEL", "gpt-3.5-turbo")
MAX_TOKENS = int(os.getenv("MAX_TOKENS", "2048"))
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.7"))</p>
<h1>Application paths</h1>
<p>APP_DIR = Path.home() / ".smart_code_assistant"
CACHE_DIR = APP_DIR / "cache"
OUTPUT_DIR = APP_DIR / "output"</p>
<h1>Ensure directories exist</h1>
<p>APP_DIR.mkdir(exist_ok=True)
CACHE_DIR.mkdir(exist_ok=True)
OUTPUT_DIR.mkdir(exist_ok=True)</p>
<h1>Default languages supported</h1>
<p>SUPPORTED_LANGUAGES = [
    "python", "javascript", "typescript", "java", "c++", "csharp", "go", "rust"
]
```</p>
<p>Create a <code>.env</code> file in the project root to store your OpenAI API key:</p>
<p><code>OPENAI_API_KEY=your_api_key_here
DEFAULT_MODEL=gpt-3.5-turbo</code></p>
<h2>7.3 Building the Core Components</h2>
<h3>7.3.1 Prompt Library</h3>
<p>First, let's create our prompt library with templates for different coding tasks. Create a <code>prompt_library.py</code> file:</p>
<p>```python
class PromptTemplate:
    def <strong>init</strong>(self, template, required_params=None):
        self.template = template
        self.required_params = required_params or []</p>
<pre class="codehilite"><code>def format(self, **kwargs):
    # Ensure all required parameters are provided
    missing = [param for param in self.required_params if param not in kwargs]
    if missing:
        raise ValueError(f&quot;Missing required parameters: {', '.join(missing)}&quot;)

    # Format the template with the provided parameters
    return self.template.format(**kwargs)
</code></pre>

<p>class CodePromptLibrary:
    def <strong>init</strong>(self):
        self.prompts = {}
        self._initialize_prompts()</p>
<pre class="codehilite"><code>def _initialize_prompts(self):
    # Code generation prompts
    self.prompts[&quot;generate_function&quot;] = PromptTemplate(
        &quot;&quot;&quot;You are an expert software developer. Write a {language} function that {description}.
</code></pre>

<p>Requirements:
{requirements}</p>
<p>Your function should be well-documented with comments explaining the logic.
Only return the code with no additional explanations.
""",
            ["language", "description", "requirements"]
        )</p>
<pre class="codehilite"><code>    # Code explanation prompts
    self.prompts[&quot;explain_code&quot;] = PromptTemplate(
        &quot;&quot;&quot;Explain the following {language} code in detail:
</code></pre>

<p><code>{language}
{code}</code></p>
<p>Include in your explanation:
1. What the code does
2. The key components and their purpose
3. Any algorithms or patterns used
4. Potential edge cases or limitations
""",
            ["language", "code"]
        )</p>
<pre class="codehilite"><code>    # Refactoring prompts
    self.prompts[&quot;refactor_code&quot;] = PromptTemplate(
        &quot;&quot;&quot;Refactor the following {language} code to improve its {focus}:
</code></pre>

<p><code>{language}
{code}</code></p>
<p>Provide the refactored code and explain what improvements you made.
Focus specifically on improving {focus} while maintaining the same functionality.
""",
            ["language", "code", "focus"]
        )</p>
<pre class="codehilite"><code>    # Unit test generation prompts
    self.prompts[&quot;generate_tests&quot;] = PromptTemplate(
        &quot;&quot;&quot;Write comprehensive unit tests for the following {language} function:
</code></pre>

<p><code>{language}
{code}</code></p>
<p>The tests should:
1. Cover normal cases, edge cases, and potential errors
2. Be well-structured and properly named
3. Use {test_framework} as the testing framework
4. Include comments explaining the purpose of each test case
""",
            ["language", "code", "test_framework"]
        )</p>
<pre class="codehilite"><code>    # Documentation generation prompts
    self.prompts[&quot;generate_docs&quot;] = PromptTemplate(
        &quot;&quot;&quot;Generate comprehensive documentation for the following {language} code:
</code></pre>

<p><code>{language}
{code}</code></p>
<p>The documentation should:
1. Follow {doc_style} documentation style
2. Include parameter descriptions, return values, and exceptions
3. Provide a clear overview of what the code does and how to use it
4. Include usage examples where appropriate
""",
            ["language", "code", "doc_style"]
        )</p>
<pre class="codehilite"><code>    # Code conversion prompts
    self.prompts[&quot;convert_code&quot;] = PromptTemplate(
        &quot;&quot;&quot;Convert the following {source_language} code to {target_language} while maintaining the same functionality:
</code></pre>

<p><code>{source_language}
{code}</code></p>
<p>Ensure the converted code:
1. Follows the idiomatic conventions of {target_language}
2. Preserves the original functionality and logic
3. Includes equivalent error handling
4. Is well-commented to explain any non-trivial conversions
""",
            ["source_language", "target_language", "code"]
        )</p>
<pre class="codehilite"><code>def get_prompt(self, prompt_name, **kwargs):
    &quot;&quot;&quot;Get a formatted prompt by name with the provided parameters&quot;&quot;&quot;
    if prompt_name not in self.prompts:
        raise ValueError(f&quot;Unknown prompt: {prompt_name}&quot;)

    return self.prompts[prompt_name].format(**kwargs)
</code></pre>

<p>```</p>
<h3>7.3.2 Core Code Assistant Implementation</h3>
<p>Now, let's create the core <code>code_assistant.py</code> file that will handle interactions with the LLM:</p>
<p>```python
import openai
import tiktoken
import json
import time
from pathlib import Path</p>
<p>import config
from prompt_library import CodePromptLibrary</p>
<p>class SmartCodeAssistant:
    def <strong>init</strong>(self, api_key=None, model=None):
        """Initialize the Smart Code Assistant"""
        self.api_key = api_key or config.OPENAI_API_KEY
        self.model = model or config.DEFAULT_MODEL
        self.prompt_library = CodePromptLibrary()</p>
<pre class="codehilite"><code>    # Configure OpenAI client
    openai.api_key = self.api_key

    # Initialize tokenizer for token counting
    self.tokenizer = tiktoken.encoding_for_model(self.model)

def _send_request(self, prompt, temperature=None, max_tokens=None):
    &quot;&quot;&quot;Send a request to the OpenAI API&quot;&quot;&quot;
    temperature = temperature if temperature is not None else config.TEMPERATURE
    max_tokens = max_tokens if max_tokens is not None else config.MAX_TOKENS

    try:
        response = openai.ChatCompletion.create(
            model=self.model,
            messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
            temperature=temperature,
            max_tokens=max_tokens
        )
        return response.choices[0].message.content
    except Exception as e:
        raise Exception(f&quot;Error calling OpenAI API: {str(e)}&quot;)

def count_tokens(self, text):
    &quot;&quot;&quot;Count the number of tokens in the given text&quot;&quot;&quot;
    return len(self.tokenizer.encode(text))

def generate_function(self, description, language=&quot;python&quot;, requirements=&quot;&quot;):
    &quot;&quot;&quot;Generate code based on a natural language description&quot;&quot;&quot;
    prompt = self.prompt_library.get_prompt(
        &quot;generate_function&quot;,
        language=language,
        description=description,
        requirements=requirements
    )
    return self._send_request(prompt, temperature=0.2)

def explain_code(self, code, language=&quot;python&quot;):
    &quot;&quot;&quot;Explain the given code in detail&quot;&quot;&quot;
    prompt = self.prompt_library.get_prompt(
        &quot;explain_code&quot;,
        language=language,
        code=code
    )
    return self._send_request(prompt)

def refactor_code(self, code, focus=&quot;readability&quot;, language=&quot;python&quot;):
    &quot;&quot;&quot;Refactor code to improve a specific aspect&quot;&quot;&quot;
    prompt = self.prompt_library.get_prompt(
        &quot;refactor_code&quot;,
        language=language,
        code=code,
        focus=focus
    )
    return self._send_request(prompt)

def generate_tests(self, code, language=&quot;python&quot;, test_framework=&quot;pytest&quot;):
    &quot;&quot;&quot;Generate unit tests for the given code&quot;&quot;&quot;
    prompt = self.prompt_library.get_prompt(
        &quot;generate_tests&quot;,
        language=language,
        code=code,
        test_framework=test_framework
    )
    return self._send_request(prompt)

def generate_docs(self, code, language=&quot;python&quot;, doc_style=&quot;Google&quot;):
    &quot;&quot;&quot;Generate documentation for the given code&quot;&quot;&quot;
    prompt = self.prompt_library.get_prompt(
        &quot;generate_docs&quot;,
        language=language,
        code=code,
        doc_style=doc_style
    )
    return self._send_request(prompt)

def convert_code(self, code, source_language=&quot;python&quot;, target_language=&quot;javascript&quot;):
    &quot;&quot;&quot;Convert code from one language to another&quot;&quot;&quot;
    prompt = self.prompt_library.get_prompt(
        &quot;convert_code&quot;,
        source_language=source_language,
        target_language=target_language,
        code=code
    )
    return self._send_request(prompt)
</code></pre>

<p>```</p>
<h3>7.3.3 Utility Functions</h3>
<p>Create a utility module for file operations and clipboard interaction. First, create the <code>utils/file_utils.py</code>:</p>
<p>```python
import os
from pathlib import Path</p>
<p>def read_file(file_path):
    """Read content from a file"""
    path = Path(file_path)
    if not path.exists():
        raise FileNotFoundError(f"File not found: {file_path}")</p>
<pre class="codehilite"><code>with open(path, 'r', encoding='utf-8') as file:
    return file.read()
</code></pre>

<p>def write_file(file_path, content):
    """Write content to a file"""
    path = Path(file_path)</p>
<pre class="codehilite"><code># Create directories if they don't exist
path.parent.mkdir(parents=True, exist_ok=True)

with open(path, 'w', encoding='utf-8') as file:
    file.write(content)

return path
</code></pre>

<p>def get_language_from_extension(file_path):
    """Determine language from file extension"""
    extension_map = {
        '.py': 'python',
        '.js': 'javascript',
        '.ts': 'typescript',
        '.java': 'java',
        '.cpp': 'c++',
        '.cc': 'c++',
        '.c': 'c',
        '.h': 'c',
        '.hpp': 'c++',
        '.cs': 'csharp',
        '.go': 'go',
        '.rs': 'rust',
        '.rb': 'ruby',
        '.php': 'php',
        '.swift': 'swift',
        '.kt': 'kotlin'
    }</p>
<pre class="codehilite"><code>extension = Path(file_path).suffix.lower()
return extension_map.get(extension, 'text')
</code></pre>

<p>```</p>
<p>Now, create <code>utils/clipboard.py</code>:</p>
<p>```python
import pyperclip</p>
<p>def copy_to_clipboard(text):
    """Copy text to clipboard"""
    try:
        pyperclip.copy(text)
        return True
    except Exception as e:
        print(f"Failed to copy to clipboard: {e}")
        return False</p>
<p>def paste_from_clipboard():
    """Paste text from clipboard"""
    try:
        return pyperclip.paste()
    except Exception as e:
        print(f"Failed to paste from clipboard: {e}")
        return ""
```</p>
<p>Create <code>utils/token_counter.py</code>:</p>
<p>```python
import tiktoken
import config</p>
<p>def count_tokens(text, model=None):
    """Count tokens in the given text"""
    model = model or config.DEFAULT_MODEL</p>
<pre class="codehilite"><code>try:
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))
except Exception as e:
    # Fallback to approximate token count if encoding fails
    return len(text) // 4  # Rough approximation: ~4 characters per token
</code></pre>

<p>```</p>
<p>Create an empty <code>__init__.py</code> in the utils directory to make it a proper package:</p>
<p>```python</p>
<h1>This file makes the utils directory a Python package</h1>
<p>```</p>
<h2>7.4 Building the Command-Line Interface</h2>
<p>Let's create a command-line interface using Typer to make our tool easily accessible. Create the <code>main.py</code> file:</p>
<p>```python
import typer
import sys
from pathlib import Path
from typing import Optional, List
from rich.console import Console
from rich.syntax import Syntax</p>
<p>import config
from code_assistant import SmartCodeAssistant
from utils.file_utils import read_file, write_file, get_language_from_extension
from utils.clipboard import copy_to_clipboard, paste_from_clipboard
from utils.token_counter import count_tokens</p>
<h1>Initialize Typer app and Rich console</h1>
<p>app = typer.Typer(help="Smart Code Assistant - Your AI-powered coding companion")
console = Console()</p>
<h1>Initialize code assistant</h1>
<p>assistant = SmartCodeAssistant()</p>
<p>def print_code(code, language):
    """Print code with syntax highlighting"""
    syntax = Syntax(code, language, theme="monokai", line_numbers=True)
    console.print(syntax)</p>
<p>@app.command("generate")
def generate_function(
    description: str = typer.Argument(..., help="Description of the function to generate"),
    language: str = typer.Option("python", "--language", "-l", help="Programming language"),
    requirements: str = typer.Option("", "--requirements", "-r", help="Additional requirements"),
    output_file: Optional[Path] = typer.Option(None, "--output", "-o", help="Output file path"),
    copy: bool = typer.Option(False, "--copy", "-c", help="Copy result to clipboard")
):
    """Generate code based on a natural language description"""
    console.print(f"[bold blue]Generating {language} code for:[/bold blue] {description}")</p>
<pre class="codehilite"><code>try:
    result = assistant.generate_function(description, language, requirements)

    # Print the result
    print_code(result, language)

    # Save to file if requested
    if output_file:
        write_file(output_file, result)
        console.print(f&quot;[green]Code saved to:[/green] {output_file}&quot;)

    # Copy to clipboard if requested
    if copy:
        copy_to_clipboard(result)
        console.print(&quot;[green]Code copied to clipboard![/green]&quot;)

except Exception as e:
    console.print(f&quot;[bold red]Error:[/bold red] {str(e)}&quot;)
    raise typer.Exit(code=1)
</code></pre>

<p>@app.command("explain")
def explain_code(
    file: Optional[Path] = typer.Option(None, "--file", "-f", help="File containing code to explain"),
    language: Optional[str] = typer.Option(None, "--language", "-l", help="Programming language"),
    from_clipboard: bool = typer.Option(False, "--clipboard", "-c", help="Read code from clipboard"),
    output_file: Optional[Path] = typer.Option(None, "--output", "-o", help="Output file path")
):
    """Explain code in detail"""
    # Get the code from file or clipboard
    if file:
        code = read_file(file)
        language = language or get_language_from_extension(file)
    elif from_clipboard:
        code = paste_from_clipboard()
        if not code:
            console.print("[bold red]No code found in clipboard![/bold red]")
            raise typer.Exit(code=1)
    else:
        # Interactive mode - read from stdin
        console.print("[bold blue]Enter code to explain (Ctrl+D to finish):[/bold blue]")
        code = sys.stdin.read().strip()
        if not code:
            console.print("[bold red]No code provided![/bold red]")
            raise typer.Exit(code=1)</p>
<pre class="codehilite"><code>language = language or &quot;python&quot;  # Default to Python if not specified

console.print(f&quot;[bold blue]Explaining {language} code...[/bold blue]&quot;)

try:
    explanation = assistant.explain_code(code, language)

    # Print the explanation
    console.print(&quot;[bold green]Explanation:[/bold green]&quot;)
    console.print(explanation)

    # Save to file if requested
    if output_file:
        write_file(output_file, explanation)
        console.print(f&quot;[green]Explanation saved to:[/green] {output_file}&quot;)

except Exception as e:
    console.print(f&quot;[bold red]Error:[/bold red] {str(e)}&quot;)
    raise typer.Exit(code=1)
</code></pre>

<p>@app.command("refactor")
def refactor_code(
    file: Optional[Path] = typer.Option(None, "--file", "-f", help="File containing code to refactor"),
    language: Optional[str] = typer.Option(None, "--language", "-l", help="Programming language"),
    focus: str = typer.Option("readability", "--focus", help="What to focus on improving (e.g., readability, performance)"),
    from_clipboard: bool = typer.Option(False, "--clipboard", "-c", help="Read code from clipboard"),
    output_file: Optional[Path] = typer.Option(None, "--output", "-o", help="Output file path"),
    copy: bool = typer.Option(False, "--copy", help="Copy result to clipboard")
):
    """Refactor code to improve a specific aspect"""
    # Get the code from file or clipboard
    if file:
        code = read_file(file)
        language = language or get_language_from_extension(file)
    elif from_clipboard:
        code = paste_from_clipboard()
        if not code:
            console.print("[bold red]No code found in clipboard![/bold red]")
            raise typer.Exit(code=1)
    else:
        # Interactive mode - read from stdin
        console.print(f"[bold blue]Enter code to refactor (focus: {focus}, Ctrl+D to finish):[/bold blue]")
        code = sys.stdin.read().strip()
        if not code:
            console.print("[bold red]No code provided![/bold red]")
            raise typer.Exit(code=1)</p>
<pre class="codehilite"><code>language = language or &quot;python&quot;  # Default to Python if not specified

console.print(f&quot;[bold blue]Refactoring {language} code to improve {focus}...[/bold blue]&quot;)

try:
    refactored = assistant.refactor_code(code, focus, language)

    # Print the refactored code
    console.print(&quot;[bold green]Refactored code:[/bold green]&quot;)
    print_code(refactored, language)

    # Save to file if requested
    if output_file:
        write_file(output_file, refactored)
        console.print(f&quot;[green]Refactored code saved to:[/green] {output_file}&quot;)

    # Copy to clipboard if requested
    if copy:
        copy_to_clipboard(refactored)
        console.print(&quot;[green]Refactored code copied to clipboard![/green]&quot;)

except Exception as e:
    console.print(f&quot;[bold red]Error:[/bold red] {str(e)}&quot;)
    raise typer.Exit(code=1)
</code></pre>

<p>@app.command("test")
def generate_tests(
    file: Optional[Path] = typer.Option(None, "--file", "-f", help="File containing code to test"),
    language: Optional[str] = typer.Option(None, "--language", "-l", help="Programming language"),
    framework: str = typer.Option("pytest", "--framework", help="Testing framework to use"),
    from_clipboard: bool = typer.Option(False, "--clipboard", "-c", help="Read code from clipboard"),
    output_file: Optional[Path] = typer.Option(None, "--output", "-o", help="Output file path")
):
    """Generate unit tests for code"""
    # Get the code from file or clipboard
    if file:
        code = read_file(file)
        language = language or get_language_from_extension(file)
    elif from_clipboard:
        code = paste_from_clipboard()
        if not code:
            console.print("[bold red]No code found in clipboard![/bold red]")
            raise typer.Exit(code=1)
    else:
        # Interactive mode - read from stdin
        console.print(f"[bold blue]Enter code to generate tests for (using {framework}, Ctrl+D to finish):[/bold blue]")
        code = sys.stdin.read().strip()
        if not code:
            console.print("[bold red]No code provided![/bold red]")
            raise typer.Exit(code=1)</p>
<pre class="codehilite"><code>language = language or &quot;python&quot;  # Default to Python if not specified

console.print(f&quot;[bold blue]Generating {framework} tests for {language} code...[/bold blue]&quot;)

try:
    tests = assistant.generate_tests(code, language, framework)

    # Print the tests
    console.print(&quot;[bold green]Generated tests:[/bold green]&quot;)
    print_code(tests, language)

    # Save to file if requested
    if output_file:
        write_file(output_file, tests)
        console.print(f&quot;[green]Tests saved to:[/green] {output_file}&quot;)

except Exception as e:
    console.print(f&quot;[bold red]Error:[/bold red] {str(e)}&quot;)
    raise typer.Exit(code=1)
</code></pre>

<p>@app.command("docs")
def generate_docs(
    file: Optional[Path] = typer.Option(None, "--file", "-f", help="File containing code to document"),
    language: Optional[str] = typer.Option(None, "--language", "-l", help="Programming language"),
    style: str = typer.Option("Google", "--style", help="Documentation style (Google, NumPy, JSDoc, etc.)"),
    from_clipboard: bool = typer.Option(False, "--clipboard", "-c", help="Read code from clipboard"),
    output_file: Optional[Path] = typer.Option(None, "--output", "-o", help="Output file path")
):
    """Generate documentation for code"""
    # Get the code from file or clipboard
    if file:
        code = read_file(file)
        language = language or get_language_from_extension(file)
    elif from_clipboard:
        code = paste_from_clipboard()
        if not code:
            console.print("[bold red]No code found in clipboard![/bold red]")
            raise typer.Exit(code=1)
    else:
        # Interactive mode - read from stdin
        console.print(f"[bold blue]Enter code to document (using {style} style, Ctrl+D to finish):[/bold blue]")
        code = sys.stdin.read().strip()
        if not code:
            console.print("[bold red]No code provided![/bold red]")
            raise typer.Exit(code=1)</p>
<pre class="codehilite"><code>language = language or &quot;python&quot;  # Default to Python if not specified

console.print(f&quot;[bold blue]Generating {style} documentation for {language} code...[/bold blue]&quot;)

try:
    docs = assistant.generate_docs(code, language, style)

    # Print the documentation
    console.print(&quot;[bold green]Generated documentation:[/bold green]&quot;)
    print_code(docs, language)

    # Save to file if requested
    if output_file:
        write_file(output_file, docs)
        console.print(f&quot;[green]Documentation saved to:[/green] {output_file}&quot;)

except Exception as e:
    console.print(f&quot;[bold red]Error:[/bold red] {str(e)}&quot;)
    raise typer.Exit(code=1)
</code></pre>

<p>@app.command("convert")
def convert_code(
    source_language: str = typer.Option(..., "--from", "-f", help="Source programming language"),
    target_language: str = typer.Option(..., "--to", "-t", help="Target programming language"),
    file: Optional[Path] = typer.Option(None, "--file", help="File containing code to convert"),
    from_clipboard: bool = typer.Option(False, "--clipboard", "-c", help="Read code from clipboard"),
    output_file: Optional[Path] = typer.Option(None, "--output", "-o", help="Output file path"),
    copy: bool = typer.Option(False, "--copy", help="Copy result to clipboard")
):
    """Convert code from one language to another"""
    # Get the code from file or clipboard
    if file:
        code = read_file(file)
    elif from_clipboard:
        code = paste_from_clipboard()
        if not code:
            console.print("[bold red]No code found in clipboard![/bold red]")
            raise typer.Exit(code=1)
    else:
        # Interactive mode - read from stdin
        console.print(f"[bold blue]Enter {source_language} code to convert to {target_language} (Ctrl+D to finish):[/bold blue]")
        code = sys.stdin.read().strip()
        if not code:
            console.print("[bold red]No code provided![/bold red]")
            raise typer.Exit(code=1)</p>
<pre class="codehilite"><code>console.print(f&quot;[bold blue]Converting code from {source_language} to {target_language}...[/bold blue]&quot;)

try:
    converted = assistant.convert_code(code, source_language, target_language)

    # Print the converted code
    console.print(&quot;[bold green]Converted code:[/bold green]&quot;)
    print_code(converted, target_language)

    # Save to file if requested
    if output_file:
        write_file(output_file, converted)
        console.print(f&quot;[green]Converted code saved to:[/green] {output_file}&quot;)

    # Copy to clipboard if requested
    if copy:
        copy_to_clipboard(converted)
        console.print(&quot;[green]Converted code copied to clipboard![/green]&quot;)

except Exception as e:
    console.print(f&quot;[bold red]Error:[/bold red] {str(e)}&quot;)
    raise typer.Exit(code=1)
</code></pre>

<p>@app.command("info")
def show_info():
    """Show information about the Smart Code Assistant"""
    console.print("[bold blue]Smart Code Assistant[/bold blue]")
    console.print("Your AI-powered coding companion")
    console.print("\n[bold green]Available commands:[/bold green]")
    console.print("  generate    - Generate code from a description")
    console.print("  explain     - Explain code in detail")
    console.print("  refactor    - Refactor code to improve specific aspects")
    console.print("  test        - Generate unit tests for code")
    console.print("  docs        - Generate documentation for code")
    console.print("  convert     - Convert code between languages")
    console.print("  info        - Show this information")</p>
<pre class="codehilite"><code>console.print(&quot;\n[bold green]Configuration:[/bold green]&quot;)
console.print(f&quot;  Model: {config.DEFAULT_MODEL}&quot;)
console.print(f&quot;  Max tokens: {config.MAX_TOKENS}&quot;)
console.print(f&quot;  Temperature: {config.TEMPERATURE}&quot;)
console.print(f&quot;  Supported languages: {', '.join(config.SUPPORTED_LANGUAGES)}&quot;)
</code></pre>

<p>if <strong>name</strong> == "<strong>main</strong>":
    app()
```</p>
<h2>7.5 Project Usage Examples</h2>
<p>Let's explore how to use our Smart Code Assistant for various tasks.</p>
<h3>7.5.1 Generate a Function</h3>
<p>```bash</p>
<h1>Generate a binary search function in Python</h1>
<p>python main.py generate "implement a binary search algorithm for a sorted list" --language python --requirements "Must handle edge cases like empty lists and include proper documentation"
```</p>
<h3>7.5.2 Explain Code</h3>
<p>```bash</p>
<h1>Explain code from a file</h1>
<p>python main.py explain --file complex_algorithm.py</p>
<h1>Explain code from clipboard</h1>
<p>python main.py explain --clipboard --language javascript
```</p>
<h3>7.5.3 Refactor Code</h3>
<p>```bash</p>
<h1>Refactor code from a file to improve performance</h1>
<p>python main.py refactor --file slow_function.py --focus performance --output improved_function.py</p>
<h1>Refactor code from clipboard to improve readability</h1>
<p>python main.py refactor --clipboard --focus readability --language python
```</p>
<h3>7.5.4 Generate Tests</h3>
<p>```bash</p>
<h1>Generate tests for a function in a file</h1>
<p>python main.py test --file my_function.py --framework pytest --output test_my_function.py</p>
<h1>Generate tests for code in clipboard</h1>
<p>python main.py test --clipboard --language javascript --framework jest
```</p>
<h3>7.5.5 Generate Documentation</h3>
<p>```bash</p>
<h1>Generate documentation for a file</h1>
<p>python main.py docs --file undocumented_code.py --style Google --output documented_code.py</p>
<h1>Generate documentation for code in clipboard</h1>
<p>python main.py docs --clipboard --language typescript --style JSDoc
```</p>
<h3>7.5.6 Convert Code Between Languages</h3>
<p>```bash</p>
<h1>Convert Python code to JavaScript</h1>
<p>python main.py convert --from python --to javascript --file algorithm.py --output algorithm.js</p>
<h1>Convert JavaScript code from clipboard to Python</h1>
<p>python main.py convert --from javascript --to python --clipboard --copy
```</p>
<h2>7.6 Enhancing the Smart Code Assistant</h2>
<p>Now that we have the core functionality in place, let's explore some ways to enhance our tool.</p>
<h3>7.6.1 Adding a Simple Caching Mechanism</h3>
<p>To avoid unnecessary API calls and reduce costs, let's implement a simple caching mechanism:</p>
<p>```python</p>
<h1>Add to code_assistant.py</h1>
<p>import hashlib
import json
import os
from pathlib import Path
import time</p>
<p>class SimpleCache:
    def <strong>init</strong>(self, cache_dir=None, ttl=3600):
        """Initialize the cache with a directory and time-to-live in seconds"""
        self.cache_dir = Path(cache_dir or config.CACHE_DIR)
        self.ttl = ttl
        self.cache_dir.mkdir(parents=True, exist_ok=True)</p>
<pre class="codehilite"><code>def _get_cache_key(self, prompt, model):
    &quot;&quot;&quot;Create a hash key from the prompt and model&quot;&quot;&quot;
    key_str = f&quot;{prompt}:{model}&quot;
    return hashlib.md5(key_str.encode()).hexdigest()

def _get_cache_path(self, key):
    &quot;&quot;&quot;Get the file path for a cache key&quot;&quot;&quot;
    return self.cache_dir / f&quot;{key}.json&quot;

def get(self, prompt, model):
    &quot;&quot;&quot;Get cached response if available and not expired&quot;&quot;&quot;
    key = self._get_cache_key(prompt, model)
    cache_path = self._get_cache_path(key)

    if not cache_path.exists():
        return None

    # Check if cache has expired
    if time.time() - cache_path.stat().st_mtime &gt; self.ttl:
        os.remove(cache_path)
        return None

    try:
        with open(cache_path, 'r', encoding='utf-8') as f:
            cache_data = json.load(f)
            return cache_data[&quot;response&quot;]
    except:
        return None

def set(self, prompt, model, response):
    &quot;&quot;&quot;Cache a response&quot;&quot;&quot;
    key = self._get_cache_key(prompt, model)
    cache_path = self._get_cache_path(key)

    cache_data = {
        &quot;prompt&quot;: prompt,
        &quot;model&quot;: model,
        &quot;response&quot;: response,
        &quot;timestamp&quot;: time.time()
    }

    with open(cache_path, 'w', encoding='utf-8') as f:
        json.dump(cache_data, f, ensure_ascii=False, indent=2)
</code></pre>

<p>```</p>
<p>Update the <code>SmartCodeAssistant</code> class to use the cache:</p>
<p>```python</p>
<h1>Update _send_request method in code_assistant.py</h1>
<p>def <strong>init</strong>(self, api_key=None, model=None, use_cache=True):
    # ... existing code ...
    self.use_cache = use_cache
    self.cache = SimpleCache() if use_cache else None</p>
<p>def _send_request(self, prompt, temperature=None, max_tokens=None):
    """Send a request to the OpenAI API with caching"""
    temperature = temperature if temperature is not None else config.TEMPERATURE
    max_tokens = max_tokens if max_tokens is not None else config.MAX_TOKENS</p>
<pre class="codehilite"><code># Try to get from cache if enabled
if self.use_cache:
    cached_response = self.cache.get(prompt, self.model)
    if cached_response:
        return cached_response

try:
    response = openai.ChatCompletion.create(
        model=self.model,
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
        temperature=temperature,
        max_tokens=max_tokens
    )
    content = response.choices[0].message.content

    # Store in cache if enabled
    if self.use_cache:
        self.cache.set(prompt, self.model, content)

    return content
except Exception as e:
    raise Exception(f&quot;Error calling OpenAI API: {str(e)}&quot;)
</code></pre>

<p>```</p>
<h3>7.6.2 Adding Progressive Enhancement with File Context</h3>
<p>Let's enhance our code assistant to consider surrounding file context when processing partial code:</p>
<p>```python</p>
<h1>Add to code_assistant.py</h1>
<p>def extract_file_context(self, file_path, target_lines=None, context_lines=5):
    """Extract context from a file around the target lines"""
    with open(file_path, 'r', encoding='utf-8') as f:
        all_lines = f.readlines()</p>
<pre class="codehilite"><code>if target_lines is None:
    return &quot;&quot;.join(all_lines)

# Convert target_lines to a range if it's a single number
if isinstance(target_lines, int):
    target_start = max(0, target_lines - 1)
    target_end = target_start + 1
else:
    target_start = max(0, target_lines[0] - 1)
    target_end = min(len(all_lines), target_lines[1])

# Extract the target code
target_code = &quot;&quot;.join(all_lines[target_start:target_end])

# Get context before target
context_before_start = max(0, target_start - context_lines)
context_before = &quot;&quot;.join(all_lines[context_before_start:target_start])

# Get context after target
context_after_end = min(len(all_lines), target_end + context_lines)
context_after = &quot;&quot;.join(all_lines[target_end:context_after_end])

# Compile everything with markers
result = &quot;&quot;
if context_before:
    result += &quot;/* Context before target code */\n&quot; + context_before

result += &quot;/* Target code */\n&quot; + target_code

if context_after:
    result += &quot;/* Context after target code */\n&quot; + context_after

return result
</code></pre>

<p>def refactor_with_context(self, code, file_path, target_lines, focus="readability", language=None):
    """Refactor code with surrounding file context"""
    if not language:
        language = get_language_from_extension(file_path)</p>
<pre class="codehilite"><code># Extract code with context
code_with_context = self.extract_file_context(file_path, target_lines, context_lines=5)

prompt = self.prompt_library.get_prompt(
    &quot;refactor_with_context&quot;,
    language=language,
    code=code_with_context,
    focus=focus
)

return self._send_request(prompt)
</code></pre>

<p>```</p>
<p>Add the new prompt template to the <code>PromptLibrary</code>:</p>
<p>```python</p>
<h1>Add to _initialize_prompts in prompt_library.py</h1>
<p>self.prompts["refactor_with_context"] = PromptTemplate(
    """Refactor the target code in the following {language} code to improve its {focus}.
The file contains context before and after the target code to help you understand its purpose.
Only modify the code between the "/<em> Target code </em>/" markers.</p>
<p><code>{language}
{code}</code></p>
<p>Provide ONLY the refactored target code portion and explain what improvements you made.
The surrounding context is for reference only and should not be included in your response.
Focus specifically on improving {focus} while maintaining the same functionality.
""",
    ["language", "code", "focus"]
)
```</p>
<h3>7.6.3 Adding a Project-Level Assistant</h3>
<p>Let's extend our code assistant to understand project-level context:</p>
<p>```python</p>
<h1>Add to code_assistant.py</h1>
<p>def analyze_project_structure(self, project_dir, max_files=20, file_extensions=None):
    """Analyze project structure to provide context for code generation"""
    project_path = Path(project_dir)
    if not project_path.exists() or not project_path.is_dir():
        raise ValueError(f"Invalid project directory: {project_dir}")</p>
<pre class="codehilite"><code># Default file extensions to analyze
if file_extensions is None:
    file_extensions = [&quot;.py&quot;, &quot;.js&quot;, &quot;.ts&quot;, &quot;.java&quot;, &quot;.c&quot;, &quot;.cpp&quot;, &quot;.h&quot;, &quot;.hpp&quot;]

# Find relevant files
all_files = []
for ext in file_extensions:
    all_files.extend(project_path.glob(f&quot;**/*{ext}&quot;))

# Limit the number of files to analyze
all_files = all_files[:max_files]

# Extract file names and structures
project_structure = {
    &quot;project_name&quot;: project_path.name,
    &quot;files&quot;: [],
    &quot;imports&quot;: [],
    &quot;classes&quot;: [],
    &quot;functions&quot;: []
}

for file_path in all_files:
    rel_path = file_path.relative_to(project_path)

    try:
        content = read_file(file_path)

        # Extract high-level info from the file
        file_info = {
            &quot;path&quot;: str(rel_path),
            &quot;extension&quot;: file_path.suffix,
            &quot;size_bytes&quot;: file_path.stat().st_size
        }

        project_structure[&quot;files&quot;].append(file_info)

        # Very simple extraction of Python imports, classes, and functions
        # In a real implementation, use AST parsing or other proper code analysis
        if file_path.suffix == &quot;.py&quot;:
            # Simple regex-based extraction
            import re

            # Find imports
            imports = re.findall(r'^import\s+(.+?)$|^from\s+(.+?)\s+import', content, re.MULTILINE)
            for imp in imports:
                imp_name = imp[0] or imp[1]
                if imp_name:
                    project_structure[&quot;imports&quot;].append(imp_name)

            # Find classes
            classes = re.findall(r'^class\s+([A-Za-z0-9_]+)', content, re.MULTILINE)
            for cls in classes:
                project_structure[&quot;classes&quot;].append({
                    &quot;name&quot;: cls,
                    &quot;file&quot;: str(rel_path)
                })

            # Find functions
            functions = re.findall(r'^def\s+([A-Za-z0-9_]+)', content, re.MULTILINE)
            for func in functions:
                project_structure[&quot;functions&quot;].append({
                    &quot;name&quot;: func,
                    &quot;file&quot;: str(rel_path)
                })

    except Exception as e:
        print(f&quot;Error analyzing file {rel_path}: {e}&quot;)

return project_structure
</code></pre>

<p>def generate_code_with_project_context(self, description, project_dir, language=None):
    """Generate code with project context"""
    try:
        # Analyze project structure
        project_structure = self.analyze_project_structure(project_dir)</p>
<pre class="codehilite"><code>    # Determine language from project if not specified
    if language is None:
        # Simple heuristic: use most common language in project
        extensions = [f[&quot;extension&quot;] for f in project_structure[&quot;files&quot;]]
        if extensions:
            from collections import Counter
            most_common_ext = Counter(extensions).most_common(1)[0][0]
            language = get_language_from_extension(f&quot;file{most_common_ext}&quot;)
        else:
            language = &quot;python&quot;  # Default

    # Create prompt with project context
    prompt = f&quot;&quot;&quot;You are an expert software developer.
</code></pre>

<p>I want you to generate {language} code based on the following description:</p>
<p>{description}</p>
<p>The code will be part of an existing project with the following structure:
Project name: {project_structure['project_name']}
Files: {', '.join(f['path'] for f in project_structure['files'][:10])}</p>
<p>Key classes in the project: {', '.join(cls['name'] for cls in project_structure['classes'][:10])}
Key functions in the project: {', '.join(func['name'] for func in project_structure['functions'][:10])}
Common imports: {', '.join(project_structure['imports'][:10])}</p>
<p>Generate code that follows the style and conventions of this existing project.
Only return the code with minimal explanatory comments.
"""</p>
<pre class="codehilite"><code>    return self._send_request(prompt, temperature=0.2)

except Exception as e:
    raise Exception(f&quot;Error generating code with project context: {str(e)}&quot;)
</code></pre>

<p>```</p>
<h2>7.7 Practical Use Cases</h2>
<p>Here are some practical use cases for our Smart Code Assistant:</p>
<h3>7.7.1 Automating Repetitive Coding Tasks</h3>
<p><strong>Task</strong>: Creating REST API endpoint handlers</p>
<p><code>bash
python main.py generate "create a Flask REST API endpoint for user registration that validates email, username, and password" --language python --requirements "Must include input validation, error handling, and follow RESTful principles"</code></p>
<p><strong>Task</strong>: Generating database models</p>
<p><code>bash
python main.py generate "create a SQLAlchemy model for a blog post with title, content, author, publication date, and tags" --language python</code></p>
<h3>7.7.2 Understanding Legacy Code</h3>
<p><strong>Task</strong>: Explaining complex algorithms</p>
<p><code>bash
python main.py explain --file legacy_algorithm.py</code></p>
<p><strong>Task</strong>: Documenting undocumented functions</p>
<p><code>bash
python main.py docs --file undocumented_module.py --style Google</code></p>
<h3>7.7.3 Improving Code Quality</h3>
<p><strong>Task</strong>: Refactoring for performance</p>
<p><code>bash
python main.py refactor --file slow_function.py --focus performance</code></p>
<p><strong>Task</strong>: Creating unit tests for existing code</p>
<p><code>bash
python main.py test --file data_processor.py --framework pytest</code></p>
<h3>7.7.4 Cross-Language Development</h3>
<p><strong>Task</strong>: Converting Python utility to JavaScript</p>
<p><code>bash
python main.py convert --from python --to javascript --file utils.py --output utils.js</code></p>
<h2>7.8 Best Practices and Limitations</h2>
<h3>7.8.1 Best Practices</h3>
<ol>
<li>
<p><strong>Always review the generated code</strong>: While LLMs can provide good starting points, always review the code for correctness, security issues, and alignment with your needs.</p>
</li>
<li>
<p><strong>Break down complex tasks</strong>: For better results, break complex coding tasks into smaller, more manageable pieces.</p>
</li>
<li>
<p><strong>Provide clear requirements</strong>: The more specific your descriptions and requirements are, the better the generated code will be.</p>
</li>
<li>
<p><strong>Use project context</strong>: When working on existing projects, providing project-level context will help generate more consistent and compatible code.</p>
</li>
<li>
<p><strong>Cache responses</strong>: To reduce API costs and improve response times, implement caching for frequently requested tasks.</p>
</li>
</ol>
<h3>7.8.2 Limitations</h3>
<ol>
<li>
<p><strong>Code accuracy</strong>: LLMs may generate code with logical errors or incorrect implementations, especially for complex algorithms.</p>
</li>
<li>
<p><strong>Security considerations</strong>: Generated code might contain security vulnerabilities, so always review it carefully.</p>
</li>
<li>
<p><strong>Context limits</strong>: LLMs have context window limitations, so they might struggle with understanding very large codebases or files.</p>
</li>
<li>
<p><strong>Language limitations</strong>: Performance varies across programming languages, with better results typically for popular languages like Python and JavaScript.</p>
</li>
<li>
<p><strong>API costs</strong>: Extensive use of LLMs can incur significant API costs, so monitor usage carefully.</p>
</li>
</ol>
<h2>7.9 Future Enhancements</h2>
<p>Our Smart Code Assistant is just the beginning. Here are some potential future enhancements:</p>
<ol>
<li>
<p><strong>IDE integration</strong>: Develop plugins for popular IDEs like VS Code, PyCharm, and IntelliJ.</p>
</li>
<li>
<p><strong>More advanced project understanding</strong>: Implement deeper static analysis of project structures and coding patterns.</p>
</li>
<li>
<p><strong>Code review capabilities</strong>: Add features to review code changes and suggest improvements.</p>
</li>
<li>
<p><strong>Custom fine-tuning</strong>: Train models on specific codebases to better match company coding styles and patterns.</p>
</li>
<li>
<p><strong>Collaborative features</strong>: Allow teams to share and rate prompt templates and responses.</p>
</li>
<li>
<p><strong>Version control integration</strong>: Integrate with Git to understand code history and changes over time.</p>
</li>
</ol>
<h2>7.10 Conclusion</h2>
<p>In this chapter, we've built a practical Smart Code Assistant that demonstrates how prompt engineering can be applied to solve real-world coding challenges. By leveraging LLMs, we've created a tool that can generate code, explain existing code, refactor for improvements, create tests, and assist with documentation.</p>
<p>The key takeaway is that effective prompt engineering allows us to guide LLMs to produce valuable coding assistance. By structuring prompts with clear instructions, relevant context, and specific requirements, we can obtain high-quality results across a range of coding tasks.</p>
<p>As you continue your prompt engineering journey, consider how you might extend and customize this tool for your specific development needs. The techniques and patterns demonstrated here can be applied to a wide range of software development tasks beyond what we've covered.</p>
<p>In the next chapter, we'll build on these skills to create another practical application: an LLM-powered ML Model Explainer and Debugger.</p>
</div>

<div class="chapter-container">
    <div class="chapter-header">
        <div class="chapter-page-number">Page 128</div>
    </div>
    <h1>Chapter 8: Hands-on Project 2: LLM-Powered ML Model Explainer</h1>
<p>In this chapter, we'll build on the prompt engineering skills we've developed to create a sophisticated tool that addresses one of the most challenging aspects of machine learning: model interpretability. We'll develop an LLM-powered ML Model Explainer that can analyze, interpret, and explain complex machine learning models in accessible language.</p>
<h2>8.1 Project Overview</h2>
<h3>8.1.1 The Problem: ML Model Black Boxes</h3>
<p>Machine learning models, especially deep learning networks, are often criticized as "black boxes" due to their complexity and lack of interpretability. This presents several challenges:</p>
<p><strong>For Data Scientists and ML Engineers:</strong>
- Difficulty understanding why a model makes specific predictions
- Challenges in debugging model performance issues
- Inability to explain model behavior to stakeholders
- Struggles with model validation and trust</p>
<p><strong>For Business Stakeholders:</strong>
- Lack of confidence in automated decision-making
- Regulatory compliance requirements for explainable AI
- Need to understand model limitations and appropriate use cases
- Difficulty in communicating AI capabilities to customers</p>
<p><strong>For Developers Integrating ML Models:</strong>
- Uncertainty about when models might fail
- Challenges in debugging production issues
- Difficulty in setting appropriate confidence thresholds
- Need to understand model requirements and constraints</p>
<h3>8.1.2 Solution: LLM-Powered Model Explainer</h3>
<p>We'll create a comprehensive tool that leverages LLMs to:</p>
<ol>
<li><strong>Analyze Model Architecture</strong>: Break down complex model structures into understandable components</li>
<li><strong>Explain Hyperparameters</strong>: Interpret the significance of various hyperparameter choices</li>
<li><strong>Describe Training Approaches</strong>: Explain the training methodology and its implications</li>
<li><strong>Interpret Model Behavior</strong>: Analyze predictions and feature importance</li>
<li><strong>Provide Usage Guidance</strong>: Offer recommendations for appropriate model deployment</li>
<li><strong>Generate Documentation</strong>: Create comprehensive model documentation automatically</li>
</ol>
<h3>8.1.3 Technical Scope</h3>
<p>Our ML Model Explainer will support:
- <strong>Frameworks</strong>: TensorFlow/Keras, PyTorch, Scikit-learn
- <strong>Model Types</strong>: Neural networks (CNN, RNN, LSTM, Transformer), tree-based models, linear models
- <strong>Analysis Types</strong>: Architecture explanation, hyperparameter interpretation, performance analysis
- <strong>Output Formats</strong>: Interactive reports, markdown documentation, API responses</p>
<h2>8.2 Setting Up the Project</h2>
<h3>8.2.1 Project Structure</h3>
<p><code>ml_model_explainer/
 __init__.py
 main.py                 # CLI interface
 explainer/
    __init__.py
    core.py            # Core explanation engine
    model_analyzers/   # Model-specific analyzers
       __init__.py
       base.py        # Base analyzer class
       keras_analyzer.py
       pytorch_analyzer.py
       sklearn_analyzer.py
    prompt_templates.py # Specialized ML prompts
    report_generator.py # Report generation
 utils/
    __init__.py
    model_utils.py     # Model loading utilities
    visualization.py   # Visualization helpers
    export_utils.py    # Export functionality
 config.py
 requirements.txt
 examples/              # Example models and notebooks
     sample_models/
     notebooks/</code></p>
<h3>8.2.2 Dependencies</h3>
<p>Create <code>requirements.txt</code>:</p>
<p><code>openai&gt;=1.0.0
tensorflow&gt;=2.12.0
torch&gt;=2.0.0
scikit-learn&gt;=1.3.0
pandas&gt;=2.0.0
numpy&gt;=1.24.0
matplotlib&gt;=3.7.0
seaborn&gt;=0.12.0
plotly&gt;=5.15.0
typer&gt;=0.9.0
rich&gt;=13.5.0
jinja2&gt;=3.1.0
python-dotenv&gt;=1.0.0
tiktoken&gt;=0.5.0</code></p>
<h3>8.2.3 Configuration</h3>
<p>Create <code>config.py</code>:</p>
<p>```python
import os
from pathlib import Path
import dotenv</p>
<h1>Load environment variables</h1>
<p>dotenv.load_dotenv()</p>
<h1>API Configuration</h1>
<p>OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
DEFAULT_MODEL = os.getenv("DEFAULT_MODEL", "gpt-4")
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.3"))
MAX_TOKENS = int(os.getenv("MAX_TOKENS", "3000"))</p>
<h1>Application paths</h1>
<p>APP_DIR = Path.home() / ".ml_model_explainer"
CACHE_DIR = APP_DIR / "cache"
REPORTS_DIR = APP_DIR / "reports"
MODELS_DIR = APP_DIR / "models"</p>
<h1>Ensure directories exist</h1>
<p>for directory in [APP_DIR, CACHE_DIR, REPORTS_DIR, MODELS_DIR]:
    directory.mkdir(exist_ok=True)</p>
<h1>Supported frameworks</h1>
<p>SUPPORTED_FRAMEWORKS = ["tensorflow", "pytorch", "sklearn"]</p>
<h1>Model type categories</h1>
<p>MODEL_CATEGORIES = {
    "neural_networks": ["Sequential", "Model", "CNN", "RNN", "LSTM", "GRU", "Transformer"],
    "tree_based": ["RandomForest", "GradientBoosting", "XGBoost", "LightGBM"],
    "linear": ["LinearRegression", "LogisticRegression", "SVM", "Ridge", "Lasso"],
    "clustering": ["KMeans", "DBSCAN", "HierarchicalClustering"],
    "ensemble": ["VotingClassifier", "BaggingClassifier", "AdaBoost"]
}</p>
<h1>Default visualization settings</h1>
<p>VIZ_SETTINGS = {
    "figure_size": (12, 8),
    "dpi": 300,
    "style": "seaborn-v0_8",
    "color_palette": "husl"
}
```</p>
<h2>8.3 Building the Core Components</h2>
<h3>8.3.1 Specialized ML Prompt Templates</h3>
<p>Create <code>explainer/prompt_templates.py</code>:</p>
<p>```python
class MLPromptTemplates:
    def <strong>init</strong>(self):
        self.templates = self._initialize_templates()</p>
<pre class="codehilite"><code>def _initialize_templates(self):
    return {
        &quot;model_architecture_analysis&quot;: &quot;&quot;&quot;
</code></pre>

<p>You are an expert machine learning engineer and educator. Analyze the following model architecture and provide a comprehensive explanation.</p>
<p>Model Information:
Framework: {framework}
Model Type: {model_type}
Architecture Summary:
{architecture_summary}</p>
<p>Layer Details:
{layer_details}</p>
<p>Please provide:
1. <strong>High-Level Overview</strong>: What type of model this is and its primary purpose
2. <strong>Architecture Breakdown</strong>: Explain each component and its role
3. <strong>Design Rationale</strong>: Why this architecture is suitable for the intended task
4. <strong>Strengths and Limitations</strong>: What this model does well and where it might struggle
5. <strong>Computational Complexity</strong>: Discuss the model's resource requirements</p>
<p>Use clear, educational language that would be accessible to both technical and non-technical stakeholders.
""",</p>
<pre class="codehilite"><code>        &quot;hyperparameter_explanation&quot;: &quot;&quot;&quot;
</code></pre>

<p>You are an ML expert explaining model hyperparameters to a team that includes both technical and business stakeholders.</p>
<p>Model: {model_type}
Hyperparameters:
{hyperparameters}</p>
<p>Training Configuration:
{training_config}</p>
<p>For each hyperparameter, explain:
1. <strong>What it controls</strong>: The aspect of model behavior it influences
2. <strong>Current value significance</strong>: Why this specific value was chosen
3. <strong>Impact of changes</strong>: How increasing/decreasing would affect the model
4. <strong>Tuning considerations</strong>: Guidelines for optimization</p>
<p>Conclude with an overall assessment of the hyperparameter choices and their implications for model performance and behavior.
""",</p>
<pre class="codehilite"><code>        &quot;training_methodology_analysis&quot;: &quot;&quot;&quot;
</code></pre>

<p>You are an experienced ML practitioner explaining the training approach for a machine learning model.</p>
<p>Training Details:
Model Type: {model_type}
Training Method: {training_method}
Dataset Information: {dataset_info}
Training Configuration: {training_config}
Performance Metrics: {performance_metrics}</p>
<p>Please explain:
1. <strong>Training Approach</strong>: The methodology used and why it's appropriate
2. <strong>Data Preparation</strong>: How the data was processed for training
3. <strong>Optimization Strategy</strong>: The learning approach and convergence strategy
4. <strong>Validation Method</strong>: How model performance was evaluated during training
5. <strong>Performance Interpretation</strong>: What the metrics tell us about model quality
6. <strong>Potential Issues</strong>: Any concerns or limitations evident from the training process</p>
<p>Provide actionable insights about the model's reliability and expected performance.
""",</p>
<pre class="codehilite"><code>        &quot;prediction_explanation&quot;: &quot;&quot;&quot;
</code></pre>

<p>You are an AI explainability expert helping users understand a specific model prediction.</p>
<p>Model Information:
Type: {model_type}
Task: {task_type}
Input Features: {input_features}
Prediction: {prediction}
Confidence/Probability: {confidence}</p>
<p>Feature Importance (if available):
{feature_importance}</p>
<p>Please provide:
1. <strong>Prediction Summary</strong>: What the model predicted and confidence level
2. <strong>Key Drivers</strong>: Which features most influenced this prediction
3. <strong>Decision Logic</strong>: The reasoning process the model likely followed
4. <strong>Confidence Assessment</strong>: How reliable this prediction appears to be
5. <strong>Alternative Scenarios</strong>: How changing key inputs might affect the outcome
6. <strong>Limitations</strong>: What this prediction doesn't tell us</p>
<p>Make the explanation accessible to non-technical users while maintaining accuracy.
""",</p>
<pre class="codehilite"><code>        &quot;model_comparison&quot;: &quot;&quot;&quot;
</code></pre>

<p>You are an ML consultant comparing different model approaches for a specific problem.</p>
<p>Models to Compare:
{model_details}</p>
<p>Comparison Criteria:
- Performance metrics
- Interpretability
- Computational requirements
- Robustness
- Maintenance complexity</p>
<p>For each model, analyze:
1. <strong>Strengths</strong>: What it does particularly well
2. <strong>Weaknesses</strong>: Where it struggles or has limitations
3. <strong>Use Case Fit</strong>: How well it matches the intended application
4. <strong>Trade-offs</strong>: What you sacrifice vs. what you gain</p>
<p>Conclude with a recommendation for which model to use in different scenarios.
""",</p>
<pre class="codehilite"><code>        &quot;model_deployment_guidance&quot;: &quot;&quot;&quot;
</code></pre>

<p>You are a production ML expert providing deployment guidance for a trained model.</p>
<p>Model Details:
Type: {model_type}
Performance: {performance_summary}
Requirements: {requirements}
Constraints: {constraints}</p>
<p>Please provide guidance on:
1. <strong>Deployment Readiness</strong>: Is this model ready for production use?
2. <strong>Infrastructure Requirements</strong>: What resources and setup are needed?
3. <strong>Monitoring Strategy</strong>: What to track in production
4. <strong>Risk Assessment</strong>: Potential failure modes and mitigation strategies
5. <strong>Maintenance Plan</strong>: How to keep the model performing well over time
6. <strong>Scaling Considerations</strong>: How to handle increasing load or changing requirements</p>
<p>Include specific, actionable recommendations for successful deployment.
"""
        }</p>
<pre class="codehilite"><code>def get_template(self, template_name):
    if template_name not in self.templates:
        raise ValueError(f&quot;Template '{template_name}' not found&quot;)
    return self.templates[template_name]

def format_template(self, template_name, **kwargs):
    template = self.get_template(template_name)
    return template.format(**kwargs)
</code></pre>

<p>```</p>
<h3>8.3.2 Base Model Analyzer</h3>
<p>Create <code>explainer/model_analyzers/base.py</code>:</p>
<p>```python
from abc import ABC, abstractmethod
import json
from pathlib import Path
from typing import Dict, Any, List, Optional</p>
<p>class BaseModelAnalyzer(ABC):
    """Base class for model analyzers"""</p>
<pre class="codehilite"><code>def __init__(self, model_path: Optional[str] = None):
    self.model_path = model_path
    self.model = None
    self.model_info = {}
    self.analysis_cache = {}

@abstractmethod
def load_model(self, model_path: str) -&gt; Any:
    &quot;&quot;&quot;Load the model from file&quot;&quot;&quot;
    pass

@abstractmethod
def extract_architecture(self) -&gt; Dict[str, Any]:
    &quot;&quot;&quot;Extract model architecture information&quot;&quot;&quot;
    pass

@abstractmethod
def extract_hyperparameters(self) -&gt; Dict[str, Any]:
    &quot;&quot;&quot;Extract model hyperparameters&quot;&quot;&quot;
    pass

@abstractmethod
def get_model_summary(self) -&gt; str:
    &quot;&quot;&quot;Get a string summary of the model&quot;&quot;&quot;
    pass

@abstractmethod
def predict_sample(self, sample_input: Any) -&gt; Dict[str, Any]:
    &quot;&quot;&quot;Make a prediction on sample input and return explanation data&quot;&quot;&quot;
    pass

def analyze_model(self) -&gt; Dict[str, Any]:
    &quot;&quot;&quot;Perform comprehensive model analysis&quot;&quot;&quot;
    if not self.model:
        raise ValueError(&quot;Model not loaded. Call load_model() first.&quot;)

    analysis = {
        &quot;framework&quot;: self.get_framework_name(),
        &quot;model_type&quot;: self.get_model_type(),
        &quot;architecture&quot;: self.extract_architecture(),
        &quot;hyperparameters&quot;: self.extract_hyperparameters(),
        &quot;summary&quot;: self.get_model_summary(),
        &quot;complexity&quot;: self.analyze_complexity(),
        &quot;metadata&quot;: self.extract_metadata()
    }

    self.analysis_cache = analysis
    return analysis

@abstractmethod
def get_framework_name(self) -&gt; str:
    &quot;&quot;&quot;Return the ML framework name&quot;&quot;&quot;
    pass

@abstractmethod
def get_model_type(self) -&gt; str:
    &quot;&quot;&quot;Return the specific model type&quot;&quot;&quot;
    pass

def analyze_complexity(self) -&gt; Dict[str, Any]:
    &quot;&quot;&quot;Analyze model computational complexity&quot;&quot;&quot;
    # Default implementation - can be overridden
    complexity = {
        &quot;estimated_parameters&quot;: &quot;Unknown&quot;,
        &quot;memory_usage&quot;: &quot;Unknown&quot;,
        &quot;inference_time&quot;: &quot;Unknown&quot;
    }
    return complexity

def extract_metadata(self) -&gt; Dict[str, Any]:
    &quot;&quot;&quot;Extract additional metadata&quot;&quot;&quot;
    metadata = {
        &quot;model_path&quot;: self.model_path,
        &quot;analysis_timestamp&quot;: None,
        &quot;version_info&quot;: {}
    }
    return metadata

def save_analysis(self, output_path: str):
    &quot;&quot;&quot;Save analysis results to file&quot;&quot;&quot;
    if not self.analysis_cache:
        raise ValueError(&quot;No analysis cached. Run analyze_model() first.&quot;)

    output_path = Path(output_path)
    with open(output_path, 'w') as f:
        json.dump(self.analysis_cache, f, indent=2, default=str)

def load_analysis(self, analysis_path: str) -&gt; Dict[str, Any]:
    &quot;&quot;&quot;Load previously saved analysis&quot;&quot;&quot;
    with open(analysis_path, 'r') as f:
        self.analysis_cache = json.load(f)
    return self.analysis_cache
</code></pre>

<p>```</p>
<h3>8.3.3 Keras/TensorFlow Analyzer</h3>
<p>Create <code>explainer/model_analyzers/keras_analyzer.py</code>:</p>
<p>```python
import tensorflow as tf
from tensorflow import keras
import numpy as np
from typing import Dict, Any, List, Optional
import json</p>
<p>from .base import BaseModelAnalyzer</p>
<p>class KerasModelAnalyzer(BaseModelAnalyzer):
    """Analyzer for Keras/TensorFlow models"""</p>
<pre class="codehilite"><code>def load_model(self, model_path: str):
    &quot;&quot;&quot;Load Keras model&quot;&quot;&quot;
    try:
        self.model = keras.models.load_model(model_path)
        self.model_path = model_path
        return self.model
    except Exception as e:
        raise ValueError(f&quot;Failed to load Keras model: {str(e)}&quot;)

def extract_architecture(self) -&gt; Dict[str, Any]:
    &quot;&quot;&quot;Extract detailed architecture information&quot;&quot;&quot;
    if not self.model:
        raise ValueError(&quot;Model not loaded&quot;)

    architecture = {
        &quot;model_class&quot;: type(self.model).__name__,
        &quot;total_layers&quot;: len(self.model.layers),
        &quot;input_shape&quot;: self.model.input_shape if hasattr(self.model, 'input_shape') else None,
        &quot;output_shape&quot;: self.model.output_shape if hasattr(self.model, 'output_shape') else None,
        &quot;layers&quot;: []
    }

    # Extract layer information
    for i, layer in enumerate(self.model.layers):
        layer_info = {
            &quot;index&quot;: i,
            &quot;name&quot;: layer.name,
            &quot;class&quot;: type(layer).__name__,
            &quot;config&quot;: self._safe_config_extract(layer),
            &quot;input_shape&quot;: layer.input_shape if hasattr(layer, 'input_shape') else None,
            &quot;output_shape&quot;: layer.output_shape if hasattr(layer, 'output_shape') else None,
            &quot;trainable_params&quot;: layer.count_params() if hasattr(layer, 'count_params') else 0,
            &quot;activation&quot;: getattr(layer, 'activation', None)
        }

        # Add layer-specific information
        if hasattr(layer, 'units'):
            layer_info['units'] = layer.units
        if hasattr(layer, 'filters'):
            layer_info['filters'] = layer.filters
        if hasattr(layer, 'kernel_size'):
            layer_info['kernel_size'] = layer.kernel_size
        if hasattr(layer, 'strides'):
            layer_info['strides'] = layer.strides
        if hasattr(layer, 'dropout'):
            layer_info['dropout_rate'] = layer.rate

        architecture[&quot;layers&quot;].append(layer_info)

    return architecture

def _safe_config_extract(self, layer) -&gt; Dict[str, Any]:
    &quot;&quot;&quot;Safely extract layer configuration&quot;&quot;&quot;
    try:
        config = layer.get_config()
        # Remove non-serializable items
        safe_config = {}
        for key, value in config.items():
            try:
                json.dumps(value)  # Test if serializable
                safe_config[key] = value
            except (TypeError, ValueError):
                safe_config[key] = str(value)
        return safe_config
    except:
        return {&quot;error&quot;: &quot;Could not extract configuration&quot;}

def extract_hyperparameters(self) -&gt; Dict[str, Any]:
    &quot;&quot;&quot;Extract model hyperparameters&quot;&quot;&quot;
    hyperparams = {
        &quot;optimizer&quot;: None,
        &quot;loss_function&quot;: None,
        &quot;metrics&quot;: [],
        &quot;total_parameters&quot;: self.model.count_params() if hasattr(self.model, 'count_params') else 0,
        &quot;trainable_parameters&quot;: sum([layer.count_params() for layer in self.model.layers if layer.trainable])
    }

    # Extract optimizer information if model is compiled
    if hasattr(self.model, 'optimizer') and self.model.optimizer:
        optimizer = self.model.optimizer
        hyperparams[&quot;optimizer&quot;] = {
            &quot;class&quot;: type(optimizer).__name__,
            &quot;learning_rate&quot;: float(optimizer.learning_rate) if hasattr(optimizer, 'learning_rate') else None,
            &quot;config&quot;: self._extract_optimizer_config(optimizer)
        }

    # Extract loss function
    if hasattr(self.model, 'compiled_loss') and self.model.compiled_loss:
        hyperparams[&quot;loss_function&quot;] = str(self.model.compiled_loss)

    # Extract metrics
    if hasattr(self.model, 'compiled_metrics') and self.model.compiled_metrics:
        hyperparams[&quot;metrics&quot;] = [str(metric) for metric in self.model.compiled_metrics.metrics]

    return hyperparams

def _extract_optimizer_config(self, optimizer) -&gt; Dict[str, Any]:
    &quot;&quot;&quot;Extract optimizer configuration&quot;&quot;&quot;
    try:
        config = optimizer.get_config()
        safe_config = {}
        for key, value in config.items():
            try:
                json.dumps(value)
                safe_config[key] = value
            except (TypeError, ValueError):
                safe_config[key] = str(value)
        return safe_config
    except:
        return {&quot;error&quot;: &quot;Could not extract optimizer config&quot;}

def get_model_summary(self) -&gt; str:
    &quot;&quot;&quot;Get model summary as string&quot;&quot;&quot;
    if not self.model:
        raise ValueError(&quot;Model not loaded&quot;)

    # Capture model summary
    summary_lines = []
    self.model.summary(print_fn=lambda x: summary_lines.append(x))
    return '\n'.join(summary_lines)

def predict_sample(self, sample_input: Any) -&gt; Dict[str, Any]:
    &quot;&quot;&quot;Make prediction and return explanation data&quot;&quot;&quot;
    if not self.model:
        raise ValueError(&quot;Model not loaded&quot;)

    # Ensure input is in correct format
    if not isinstance(sample_input, np.ndarray):
        sample_input = np.array(sample_input)

    if len(sample_input.shape) == len(self.model.input_shape) - 1:
        sample_input = np.expand_dims(sample_input, axis=0)

    # Make prediction
    prediction = self.model.predict(sample_input, verbose=0)

    prediction_info = {
        &quot;input_shape&quot;: sample_input.shape,
        &quot;output_shape&quot;: prediction.shape,
        &quot;prediction&quot;: prediction.tolist(),
        &quot;input_data&quot;: sample_input.tolist()
    }

    # Add confidence for classification tasks
    if len(prediction.shape) &gt; 1 and prediction.shape[1] &gt; 1:
        prediction_info[&quot;confidence&quot;] = float(np.max(prediction))
        prediction_info[&quot;predicted_class&quot;] = int(np.argmax(prediction))
        prediction_info[&quot;class_probabilities&quot;] = prediction[0].tolist()

    return prediction_info

def get_framework_name(self) -&gt; str:
    return &quot;tensorflow&quot;

def get_model_type(self) -&gt; str:
    if not self.model:
        return &quot;Unknown&quot;

    model_class = type(self.model).__name__

    # Determine model type based on architecture
    if &quot;Sequential&quot; in model_class:
        return &quot;Sequential Neural Network&quot;
    elif &quot;Model&quot; in model_class:
        return &quot;Functional Neural Network&quot;
    else:
        return f&quot;Custom {model_class}&quot;

def analyze_complexity(self) -&gt; Dict[str, Any]:
    &quot;&quot;&quot;Analyze computational complexity&quot;&quot;&quot;
    if not self.model:
        return super().analyze_complexity()

    total_params = self.model.count_params()
    trainable_params = sum([layer.count_params() for layer in self.model.layers if layer.trainable])

    # Estimate memory usage (rough approximation)
    # Each parameter typically takes 4 bytes (float32)
    estimated_memory_mb = (total_params * 4) / (1024 * 1024)

    complexity = {
        &quot;total_parameters&quot;: total_params,
        &quot;trainable_parameters&quot;: trainable_params,
        &quot;non_trainable_parameters&quot;: total_params - trainable_params,
        &quot;estimated_memory_mb&quot;: round(estimated_memory_mb, 2),
        &quot;model_size_layers&quot;: len(self.model.layers),
        &quot;complexity_category&quot;: self._categorize_complexity(total_params)
    }

    return complexity

def _categorize_complexity(self, param_count: int) -&gt; str:
    &quot;&quot;&quot;Categorize model complexity based on parameter count&quot;&quot;&quot;
    if param_count &lt; 1000:
        return &quot;Very Simple&quot;
    elif param_count &lt; 100000:
        return &quot;Simple&quot;
    elif param_count &lt; 1000000:
        return &quot;Moderate&quot;
    elif param_count &lt; 10000000:
        return &quot;Complex&quot;
    else:
        return &quot;Very Complex&quot;
</code></pre>

<p>```</p>
<h3>8.3.4 Core Explainer Engine</h3>
<p>Create <code>explainer/core.py</code>:</p>
<p>```python
import openai
from typing import Dict, Any, Optional, List
import json
from pathlib import Path</p>
<p>import config
from .prompt_templates import MLPromptTemplates
from .model_analyzers.keras_analyzer import KerasModelAnalyzer
from .model_analyzers.pytorch_analyzer import PyTorchModelAnalyzer
from .model_analyzers.sklearn_analyzer import SklearnModelAnalyzer</p>
<p>class MLModelExplainer:
    """Core explanation engine for ML models"""</p>
<pre class="codehilite"><code>def __init__(self, api_key: Optional[str] = None, model: str = None):
    self.api_key = api_key or config.OPENAI_API_KEY
    self.model = model or config.DEFAULT_MODEL
    self.prompt_templates = MLPromptTemplates()

    # Configure OpenAI
    openai.api_key = self.api_key

    # Model analyzer mapping
    self.analyzers = {
        &quot;tensorflow&quot;: KerasModelAnalyzer,
        &quot;keras&quot;: KerasModelAnalyzer,
        &quot;pytorch&quot;: PyTorchModelAnalyzer,
        &quot;sklearn&quot;: SklearnModelAnalyzer
    }

    self.current_analyzer = None
    self.current_analysis = None

def load_model(self, model_path: str, framework: str = None) -&gt; Dict[str, Any]:
    &quot;&quot;&quot;Load and analyze a model&quot;&quot;&quot;
    if framework is None:
        framework = self._detect_framework(model_path)

    if framework not in self.analyzers:
        raise ValueError(f&quot;Unsupported framework: {framework}&quot;)

    # Initialize appropriate analyzer
    self.current_analyzer = self.analyzers[framework](model_path)
    self.current_analyzer.load_model(model_path)

    # Perform initial analysis
    self.current_analysis = self.current_analyzer.analyze_model()

    return self.current_analysis

def _detect_framework(self, model_path: str) -&gt; str:
    &quot;&quot;&quot;Detect ML framework from model file&quot;&quot;&quot;
    path = Path(model_path)

    # Check file extensions and patterns
    if path.suffix == '.h5' or 'keras' in str(path) or 'tensorflow' in str(path):
        return &quot;tensorflow&quot;
    elif path.suffix == '.pt' or path.suffix == '.pth' or 'pytorch' in str(path):
        return &quot;pytorch&quot;
    elif path.suffix == '.pkl' or path.suffix == '.joblib':
        return &quot;sklearn&quot;
    else:
        # Default to tensorflow if unsure
        return &quot;tensorflow&quot;

def explain_architecture(self, detail_level: str = &quot;comprehensive&quot;) -&gt; str:
    &quot;&quot;&quot;Generate explanation of model architecture&quot;&quot;&quot;
    if not self.current_analysis:
        raise ValueError(&quot;No model analysis available. Load a model first.&quot;)

    # Prepare architecture details
    architecture = self.current_analysis[&quot;architecture&quot;]
    layer_details = self._format_layer_details(architecture[&quot;layers&quot;])

    prompt = self.prompt_templates.format_template(
        &quot;model_architecture_analysis&quot;,
        framework=self.current_analysis[&quot;framework&quot;],
        model_type=self.current_analysis[&quot;model_type&quot;],
        architecture_summary=json.dumps(architecture, indent=2),
        layer_details=layer_details
    )

    response = self._send_request(prompt)
    return response

def explain_hyperparameters(self) -&gt; str:
    &quot;&quot;&quot;Generate explanation of model hyperparameters&quot;&quot;&quot;
    if not self.current_analysis:
        raise ValueError(&quot;No model analysis available. Load a model first.&quot;)

    hyperparams = self.current_analysis[&quot;hyperparameters&quot;]

    # Format training configuration
    training_config = {
        &quot;total_parameters&quot;: hyperparams.get(&quot;total_parameters&quot;, &quot;Unknown&quot;),
        &quot;trainable_parameters&quot;: hyperparams.get(&quot;trainable_parameters&quot;, &quot;Unknown&quot;),
        &quot;optimizer&quot;: hyperparams.get(&quot;optimizer&quot;, {}),
        &quot;loss_function&quot;: hyperparams.get(&quot;loss_function&quot;, &quot;Unknown&quot;)
    }

    prompt = self.prompt_templates.format_template(
        &quot;hyperparameter_explanation&quot;,
        model_type=self.current_analysis[&quot;model_type&quot;],
        hyperparameters=json.dumps(hyperparams, indent=2),
        training_config=json.dumps(training_config, indent=2)
    )

    response = self._send_request(prompt)
    return response

def explain_prediction(self, sample_input: Any, include_feature_importance: bool = False) -&gt; str:
    &quot;&quot;&quot;Explain a specific prediction&quot;&quot;&quot;
    if not self.current_analyzer:
        raise ValueError(&quot;No model loaded. Load a model first.&quot;)

    # Get prediction details
    prediction_info = self.current_analyzer.predict_sample(sample_input)

    # Format input features (simplified)
    input_features = f&quot;Input shape: {prediction_info['input_shape']}&quot;

    # Feature importance placeholder (would need additional implementation)
    feature_importance = &quot;Feature importance analysis not yet implemented&quot;
    if include_feature_importance:
        # This would require additional analysis like SHAP, LIME, etc.
        pass

    prompt = self.prompt_templates.format_template(
        &quot;prediction_explanation&quot;,
        model_type=self.current_analysis[&quot;model_type&quot;],
        task_type=self._infer_task_type(),
        input_features=input_features,
        prediction=json.dumps(prediction_info[&quot;prediction&quot;]),
        confidence=prediction_info.get(&quot;confidence&quot;, &quot;N/A&quot;),
        feature_importance=feature_importance
    )

    response = self._send_request(prompt)
    return response

def generate_deployment_guidance(self, target_environment: str = &quot;production&quot;) -&gt; str:
    &quot;&quot;&quot;Generate deployment guidance for the model&quot;&quot;&quot;
    if not self.current_analysis:
        raise ValueError(&quot;No model analysis available. Load a model first.&quot;)

    # Prepare performance summary
    complexity = self.current_analysis.get(&quot;complexity&quot;, {})
    performance_summary = {
        &quot;parameter_count&quot;: complexity.get(&quot;total_parameters&quot;, &quot;Unknown&quot;),
        &quot;memory_usage&quot;: complexity.get(&quot;estimated_memory_mb&quot;, &quot;Unknown&quot;),
        &quot;complexity_category&quot;: complexity.get(&quot;complexity_category&quot;, &quot;Unknown&quot;)
    }

    # Prepare requirements and constraints
    requirements = {
        &quot;framework&quot;: self.current_analysis[&quot;framework&quot;],
        &quot;model_type&quot;: self.current_analysis[&quot;model_type&quot;],
        &quot;memory_requirements&quot;: f&quot;{complexity.get('estimated_memory_mb', 'Unknown')} MB&quot;
    }

    constraints = {
        &quot;environment&quot;: target_environment,
        &quot;scalability_needs&quot;: &quot;To be determined&quot;,
        &quot;latency_requirements&quot;: &quot;To be determined&quot;
    }

    prompt = self.prompt_templates.format_template(
        &quot;model_deployment_guidance&quot;,
        model_type=self.current_analysis[&quot;model_type&quot;],
        performance_summary=json.dumps(performance_summary, indent=2),
        requirements=json.dumps(requirements, indent=2),
        constraints=json.dumps(constraints, indent=2)
    )

    response = self._send_request(prompt)
    return response

def generate_comprehensive_report(self) -&gt; Dict[str, str]:
    &quot;&quot;&quot;Generate a comprehensive explanation report&quot;&quot;&quot;
    if not self.current_analysis:
        raise ValueError(&quot;No model analysis available. Load a model first.&quot;)

    report = {
        &quot;architecture_explanation&quot;: self.explain_architecture(),
        &quot;hyperparameter_explanation&quot;: self.explain_hyperparameters(),
        &quot;deployment_guidance&quot;: self.generate_deployment_guidance(),
        &quot;model_summary&quot;: self.current_analysis[&quot;summary&quot;],
        &quot;technical_details&quot;: json.dumps(self.current_analysis, indent=2)
    }

    return report

def _format_layer_details(self, layers: List[Dict]) -&gt; str:
    &quot;&quot;&quot;Format layer details for prompt&quot;&quot;&quot;
    details = []
    for layer in layers:
        layer_str = f&quot;Layer {layer['index']}: {layer['name']} ({layer['class']})&quot;
        if layer.get('units'):
            layer_str += f&quot; - Units: {layer['units']}&quot;
        if layer.get('filters'):
            layer_str += f&quot; - Filters: {layer['filters']}&quot;
        if layer.get('kernel_size'):
            layer_str += f&quot; - Kernel Size: {layer['kernel_size']}&quot;
        if layer.get('trainable_params'):
            layer_str += f&quot; - Parameters: {layer['trainable_params']}&quot;
        details.append(layer_str)

    return '\n'.join(details)

def _infer_task_type(self) -&gt; str:
    &quot;&quot;&quot;Infer the type of ML task based on model architecture&quot;&quot;&quot;
    if not self.current_analysis:
        return &quot;Unknown&quot;

    architecture = self.current_analysis[&quot;architecture&quot;]
    output_shape = architecture.get(&quot;output_shape&quot;)

    if output_shape:
        if isinstance(output_shape, (list, tuple)) and len(output_shape) &gt; 1:
            output_size = output_shape[-1] if output_shape[-1] is not None else 1
            if output_size == 1:
                return &quot;Binary Classification or Regression&quot;
            elif output_size &gt; 1:
                return &quot;Multi-class Classification&quot;

    return &quot;Unknown Task Type&quot;

def _send_request(self, prompt: str, temperature: float = None) -&gt; str:
    &quot;&quot;&quot;Send request to OpenAI API&quot;&quot;&quot;
    temperature = temperature if temperature is not None else config.TEMPERATURE

    try:
        response = openai.ChatCompletion.create(
            model=self.model,
            messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
            temperature=temperature,
            max_tokens=config.MAX_TOKENS
        )
        return response.choices[0].message.content
    except Exception as e:
        raise Exception(f&quot;Error calling OpenAI API: {str(e)}&quot;)
</code></pre>

<p>```</p>
<h3>8.3.5 Report Generator</h3>
<p>Create <code>explainer/report_generator.py</code>:</p>
<p>```python
from jinja2 import Template
from pathlib import Path
import json
from datetime import datetime
from typing import Dict, Any, Optional</p>
<p>class ReportGenerator:
    """Generate formatted reports from model explanations"""</p>
<pre class="codehilite"><code>def __init__(self, template_dir: Optional[str] = None):
    self.template_dir = Path(template_dir) if template_dir else Path(__file__).parent / &quot;templates&quot;
    self.template_dir.mkdir(exist_ok=True)
    self._create_default_templates()

def _create_default_templates(self):
    &quot;&quot;&quot;Create default report templates&quot;&quot;&quot;
    # HTML Report Template
    html_template = &quot;&quot;&quot;
</code></pre>

<!DOCTYPE html>
<html>
<head>
    <title>ML Model Explanation Report</title>
    <style>
        body { font-family: Arial, sans-serif; max-width: 1200px; margin: 0 auto; padding: 20px; }
        h1, h2, h3 { color: #2c3e50; }
        .section { margin-bottom: 30px; padding: 20px; border-left: 4px solid #3498db; background-color: #f8f9fa; }
        .technical-details { background-color: #f1f2f6; padding: 15px; border-radius: 5px; font-family: monospace; }
        .metadata { color: #7f8c8d; font-size: 0.9em; }
        table { width: 100%; border-collapse: collapse; margin: 10px 0; }
        th, td { padding: 10px; text-align: left; border-bottom: 1px solid #ddd; }
        th { background-color: #34495e; color: white; }
    </style>
</head>
<body>
    <h1>ML Model Explanation Report</h1>

    <div class="metadata">
        <p>Generated on: {{ timestamp }}</p>
        <p>Model Path: {{ model_path }}</p>
        <p>Framework: {{ framework }}</p>
    </div>

    <div class="section">
        <h2>Model Architecture</h2>
        {{ architecture_explanation }}
    </div>

    <div class="section">
        <h2>Hyperparameters</h2>
        {{ hyperparameter_explanation }}
    </div>

    <div class="section">
        <h2>Deployment Guidance</h2>
        {{ deployment_guidance }}
    </div>

    <div class="section">
        <h2>Technical Summary</h2>
        <div class="technical-details">
            <pre>{{ model_summary }}</pre>
        </div>
    </div>

    {% if technical_details %}
    <div class="section">
        <h2>Raw Analysis Data</h2>
        <details>
            <summary>Click to expand technical details</summary>
            <div class="technical-details">
                <pre>{{ technical_details }}</pre>
            </div>
        </details>
    </div>
    {% endif %}
</body>
</html>
<pre class="codehilite"><code>    &quot;&quot;&quot;

    # Markdown Report Template
    markdown_template = &quot;&quot;&quot;
</code></pre>

<h1>ML Model Explanation Report</h1>
<p><strong>Generated on:</strong> {{ timestamp }}<br />
<strong>Model Path:</strong> {{ model_path }}<br />
<strong>Framework:</strong> {{ framework }}</p>
<h2>Model Architecture</h2>
<p>{{ architecture_explanation }}</p>
<h2>Hyperparameters</h2>
<p>{{ hyperparameter_explanation }}</p>
<h2>Deployment Guidance</h2>
<p>{{ deployment_guidance }}</p>
<h2>Technical Summary</h2>
<p><code>{{ model_summary }}</code></p>
<p>{% if technical_details %}</p>
<h2>Raw Analysis Data</h2>
<details>
<summary>Technical Details</summary>

```json
{{ technical_details }}
```

</details>
<p>{% endif %}
        """</p>
<pre class="codehilite"><code>    # Save templates
    with open(self.template_dir / &quot;report.html&quot;, &quot;w&quot;) as f:
        f.write(html_template.strip())

    with open(self.template_dir / &quot;report.md&quot;, &quot;w&quot;) as f:
        f.write(markdown_template.strip())

def generate_html_report(self, explanations: Dict[str, Any], analysis: Dict[str, Any], output_path: str):
    &quot;&quot;&quot;Generate HTML report&quot;&quot;&quot;
    template_path = self.template_dir / &quot;report.html&quot;
    with open(template_path, &quot;r&quot;) as f:
        template = Template(f.read())

    report_data = {
        &quot;timestamp&quot;: datetime.now().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;),
        &quot;model_path&quot;: analysis.get(&quot;metadata&quot;, {}).get(&quot;model_path&quot;, &quot;Unknown&quot;),
        &quot;framework&quot;: analysis.get(&quot;framework&quot;, &quot;Unknown&quot;),
        &quot;architecture_explanation&quot;: self._format_for_html(explanations.get(&quot;architecture_explanation&quot;, &quot;&quot;)),
        &quot;hyperparameter_explanation&quot;: self._format_for_html(explanations.get(&quot;hyperparameter_explanation&quot;, &quot;&quot;)),
        &quot;deployment_guidance&quot;: self._format_for_html(explanations.get(&quot;deployment_guidance&quot;, &quot;&quot;)),
        &quot;model_summary&quot;: explanations.get(&quot;model_summary&quot;, &quot;&quot;),
        &quot;technical_details&quot;: json.dumps(analysis, indent=2)
    }

    html_content = template.render(**report_data)

    with open(output_path, &quot;w&quot;) as f:
        f.write(html_content)

    return output_path

def generate_markdown_report(self, explanations: Dict[str, Any], analysis: Dict[str, Any], output_path: str):
    &quot;&quot;&quot;Generate Markdown report&quot;&quot;&quot;
    template_path = self.template_dir / &quot;report.md&quot;
    with open(template_path, &quot;r&quot;) as f:
        template = Template(f.read())

    report_data = {
        &quot;timestamp&quot;: datetime.now().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;),
        &quot;model_path&quot;: analysis.get(&quot;metadata&quot;, {}).get(&quot;model_path&quot;, &quot;Unknown&quot;),
        &quot;framework&quot;: analysis.get(&quot;framework&quot;, &quot;Unknown&quot;),
        &quot;architecture_explanation&quot;: explanations.get(&quot;architecture_explanation&quot;, &quot;&quot;),
        &quot;hyperparameter_explanation&quot;: explanations.get(&quot;hyperparameter_explanation&quot;, &quot;&quot;),
        &quot;deployment_guidance&quot;: explanations.get(&quot;deployment_guidance&quot;, &quot;&quot;),
        &quot;model_summary&quot;: explanations.get(&quot;model_summary&quot;, &quot;&quot;),
        &quot;technical_details&quot;: json.dumps(analysis, indent=2)
    }

    markdown_content = template.render(**report_data)

    with open(output_path, &quot;w&quot;) as f:
        f.write(markdown_content)

    return output_path

def _format_for_html(self, text: str) -&gt; str:
    &quot;&quot;&quot;Format text for HTML display&quot;&quot;&quot;
    # Convert line breaks to HTML line breaks
    text = text.replace('\n', '&lt;br&gt;\n')

    # Convert **bold** to &lt;strong&gt;
    import re
    text = re.sub(r'\*\*(.*?)\*\*', r'&lt;strong&gt;\1&lt;/strong&gt;', text)

    # Convert *italic* to &lt;em&gt;
    text = re.sub(r'\*(.*?)\*', r'&lt;em&gt;\1&lt;/em&gt;', text)

    return text
</code></pre>

<p>```</p>
<h2>8.4 Building the Command-Line Interface</h2>
<p>Create <code>main.py</code>:</p>
<p>```python
import typer
from pathlib import Path
from typing import Optional
from rich.console import Console
from rich.panel import Panel
from rich.markdown import Markdown</p>
<p>import config
from explainer.core import MLModelExplainer
from explainer.report_generator import ReportGenerator</p>
<p>app = typer.Typer(help="ML Model Explainer - AI-powered model interpretation")
console = Console()</p>
<p>@app.command("explain")
def explain_model(
    model_path: Path = typer.Argument(..., help="Path to the model file"),
    framework: Optional[str] = typer.Option(None, "--framework", "-f", help="ML framework (tensorflow, pytorch, sklearn)"),
    output_dir: Optional[Path] = typer.Option(None, "--output", "-o", help="Output directory for reports"),
    format: str = typer.Option("markdown", "--format", help="Report format (html, markdown, json)"),
    sections: str = typer.Option("all", "--sections", help="Sections to include (all, architecture, hyperparams, deployment)")
):
    """Explain a machine learning model comprehensively"""</p>
<pre class="codehilite"><code>console.print(f&quot;[bold blue]Loading model:[/bold blue] {model_path}&quot;)

try:
    # Initialize explainer
    explainer = MLModelExplainer()

    # Load and analyze model
    analysis = explainer.load_model(str(model_path), framework)

    console.print(f&quot;[green] Model loaded successfully[/green]&quot;)
    console.print(f&quot;Framework: {analysis['framework']}&quot;)
    console.print(f&quot;Model Type: {analysis['model_type']}&quot;)
    console.print(f&quot;Total Parameters: {analysis.get('complexity', {}).get('total_parameters', 'Unknown')}&quot;)

    # Generate explanations based on requested sections
    explanations = {}

    if sections == &quot;all&quot; or &quot;architecture&quot; in sections:
        console.print(&quot;\n[bold blue]Generating architecture explanation...[/bold blue]&quot;)
        explanations[&quot;architecture_explanation&quot;] = explainer.explain_architecture()

    if sections == &quot;all&quot; or &quot;hyperparams&quot; in sections:
        console.print(&quot;[bold blue]Generating hyperparameter explanation...[/bold blue]&quot;)
        explanations[&quot;hyperparameter_explanation&quot;] = explainer.explain_hyperparameters()

    if sections == &quot;all&quot; or &quot;deployment&quot; in sections:
        console.print(&quot;[bold blue]Generating deployment guidance...[/bold blue]&quot;)
        explanations[&quot;deployment_guidance&quot;] = explainer.generate_deployment_guidance()

    # Add model summary
    explanations[&quot;model_summary&quot;] = analysis[&quot;summary&quot;]

    # Display explanations
    console.print(&quot;\n&quot; + &quot;=&quot;*80)

    if &quot;architecture_explanation&quot; in explanations:
        console.print(Panel(Markdown(explanations[&quot;architecture_explanation&quot;]), 
                          title=&quot;[bold]Architecture Explanation[/bold]&quot;, border_style=&quot;blue&quot;))

    if &quot;hyperparameter_explanation&quot; in explanations:
        console.print(Panel(Markdown(explanations[&quot;hyperparameter_explanation&quot;]), 
                          title=&quot;[bold]Hyperparameter Explanation[/bold]&quot;, border_style=&quot;green&quot;))

    if &quot;deployment_guidance&quot; in explanations:
        console.print(Panel(Markdown(explanations[&quot;deployment_guidance&quot;]), 
                          title=&quot;[bold]Deployment Guidance[/bold]&quot;, border_style=&quot;yellow&quot;))

    # Generate report if output directory specified
    if output_dir:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        report_generator = ReportGenerator()
        model_name = model_path.stem

        if format == &quot;html&quot;:
            report_path = output_dir / f&quot;{model_name}_explanation.html&quot;
            report_generator.generate_html_report(explanations, analysis, str(report_path))
            console.print(f&quot;[green]HTML report saved to:[/green] {report_path}&quot;)

        elif format == &quot;markdown&quot;:
            report_path = output_dir / f&quot;{model_name}_explanation.md&quot;
            report_generator.generate_markdown_report(explanations, analysis, str(report_path))
            console.print(f&quot;[green]Markdown report saved to:[/green] {report_path}&quot;)

        elif format == &quot;json&quot;:
            import json
            report_path = output_dir / f&quot;{model_name}_analysis.json&quot;
            full_report = {
                &quot;analysis&quot;: analysis,
                &quot;explanations&quot;: explanations
            }
            with open(report_path, 'w') as f:
                json.dump(full_report, f, indent=2, default=str)
            console.print(f&quot;[green]JSON analysis saved to:[/green] {report_path}&quot;)

except Exception as e:
    console.print(f&quot;[bold red]Error:[/bold red] {str(e)}&quot;)
    raise typer.Exit(code=1)
</code></pre>

<p>@app.command("predict")
def explain_prediction(
    model_path: Path = typer.Argument(..., help="Path to the model file"),
    input_data: str = typer.Argument(..., help="Input data (JSON format or file path)"),
    framework: Optional[str] = typer.Option(None, "--framework", "-f", help="ML framework"),
    output_file: Optional[Path] = typer.Option(None, "--output", "-o", help="Output file for explanation")
):
    """Explain a specific model prediction"""</p>
<pre class="codehilite"><code>console.print(f&quot;[bold blue]Loading model for prediction explanation:[/bold blue] {model_path}&quot;)

try:
    # Initialize explainer
    explainer = MLModelExplainer()

    # Load model
    explainer.load_model(str(model_path), framework)

    # Parse input data
    import json
    import numpy as np

    if Path(input_data).exists():
        # Load from file
        with open(input_data, 'r') as f:
            if input_data.endswith('.json'):
                sample_input = json.load(f)
            else:
                # Assume it's a text file with comma-separated values
                sample_input = [float(x.strip()) for x in f.read().split(',')]
    else:
        # Parse as JSON string
        sample_input = json.loads(input_data)

    # Convert to numpy array
    sample_input = np.array(sample_input)

    console.print(f&quot;[green] Input data loaded, shape: {sample_input.shape}[/green]&quot;)

    # Generate prediction explanation
    console.print(&quot;[bold blue]Generating prediction explanation...[/bold blue]&quot;)
    explanation = explainer.explain_prediction(sample_input)

    # Display explanation
    console.print(Panel(Markdown(explanation), 
                      title=&quot;[bold]Prediction Explanation[/bold]&quot;, border_style=&quot;cyan&quot;))

    # Save to file if requested
    if output_file:
        with open(output_file, 'w') as f:
            f.write(explanation)
        console.print(f&quot;[green]Explanation saved to:[/green] {output_file}&quot;)

except Exception as e:
    console.print(f&quot;[bold red]Error:[/bold red] {str(e)}&quot;)
    raise typer.Exit(code=1)
</code></pre>

<p>@app.command("compare")
def compare_models(
    model_paths: str = typer.Argument(..., help="Comma-separated list of model paths"),
    output_dir: Optional[Path] = typer.Option(None, "--output", "-o", help="Output directory for comparison report")
):
    """Compare multiple models"""</p>
<pre class="codehilite"><code>paths = [p.strip() for p in model_paths.split(',')]
console.print(f&quot;[bold blue]Comparing {len(paths)} models...[/bold blue]&quot;)

try:
    model_analyses = []

    for i, model_path in enumerate(paths):
        console.print(f&quot;[blue]Analyzing model {i+1}:[/blue] {model_path}&quot;)

        explainer = MLModelExplainer()
        analysis = explainer.load_model(model_path)
        model_analyses.append({
            &quot;path&quot;: model_path,
            &quot;analysis&quot;: analysis,
            &quot;explainer&quot;: explainer
        })

    # Generate comparison (simplified for this example)
    console.print(&quot;\n[bold green]Model Comparison Summary:[/bold green]&quot;)

    for i, model_data in enumerate(model_analyses):
        analysis = model_data[&quot;analysis&quot;]
        console.print(f&quot;\n[bold]Model {i+1}:[/bold] {Path(model_data['path']).name}&quot;)
        console.print(f&quot;  Framework: {analysis['framework']}&quot;)
        console.print(f&quot;  Type: {analysis['model_type']}&quot;)
        console.print(f&quot;  Parameters: {analysis.get('complexity', {}).get('total_parameters', 'Unknown')}&quot;)
        console.print(f&quot;  Complexity: {analysis.get('complexity', {}).get('complexity_category', 'Unknown')}&quot;)

    if output_dir:
        # Generate detailed comparison report
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        comparison_data = {
            &quot;models&quot;: model_analyses,
            &quot;timestamp&quot;: datetime.now().isoformat()
        }

        with open(output_dir / &quot;model_comparison.json&quot;, 'w') as f:
            json.dump(comparison_data, f, indent=2, default=str)

        console.print(f&quot;[green]Comparison data saved to:[/green] {output_dir / 'model_comparison.json'}&quot;)

except Exception as e:
    console.print(f&quot;[bold red]Error:[/bold red] {str(e)}&quot;)
    raise typer.Exit(code=1)
</code></pre>

<p>@app.command("info")
def show_info():
    """Show information about the ML Model Explainer"""
    console.print(Panel.fit("""
[bold blue]ML Model Explainer[/bold blue]</p>
<p>AI-powered tool for understanding and explaining machine learning models.</p>
<p>[bold green]Supported Frameworks:[/bold green]
 TensorFlow/Keras
 PyTorch<br />
 Scikit-learn</p>
<p>[bold green]Available Commands:[/bold green]
 explain    - Comprehensive model explanation
 predict    - Explain specific predictions
 compare    - Compare multiple models
 info       - Show this information</p>
<p>[bold green]Configuration:[/bold green]
 Model: {model}
 Supported formats: .h5, .pt, .pth, .pkl, .joblib
    """.format(model=config.DEFAULT_MODEL), 
    title="ML Model Explainer", border_style="blue"))</p>
<p>if <strong>name</strong> == "<strong>main</strong>":
    app()
```</p>
<h2>8.5 Usage Examples and Practical Applications</h2>
<h3>8.5.1 Explaining a Keras Model</h3>
<p>```bash</p>
<h1>Explain a complete Keras model</h1>
<p>python main.py explain ./models/image_classifier.h5 --framework tensorflow --output ./reports --format html</p>
<h1>Explain only architecture and hyperparameters</h1>
<p>python main.py explain ./models/text_classifier.h5 --sections architecture,hyperparams --format markdown
```</p>
<h3>8.5.2 Understanding Model Predictions</h3>
<p>```bash</p>
<h1>Explain a prediction with JSON input</h1>
<p>python main.py predict ./models/sentiment_model.h5 '[0.1, 0.5, 0.3, 0.8]' --output prediction_explanation.md</p>
<h1>Explain prediction with input from file</h1>
<p>python main.py predict ./models/price_predictor.pkl ./data/sample_input.json --framework sklearn
```</p>
<h3>8.5.3 Comparing Different Models</h3>
<p>```bash</p>
<h1>Compare multiple models</h1>
<p>python main.py compare "./models/model_v1.h5,./models/model_v2.h5,./models/model_v3.h5" --output ./comparison_reports
```</p>
<h2>8.6 Best Practices and Limitations</h2>
<h3>8.6.1 Best Practices</h3>
<ol>
<li>
<p><strong>Model Documentation</strong>: Always maintain detailed documentation about your model's training process, data preprocessing, and intended use cases.</p>
</li>
<li>
<p><strong>Explanation Validation</strong>: Cross-check LLM explanations with your domain knowledge and established ML principles.</p>
</li>
<li>
<p><strong>Context Awareness</strong>: Provide business context when generating explanations for stakeholders.</p>
</li>
<li>
<p><strong>Regular Updates</strong>: Keep explanations current as models are retrained or updated.</p>
</li>
<li>
<p><strong>Security Considerations</strong>: Be cautious about exposing sensitive model details in explanations.</p>
</li>
</ol>
<h3>8.6.2 Current Limitations</h3>
<ol>
<li>
<p><strong>Framework Coverage</strong>: Our current implementation has basic support for major frameworks but may need extension for specialized architectures.</p>
</li>
<li>
<p><strong>Feature Importance</strong>: True feature importance analysis requires additional tools like SHAP or LIME integration.</p>
</li>
<li>
<p><strong>Explanation Accuracy</strong>: LLM explanations should be validated by domain experts.</p>
</li>
<li>
<p><strong>Complex Architectures</strong>: Very complex or custom architectures may require specialized analysis approaches.</p>
</li>
</ol>
<h2>8.7 Future Enhancements</h2>
<p>Potential improvements for our ML Model Explainer include:</p>
<ol>
<li><strong>Advanced Interpretability</strong>: Integration with SHAP, LIME, and other interpretability tools</li>
<li><strong>Visualization Generation</strong>: Automatic creation of architecture diagrams and performance plots</li>
<li><strong>Interactive Dashboards</strong>: Web-based interface for exploring model explanations</li>
<li><strong>Custom Model Support</strong>: Extensible architecture for specialized model types</li>
<li><strong>Deployment Monitoring</strong>: Integration with model monitoring and drift detection tools</li>
</ol>
<h2>8.8 Conclusion</h2>
<p>In this chapter, we've built a sophisticated ML Model Explainer that demonstrates how LLMs can be used to make complex machine learning models more interpretable and accessible. By combining traditional model analysis techniques with the natural language generation capabilities of LLMs, we've created a tool that can bridge the gap between technical complexity and human understanding.</p>
<p>The key insights from this project include:</p>
<ol>
<li><strong>Prompt Specialization</strong>: Domain-specific prompts yield much better results than generic explanations</li>
<li><strong>Structured Analysis</strong>: Breaking down model analysis into clear components (architecture, hyperparameters, etc.) enables more focused explanations</li>
<li><strong>Multi-Format Output</strong>: Different stakeholders need different types of explanations and reports</li>
<li><strong>Framework Abstraction</strong>: Using a base analyzer class allows for easy extension to new ML frameworks</li>
</ol>
<p>In the next chapter, we'll build on these concepts to create an ML Training Debugger and Optimizer that can help identify and resolve common training issues.</p>
</div>

<div class="chapter-container">
    <div class="chapter-header">
        <div class="chapter-page-number">Page 7</div>
    </div>
    <h1>Chapter 9: Hands-on Project 3: ML Training Debugger and Optimizer</h1>
<p>Building on our ML Model Explainer from Chapter 8, we now tackle one of the most frustrating aspects of machine learning: debugging failed or suboptimal training runs. This chapter focuses on creating an intelligent system that can analyze training logs, identify issues, and provide actionable optimization recommendations.</p>
<h2>9.1 The Training Debug Challenge</h2>
<h3>9.1.1 Common Training Problems</h3>
<p>Machine learning training often fails in subtle ways:
- <strong>Convergence Issues</strong>: Models that won't converge or converge too slowly
- <strong>Overfitting/Underfitting</strong>: Poor generalization or insufficient learning
- <strong>Hyperparameter Problems</strong>: Learning rates too high/low, poor batch sizes
- <strong>Technical Issues</strong>: Memory problems, gradient explosions, vanishing gradients</p>
<h3>9.1.2 Why LLMs Help</h3>
<p>Traditional debugging relies on manual interpretation of metrics and charts. LLMs can:
- Recognize patterns across thousands of training runs
- Correlate multiple metrics simultaneously
- Provide contextual explanations in natural language
- Suggest specific, actionable fixes</p>
<h2>9.2 Project Architecture</h2>
<p>Our ML Training Debugger will have three core components:
1. <strong>Log Parser</strong>: Extract metrics from TensorFlow, PyTorch, or custom logs
2. <strong>Pattern Analyzer</strong>: Detect common training issues automatically
3. <strong>LLM Explainer</strong>: Generate insights and optimization recommendations</p>
<h2>9.3 Core Implementation</h2>
<h3>9.3.1 Training Log Parser</h3>
<p>```python</p>
<h1>training_debugger.py</h1>
<p>import pandas as pd
import re
import json
from pathlib import Path</p>
<p>class TrainingLogParser:
    def <strong>init</strong>(self, log_path):
        self.log_path = Path(log_path)
        self.data = None</p>
<pre class="codehilite"><code>def parse_logs(self):
    &quot;&quot;&quot;Parse training logs from various formats&quot;&quot;&quot;
    if self.log_path.suffix == '.csv':
        return self._parse_csv()
    elif self.log_path.suffix == '.json':
        return self._parse_json()
    else:
        return self._parse_text_logs()

def _parse_csv(self):
    &quot;&quot;&quot;Parse CSV training history&quot;&quot;&quot;
    self.data = pd.read_csv(self.log_path)
    self.data.columns = [col.lower().replace(' ', '_') for col in self.data.columns]
    return self.data

def _parse_text_logs(self):
    &quot;&quot;&quot;Extract metrics from text logs using regex&quot;&quot;&quot;
    with open(self.log_path, 'r') as f:
        content = f.read()

    # Common patterns for training logs
    patterns = {
        'epoch': r'Epoch (\d+)',
        'loss': r'loss:\s*([0-9.]+)',
        'accuracy': r'accuracy:\s*([0-9.]+)',
        'val_loss': r'val_loss:\s*([0-9.]+)',
        'val_accuracy': r'val_accuracy:\s*([0-9.]+)',
        'lr': r'lr:\s*([0-9.e-]+)'
    }

    data_dict = {key: [] for key in patterns.keys()}

    lines = content.split('\n')
    for line in lines:
        epoch_match = re.search(patterns['epoch'], line)
        if epoch_match:
            current_epoch = int(epoch_match.group(1))

            # Extract all metrics for this epoch
            epoch_data = {'epoch': current_epoch}
            for metric, pattern in patterns.items():
                if metric == 'epoch':
                    continue
                match = re.search(pattern, line)
                if match:
                    epoch_data[metric] = float(match.group(1))

            # Add to data_dict
            for key in data_dict.keys():
                data_dict[key].append(epoch_data.get(key, None))

    self.data = pd.DataFrame(data_dict).dropna()
    return self.data
</code></pre>

<p>```</p>
<h3>9.3.2 Issue Detection System</h3>
<p>```python
class TrainingIssueDetector:
    def <strong>init</strong>(self, data):
        self.data = data
        self.issues = []</p>
<pre class="codehilite"><code>def detect_all_issues(self):
    &quot;&quot;&quot;Run all issue detection methods&quot;&quot;&quot;
    self.detect_overfitting()
    self.detect_learning_rate_issues()
    self.detect_convergence_problems()
    self.detect_loss_explosions()
    return self.issues

def detect_overfitting(self):
    &quot;&quot;&quot;Detect train/validation performance gaps&quot;&quot;&quot;
    if 'loss' in self.data.columns and 'val_loss' in self.data.columns:
        train_loss = self.data['loss'].dropna()
        val_loss = self.data['val_loss'].dropna()

        if len(train_loss) &gt; 10 and len(val_loss) &gt; 10:
            # Check recent performance gap
            recent_train = train_loss.tail(5).mean()
            recent_val = val_loss.tail(5).mean()
            gap = (recent_val - recent_train) / recent_train

            if gap &gt; 0.15:  # 15% gap threshold
                self.issues.append({
                    'type': 'overfitting',
                    'severity': 'high' if gap &gt; 0.3 else 'medium',
                    'description': f'Validation loss {gap:.1%} higher than training loss',
                    'metrics': {
                        'train_loss': recent_train,
                        'val_loss': recent_val,
                        'gap': gap
                    }
                })

def detect_learning_rate_issues(self):
    &quot;&quot;&quot;Detect learning rate problems&quot;&quot;&quot;
    if 'lr' in self.data.columns:
        current_lr = self.data['lr'].iloc[-1]

        if current_lr &gt; 0.1:
            self.issues.append({
                'type': 'learning_rate_too_high',
                'severity': 'high',
                'description': f'Learning rate {current_lr} may be too high',
                'metrics': {'current_lr': current_lr}
            })
        elif current_lr &lt; 1e-6:
            self.issues.append({
                'type': 'learning_rate_too_low',
                'severity': 'medium',
                'description': f'Learning rate {current_lr} may be too low',
                'metrics': {'current_lr': current_lr}
            })

def detect_convergence_problems(self):
    &quot;&quot;&quot;Detect poor convergence&quot;&quot;&quot;
    if 'loss' in self.data.columns:
        loss_series = self.data['loss'].dropna()

        if len(loss_series) &gt; 20:
            # Check if loss is still decreasing in recent epochs
            recent_improvement = (loss_series.iloc[-10:].iloc[0] - loss_series.iloc[-1]) / loss_series.iloc[-10:].iloc[0]

            if recent_improvement &lt; 0.01:  # Less than 1% improvement
                self.issues.append({
                    'type': 'poor_convergence',
                    'severity': 'medium',
                    'description': 'Loss not improving in recent epochs',
                    'metrics': {'recent_improvement': recent_improvement}
                })

def detect_loss_explosions(self):
    &quot;&quot;&quot;Detect sudden loss increases&quot;&quot;&quot;
    if 'loss' in self.data.columns:
        loss_series = self.data['loss'].dropna()

        # Check for sudden jumps (&gt;5x increase)
        pct_changes = loss_series.pct_change().fillna(0)
        explosions = pct_changes &gt; 5.0

        if explosions.any():
            explosion_idx = explosions.idxmax()
            self.issues.append({
                'type': 'loss_explosion',
                'severity': 'critical',
                'description': 'Sudden loss explosion detected',
                'metrics': {
                    'explosion_epoch': explosion_idx,
                    'change_factor': pct_changes.loc[explosion_idx]
                }
            })
</code></pre>

<p>```</p>
<h3>9.3.3 LLM-Powered Analysis Engine</h3>
<p>```python
import openai
import json</p>
<p>class TrainingAnalyzer:
    def <strong>init</strong>(self, api_key):
        openai.api_key = api_key</p>
<pre class="codehilite"><code>def analyze_training_run(self, data, issues, hyperparams=None):
    &quot;&quot;&quot;Generate comprehensive training analysis&quot;&quot;&quot;

    # Prepare training summary
    summary = self._create_training_summary(data, issues)

    prompt = f&quot;&quot;&quot;You are an expert ML engineer analyzing a training run.
</code></pre>

<p>Training Summary:
{summary}</p>
<p>Detected Issues:
{json.dumps(issues, indent=2)}</p>
<p>Current Hyperparameters:
{json.dumps(hyperparams or {}, indent=2)}</p>
<p>Please provide:
1. <strong>Overall Assessment</strong>: How is this training run performing?
2. <strong>Root Cause Analysis</strong>: What's causing the main issues?
3. <strong>Specific Recommendations</strong>: What should be changed next?
4. <strong>Priority Actions</strong>: What to fix first?</p>
<p>Be specific and actionable in your recommendations."""</p>
<pre class="codehilite"><code>    response = openai.ChatCompletion.create(
        model=&quot;gpt-4&quot;,
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
        temperature=0.3,
        max_tokens=2000
    )

    return response.choices[0].message.content

def suggest_hyperparameter_optimization(self, current_params, issues, performance_data):
    &quot;&quot;&quot;Suggest specific hyperparameter changes&quot;&quot;&quot;

    prompt = f&quot;&quot;&quot;You are a hyperparameter optimization expert. Based on the training issues and performance, suggest specific parameter adjustments.
</code></pre>

<p>Current Hyperparameters:
{json.dumps(current_params, indent=2)}</p>
<p>Training Issues Detected:
{json.dumps([issue['type'] for issue in issues])}</p>
<p>Performance Data:
- Final Loss: {performance_data.get('final_loss', 'Unknown')}
- Best Validation: {performance_data.get('best_val', 'Unknown')}
- Training Epochs: {performance_data.get('epochs', 'Unknown')}</p>
<p>Provide specific recommendations for:
1. <strong>Learning Rate</strong>: Exact values to try
2. <strong>Batch Size</strong>: Optimal batch size
3. <strong>Architecture Changes</strong>: If needed
4. <strong>Regularization</strong>: Dropout, weight decay adjustments
5. <strong>Training Schedule</strong>: Learning rate schedules, early stopping</p>
<p>Focus on the top 2-3 most impactful changes."""</p>
<pre class="codehilite"><code>    response = openai.ChatCompletion.create(
        model=&quot;gpt-4&quot;, 
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
        temperature=0.2,
        max_tokens=1500
    )

    return response.choices[0].message.content

def _create_training_summary(self, data, issues):
    &quot;&quot;&quot;Create a concise training summary&quot;&quot;&quot;
    summary = []

    if 'epoch' in data.columns:
        summary.append(f&quot;Total epochs: {data['epoch'].max()}&quot;)

    if 'loss' in data.columns:
        loss_series = data['loss'].dropna()
        summary.append(f&quot;Final loss: {loss_series.iloc[-1]:.4f}&quot;)
        summary.append(f&quot;Best loss: {loss_series.min():.4f}&quot;)

    if 'val_loss' in data.columns:
        val_loss = data['val_loss'].dropna()
        summary.append(f&quot;Final val_loss: {val_loss.iloc[-1]:.4f}&quot;)
        summary.append(f&quot;Best val_loss: {val_loss.min():.4f}&quot;)

    if 'accuracy' in data.columns:
        acc = data['accuracy'].dropna()
        summary.append(f&quot;Final accuracy: {acc.iloc[-1]:.3f}&quot;)

    summary.append(f&quot;Issues detected: {len(issues)}&quot;)

    return &quot;\n&quot;.join(summary)
</code></pre>

<p>```</p>
<h3>9.3.4 Main Training Debugger Interface</h3>
<p>```python
class MLTrainingDebugger:
    def <strong>init</strong>(self, api_key):
        self.analyzer = TrainingAnalyzer(api_key)</p>
<pre class="codehilite"><code>def debug_training_run(self, log_path, hyperparams=None):
    &quot;&quot;&quot;Complete debugging workflow&quot;&quot;&quot;

    # 1. Parse logs
    print(&quot; Parsing training logs...&quot;)
    parser = TrainingLogParser(log_path)
    data = parser.parse_logs()
    print(f&quot; Loaded {len(data)} training records&quot;)

    # 2. Detect issues
    print(&quot; Detecting training issues...&quot;)
    detector = TrainingIssueDetector(data)
    issues = detector.detect_all_issues()
    print(f&quot;  Found {len(issues)} potential issues&quot;)

    # 3. Generate analysis
    print(&quot; Generating AI analysis...&quot;)
    analysis = self.analyzer.analyze_training_run(data, issues, hyperparams)

    # 4. Get optimization suggestions
    performance_data = {
        'final_loss': data['loss'].iloc[-1] if 'loss' in data.columns else None,
        'best_val': data['val_loss'].min() if 'val_loss' in data.columns else None,
        'epochs': data['epoch'].max() if 'epoch' in data.columns else len(data)
    }

    optimization = self.analyzer.suggest_hyperparameter_optimization(
        hyperparams or {}, issues, performance_data
    )

    return {
        'data': data,
        'issues': issues,
        'analysis': analysis,
        'optimization_suggestions': optimization
    }
</code></pre>

<p>```</p>
<h2>9.4 Usage Examples</h2>
<h3>9.4.1 Basic Usage</h3>
<p>```python</p>
<h1>Initialize debugger</h1>
<p>debugger = MLTrainingDebugger(api_key="your-openai-key")</p>
<h1>Analyze a training run</h1>
<p>result = debugger.debug_training_run(
    log_path="training_history.csv",
    hyperparams={
        "learning_rate": 0.001,
        "batch_size": 32,
        "optimizer": "adam"
    }
)</p>
<h1>Print analysis</h1>
<p>print("=== TRAINING ANALYSIS ===")
print(result['analysis'])
print("\n=== OPTIMIZATION SUGGESTIONS ===")<br />
print(result['optimization_suggestions'])
```</p>
<h3>9.4.2 CLI Interface</h3>
<p>```python
import typer
from rich.console import Console</p>
<p>app = typer.Typer()
console = Console()</p>
<p>@app.command()
def debug(
    log_file: str = typer.Argument(..., help="Path to training log file"),
    api_key: str = typer.Option(..., envvar="OPENAI_API_KEY", help="OpenAI API key"),
    output: str = typer.Option(None, help="Save report to file")
):
    """Debug a machine learning training run"""</p>
<pre class="codehilite"><code>console.print(f&quot;[blue]Analyzing training log:[/blue] {log_file}&quot;)

try:
    debugger = MLTrainingDebugger(api_key)
    result = debugger.debug_training_run(log_file)

    # Display results
    console.print(&quot;\n[bold green] Training Analysis[/bold green]&quot;)
    console.print(result['analysis'])

    console.print(&quot;\n[bold blue] Optimization Suggestions[/bold blue]&quot;)
    console.print(result['optimization_suggestions'])

    # Save if requested
    if output:
        with open(output, 'w') as f:
            f.write(f&quot;# Training Debug Report\n\n&quot;)
            f.write(f&quot;## Analysis\n{result['analysis']}\n\n&quot;)
            f.write(f&quot;## Optimization\n{result['optimization_suggestions']}&quot;)
        console.print(f&quot;[green]Report saved to {output}[/green]&quot;)

except Exception as e:
    console.print(f&quot;[red]Error: {e}[/red]&quot;)
</code></pre>

<p>if <strong>name</strong> == "<strong>main</strong>":
    app()
```</p>
<h2>9.5 Advanced Features</h2>
<h3>9.5.1 Experiment Comparison</h3>
<p>```python
def compare_experiments(self, experiment_logs):
    """Compare multiple training experiments"""</p>
<pre class="codehilite"><code>experiments = []
for log_path in experiment_logs:
    parser = TrainingLogParser(log_path)
    data = parser.parse_logs()
    detector = TrainingIssueDetector(data)
    issues = detector.detect_all_issues()

    experiments.append({
        'name': Path(log_path).stem,
        'final_loss': data['loss'].iloc[-1] if 'loss' in data.columns else None,
        'best_val': data['val_loss'].min() if 'val_loss' in data.columns else None,
        'issues': len(issues),
        'converged': len([i for i in issues if i['type'] == 'poor_convergence']) == 0
    })

# Generate comparison analysis
comparison_prompt = f&quot;&quot;&quot;Compare these ML experiments and identify the best performing approach:
</code></pre>

<p>{json.dumps(experiments, indent=2)}</p>
<p>Provide:
1. <strong>Best Experiment</strong>: Which performed best and why?
2. <strong>Key Patterns</strong>: What patterns lead to success?
3. <strong>Recommendations</strong>: What to try next?"""</p>
<pre class="codehilite"><code># ...rest of comparison logic
</code></pre>

<p>```</p>
<h3>9.5.2 Visualization Integration</h3>
<p>```python
import matplotlib.pyplot as plt</p>
<p>def generate_training_plots(data, output_dir="plots"):
    """Generate training visualization plots"""</p>
<pre class="codehilite"><code>fig, axes = plt.subplots(2, 2, figsize=(12, 8))

# Loss curves
if 'loss' in data.columns:
    axes[0,0].plot(data['epoch'], data['loss'], label='Training Loss')
    if 'val_loss' in data.columns:
        axes[0,0].plot(data['epoch'], data['val_loss'], label='Validation Loss')
    axes[0,0].set_title('Loss Curves')
    axes[0,0].legend()

# Accuracy curves  
if 'accuracy' in data.columns:
    axes[0,1].plot(data['epoch'], data['accuracy'], label='Training Accuracy')
    if 'val_accuracy' in data.columns:
        axes[0,1].plot(data['epoch'], data['val_accuracy'], label='Validation Accuracy')
    axes[0,1].set_title('Accuracy Curves')
    axes[0,1].legend()

# Learning rate schedule
if 'lr' in data.columns:
    axes[1,0].plot(data['epoch'], data['lr'])
    axes[1,0].set_title('Learning Rate Schedule')
    axes[1,0].set_yscale('log')

plt.tight_layout()
plt.savefig(f&quot;{output_dir}/training_analysis.png&quot;, dpi=300, bbox_inches='tight')
plt.close()
</code></pre>

<p>```</p>
<h2>9.6 Best Practices and Limitations</h2>
<h3>9.6.1 Best Practices</h3>
<ol>
<li><strong>Log Everything</strong>: Include learning rates, gradient norms, and custom metrics</li>
<li><strong>Consistent Formatting</strong>: Use standard logging formats for easier parsing</li>
<li><strong>Domain Context</strong>: Provide model architecture and dataset information</li>
<li><strong>Multiple Runs</strong>: Compare across experiments for better insights</li>
<li><strong>Human Validation</strong>: Always verify LLM suggestions with domain knowledge</li>
</ol>
<h3>9.6.2 Current Limitations</h3>
<ol>
<li><strong>Pattern Recognition</strong>: LLMs may miss domain-specific issues</li>
<li><strong>Causation vs Correlation</strong>: May suggest fixes that don't address root causes</li>
<li><strong>Framework Specifics</strong>: Different frameworks have unique debugging needs</li>
<li><strong>Resource Costs</strong>: Extensive analysis can be expensive with API calls</li>
</ol>
<h2>9.7 Conclusion</h2>
<p>The ML Training Debugger demonstrates how LLMs can transform the traditionally manual and expertise-heavy process of training diagnosis. By combining automatic issue detection with intelligent analysis, we can:</p>
<ul>
<li><strong>Reduce Debug Time</strong>: Quickly identify common training problems</li>
<li><strong>Improve Training Success</strong>: Get specific, actionable recommendations  </li>
<li><strong>Learn Faster</strong>: Understand why certain approaches work or fail</li>
<li><strong>Scale Expertise</strong>: Make advanced debugging accessible to more developers</li>
</ul>
<p>The key insight is that LLMs excel at pattern recognition across training metrics when provided with the right context and structured prompts. This approach can significantly accelerate the iterative process of ML model development.</p>
<p>In Part 2 of this book, we'll shift focus to architectural considerations for deploying LLM-powered systems at enterprise scale.</p>
</div>
</body>
</html>