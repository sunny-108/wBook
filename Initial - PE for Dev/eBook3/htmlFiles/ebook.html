<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>Complete Book</title>
    <style>
@page {
    size: letter;
    margin: 2cm;
    @bottom-center {
        content: "Page " counter(page);
    }
}

body {
    font-family: "Helvetica", "Arial", sans-serif;
    line-height: 1.6;
    margin: 0;
    padding: 0 0.5cm;
    font-size: 10.5pt;
    counter-reset: chapter;
}

h1 {
    font-size: 20pt;
    color: #2c3e50;
    page-break-before: always;
    margin-top: 1.5cm;
}

h1:first-of-type {
    page-break-before: avoid;
}

h2 {
    font-size: 16pt;
    color: #3498db;
    margin-top: 1em;
    page-break-after: avoid;
}

h3 {
    font-size: 14pt;
    color: #2980b9;
    page-break-after: avoid;
}

/* Table of Contents styling */
.toc {
    page-break-before: always;
    page-break-after: always;
}

.toc h1 {
    text-align: center;
    font-size: 24pt;
    margin-bottom: 1.5em;
    page-break-before: avoid;
}

.toc ul {
    list-style-type: none;
    padding-left: 0;
}

.toc ul ul {
    padding-left: 1.5em;
}

.toc li {
    margin-bottom: 0.5em;
}

.toc-entry {
    display: flex;
    width: 100%;
    align-items: baseline;
}

.chapter-number {
    font-weight: bold;
    margin-right: 0.5em;
}

.chapter-title {
    font-weight: bold;
}

.dots {
    flex-grow: 1;
    margin: 0 5em;
    border-bottom: 1px dotted #999;
    position: relative;
    bottom: 0.3em;
}

.page-number {
    font-weight: normal;
}

.section-entry {
    font-weight: normal;
    margin-top: 0.3em;
}

.section-entry .toc-entry {
    font-weight: normal;
}

/* Link styling */
a {
    color: #3498db;
    text-decoration: none;
}

a:hover {
    text-decoration: underline;
}

/* Code blocks */
pre {
    background-color: #f8f8f8;
    border: 1px solid #ddd;
    border-radius: 3px;
    padding: 10px;
    font-family: "Courier New", monospace;
    font-size: 9pt;
    white-space: pre-wrap;
    overflow-wrap: break-word;
    max-width: 100%;
}

code {
    font-family: "Courier New", monospace;
    font-size: 9pt;
    background-color: #f8f8f8;
    padding: 2px 4px;
    border-radius: 3px;
}

/* Table styling */
table {
    border-collapse: collapse;
    width: 100%;
    margin: 1em 0;
}

table, th, td {
    border: 1px solid #ddd;
}

td, th {
    padding: 8px;
    text-align: left;
}

th {
    background-color: #f2f2f2;
}

img {
    max-width: 100%;
}

/* Front matter styling */
.front-matter {
    page-break-after: always;
}

.front-matter h1 {
    page-break-before: avoid;
}

/* Cover page */
.cover {
    display: flex;
    flex-direction: column;
    justify-content: center;
    align-items: center;
    height: 90%;
    text-align: center;
}

.cover h1 {
    font-size: 32pt;
    margin-bottom: 1em;
    color: #2c3e50;
    page-break-before: avoid;
}

.cover .subtitle {
    font-size: 18pt;
    margin-bottom: 2em;
    color: #7f8c8d;
}

/* Navigation */
.navigation {
    display: flex;
    justify-content: space-between;
    margin-top: 2em;
    border-top: 1px solid #eee;
    padding-top: 1em;
}

.prev-chapter, .next-chapter {
    padding: 5px 10px;
}

.prev-chapter:before {
    content: "← ";
}

.next-chapter:after {
    content: " →";
}

</style>

    <style>
        /* Cover page styling */
        .cover-page {
            page-break-after: always;
            width: 100%;
            height: 100vh;
            background: linear-gradient(to top, #0A2342, #2C74B3, #483D8B);
            padding: 0.5in;
            box-sizing: border-box;
            position: relative;
        }
        
        .title-container {
            text-align: center;
            margin-top: 1.5in;
            position: relative;
            z-index: 10;
        }
        
        .main-title {
            color: white;
            font-family: 'Arial', sans-serif;
            font-size: 36pt;
            font-weight: 800;
            margin: 0;
            line-height: 1.2;
            letter-spacing: 1px;
            text-shadow: 0px 2px 4px rgba(0,0,0,0.5);
        }
        
        .subtitle {
            color: #B5D0FF;
            font-family: 'Arial', sans-serif;
            font-size: 18pt;
            font-weight: 300;
            margin-top: 15px;
            margin-bottom: 0;
        }
        
        .graphic {
            margin: 0.8in auto;
            height: 3in;
            position: relative;
            display: flex;
            justify-content: center;
            align-items: center;
        }
        
        .code-column {
            background-color: rgba(30, 30, 30, 0.7);
            border-radius: 5px;
            padding: 15px;
            width: 1.8in;
            font-family: 'Courier New', monospace;
            font-size: 11pt;
            color: #d4d4d4;
            box-shadow: 0 5px 15px rgba(0,0,0,0.3);
            z-index: 10;
            position: relative;
            left: -30px;
        }
        
        .prompt-column {
            background-color: rgba(49, 49, 68, 0.7);
            border-radius: 5px;
            padding: 15px;
            width: 1.8in;
            font-family: 'Arial', sans-serif;
            font-size: 11pt;
            color: #e0e0e0;
            box-shadow: 0 5px 15px rgba(0,0,0,0.3);
            z-index: 10;
            position: relative;
            right: -30px;
        }
        
        .keyword { color: #569CD6; }
        .string { color: #CE9178; }
        .comment { color: #6A9955; }
        .function { color: #DCDCAA; }
        .variable { color: #9CDCFE; }
        
        .author {
            text-align: center;
            position: absolute;
            bottom: 0.8in;
            left: 0;
            width: 100%;
            color: white;
            font-family: 'Arial', sans-serif;
            font-size: 14pt;
            font-weight: 300;
            z-index: 10;
        }
        
        .brackets {
            position: absolute;
            font-size: 120pt;
            font-weight: 300;
            color: rgba(255,255,255,0.1);
            font-family: 'Courier New', monospace;
        }
        
        .bracket-left {
            top: 40%;
            left: 15px;
        }
        
        .bracket-right {
            top: 40%;
            right: 15px;
        }
        
        .connection-dots {
            position: absolute;
            left: 50%;
            transform: translateX(-50%);
            z-index: 5;
        }
        
        .dot {
            width: 8px;
            height: 8px;
            background-color: #8A2BE2;
            border-radius: 50%;
            position: absolute;
        }
        
        .connection-line {
            position: absolute;
            height: 2px;
            background: linear-gradient(to right, transparent, #00FFFF, transparent);
        }
        
        /* Title page styling */
        .title-page {
            page-break-after: always;
            width: 100%;
            height: 100vh;
            background: white;
            display: flex;
            flex-direction: column;
            text-align: center;
            padding: 2in 1in;
            box-sizing: border-box;
            position: relative;
        }
        
        .title-content {
            display: flex;
            flex-direction: column;
            height: 100%;
        }
        
        .book-title-section {
            margin-top: 3in;
            margin-bottom: auto;
        }
        
        .title-main {
            font-size: 42pt;
            font-weight: bold;
            color: #2c3e50;
            margin: 0 0 2em 0;
            line-height: 1.2;
            letter-spacing: 1px;
        }
        
        .title-subtitle {
            font-size: 24pt;
            color: #3498db;
            font-weight: 300;
            margin: 0;
        }
        
        .author-bottom {
            margin-top: auto;
            margin-bottom: 1in;
        }
        
        .title-author {
            font-size: 24pt;
            color: #2c3e50;
            font-weight: 400;
            margin: 0;
        }
        
        /* Book content styling */
        .book-content {
            font-family: inherit;
        }
        
        .index-section {
            page-break-before: always;
        }
        
        /* CRITICAL: Ultimate href link preservation */
        a[href^="#"] {
            color: #3498db !important;
            text-decoration: none !important;
        }
        
        a {
            color: #3498db !important;
            text-decoration: none !important;
        }
        
        a:hover {
            text-decoration: underline !important;
        }
        
        /* Table of Contents links */
        .toc a {
            color: #3498db !important;
        }
        
        /* Navigation links */
        .navigation a {
            color: #3498db !important;
        }
        
        /* Index links */
        .index-entry a {
            color: #3498db !important;
        }
        
        /* Page numbering */
        @page {
            size: letter;
            margin: 2cm;
            @bottom-center {
                content: "Page " counter(page);
                font-size: 10pt;
                color: #666;
            }
        }
        
        /* Cover and title pages without page numbers */
        .cover-page {
            page-break-after: always;
        }
        
        
        .title-page {
            page-break-after: always;
        }
        
        
        /* Ensure chapters have proper page breaks */
        .chapter {
            page-break-before: always;
        }
        
        .chapter:first-of-type {
            page-break-before: avoid;
        }
        
        .title-container {
            text-align: center;
            margin-top: 1.5in;
            position: relative;
            z-index: 10;
        }
        
        .main-title {
            color: white;
            font-family: 'Arial', sans-serif;
            font-size: 36pt;
            font-weight: 800;
            margin: 0;
            line-height: 1.2;
            letter-spacing: 1px;
            text-shadow: 0px 2px 4px rgba(0,0,0,0.5);
        }
        
        .subtitle {
            color: #B5D0FF;
            font-family: 'Arial', sans-serif;
            font-size: 18pt;
            font-weight: 300;
            margin-top: 15px;
            margin-bottom: 0;
        }
        
        .graphic {
            margin: 0.8in auto;
            height: 3in;
            position: relative;
            display: flex;
            justify-content: center;
            align-items: center;
        }
        
        .code-column {
            background-color: rgba(30, 30, 30, 0.7);
            border-radius: 5px;
            padding: 15px;
            width: 1.8in;
            font-family: 'Courier New', monospace;
            font-size: 11pt;
            color: #d4d4d4;
            box-shadow: 0 5px 15px rgba(0,0,0,0.3);
            z-index: 10;
            position: relative;
            left: -30px;
        }
        
        .prompt-column {
            background-color: rgba(49, 49, 68, 0.7);
            border-radius: 5px;
            padding: 15px;
            width: 1.8in;
            font-family: 'Arial', sans-serif;
            font-size: 11pt;
            color: #e0e0e0;
            box-shadow: 0 5px 15px rgba(0,0,0,0.3);
            z-index: 10;
            position: relative;
            right: -30px;
        }
        
        .connector-graphic {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: 5;
        }
        
        .keyword { color: #569CD6; }
        .string { color: #CE9178; }
        .comment { color: #6A9955; }
        .function { color: #DCDCAA; }
        .variable { color: #9CDCFE; }
        
        .author {
            text-align: center;
            position: absolute;
            bottom: 0.8in;
            left: 0;
            width: 100%;
            color: white;
            font-family: 'Arial', sans-serif;
            font-size: 14pt;
            font-weight: 300;
            z-index: 10;
        }
        
        .brackets {
            position: absolute;
            font-size: 120pt;
            font-weight: 300;
            color: rgba(255,255,255,0.1);
            font-family: 'Courier New', monospace;
        }
        
        .bracket-left {
            top: 40%;
            left: 15px;
        }
        
        .bracket-right {
            top: 40%;
            right: 15px;
        }
        
        .connection-dots {
            position: absolute;
            left: 50%;
            transform: translateX(-50%);
            z-index: 5;
        }
        
        .dot {
            width: 8px;
            height: 8px;
            background-color: #8A2BE2;
            border-radius: 50%;
            position: absolute;
        }
        
        .connection-line {
            position: absolute;
            height: 2px;
            background: linear-gradient(to right, transparent, #00FFFF, transparent);
        }
        
       .book-title1 {
            margin-top: 2in;
            padding: 0 1in;
        }
        
        .main-title1 {
            font-size: 20pt;
            font-weight: bold;
            color: #2c3e50;
            margin: 0;
            line-height: 1.2;
            letter-spacing: 1px;
        }
        
        .subtitle1 {
            font-size: 16pt;
            color: #3498db;
            font-weight: 300;
            margin: 20px 0 0 0;
        }
        
        .author-section1 {
            position: absolute;
            bottom: 2in;
            left: 0;
            right: 0;
            padding: 0 1in;
            text-align: center;
        }
        
        .author-name1 {
            font-size: 18pt;
            color: #2c3e50;
            font-weight: 400;
            margin: 0;
            bottom: 1in;
        }
        /* Title cover styling */
        .title-cover {
            position: relative;
            margin: 0;
            font-family: "Helvetica", "Arial", sans-serif;
            width: 100%;
            height: 100vh;
            background: white;
            padding: 0.5in;
            box-sizing: border-box;
            text-align: center;
        }
        
        .title-cover {
            page-break-after: always;
        }
        
        /* Blank page styling */
        .blank-page {
            page-break-after: always;
            width: 100%;
            height: 100vh;
            background: white;
        }
        
        /* Page numbering - exclude cover and blank pages */
        @page cover {
            @bottom-center {
                content: none;
            }
        }
        
        @page blank {
            @bottom-center {
                content: none;
            }
        }
        
        .cover-page {
            page: cover;
        }
        
        .blank-page {
            page: blank;
        }
        
    </style>
</head>
<body>
    <!-- Cover Page -->
    <div class="cover-page">
        <div class="title-container">
            <h1 class="main-title">PROMPT ENGINEERING</h1>
            <h2 class="main-title" style="font-size: 28pt;">FOR DEVELOPERS</h2>
            <p class="subtitle">Crafting Intelligent LLM Solutions</p>
        </div>
        
        <div class="graphic">
            <div class="code-column">
                <span class="keyword">def</span> <span class="function">prompt_for_code</span>():
                <br>&nbsp;&nbsp;<span class="variable">result</span> = <span class="string">"hello"</span>
                <br>&nbsp;&nbsp;<span class="keyword">return</span> <span class="variable">result</span>
            </div>
            
            <div class="connection-dots">
                <!-- Connection dots between code and natural language -->
                <div class="connection-line" style="width: 100px; top: 50px; left: -50px;"></div>
                <div class="dot" style="top: 50px; left: -40px;"></div>
                <div class="dot" style="top: 50px; left: -20px;"></div>
                <div class="dot" style="top: 50px; left: 0px;"></div>
                <div class="dot" style="top: 50px; left: 20px;"></div>
                <div class="dot" style="top: 50px; left: 40px;"></div>
                
                <div class="connection-line" style="width: 100px; top: 100px; left: -50px;"></div>
                <div class="dot" style="top: 100px; left: -40px;"></div>
                <div class="dot" style="top: 100px; left: -20px;"></div>
                <div class="dot" style="top: 100px; left: 0px;"></div>
                <div class="dot" style="top: 100px; left: 20px;"></div>
                <div class="dot" style="top: 100px; left: 40px;"></div>
                
                <div class="connection-line" style="width: 100px; top: 150px; left: -50px;"></div>
                <div class="dot" style="top: 150px; left: -40px;"></div>
                <div class="dot" style="top: 150px; left: -20px;"></div>
                <div class="dot" style="top: 150px; left: 0px;"></div>
                <div class="dot" style="top: 150px; left: 20px;"></div>
                <div class="dot" style="top: 150px; left: 40px;"></div>
            </div>
            
            <div class="prompt-column">
                Write a Python function that:
                <br>1. Takes a string input
                <br>2. Processes the text
                <br>3. Returns a greeting
            </div>
        </div>
        
        <div class="author">By Sunny Shivam</div>
        
        <div class="brackets bracket-left">{</div>
        <div class="brackets bracket-right">}</div>
    </div>
    
    <!-- Blank Page -->
    <div class="blank-page"></div>
    
    <!-- Title Page -->
    
    <div class="title-cover">
        <div class="book-title1">
            <h2 class="main-title1">PROMPT ENGINEERING FOR DEVELOPERS</h2>
            <br>
            <p class="subtitle1">Crafting Intelligent LLM Solutions</p>
            <br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
            <p class="author-name1"> sunny shivam </p>    
        </div>
    </div>

    <!-- Title Page -->
    <!--
    <div class="title-page">
        <div class="title-content">
            <div class="book-title-section">
                <h1 class="title-main">PROMPT ENGINEERING FOR DEVELOPERS</h1>
                <p class="title-subtitle">Crafting Intelligent LLM Solutions</p>
            </div>
            <div class="author-bottom">
                <p class="title-author">By Sunny Shivam</p>
            </div>
        </div>
    </div>
    -->
<div class="book-content">

    <div class="front-matter" id="about-book"><h1 id="about-the-book">About the Book</h1>
<p><strong>Prompt Engineering for Developers: Crafting Intelligent LLM Solutions</strong> is a comprehensive, hands-on guide designed specifically for developers who want to harness the power of large language models (LLMs) in their daily work and projects. Rather than focusing on theoretical concepts, this book takes a practical, example-driven approach to teaching prompt engineering as an essential developer skill.</p>
<h3 id="who-this-book-is-for">Who This Book Is For</h3>
<p>This book is written for:
- <strong>Software developers</strong> with Python knowledge who want to integrate LLMs into their applications
- <strong>Data scientists</strong> looking to leverage LLMs for analysis, explanation, and automation tasks<br />
- <strong>Engineering teams</strong> seeking to improve productivity through AI-powered development tools
- <strong>Technical leads</strong> evaluating how to responsibly adopt LLM technology in their organizations</p>
<h3 id="what-youll-learn">What You'll Learn</h3>
<p>The book progresses from foundational concepts to advanced implementation techniques:</p>
<p><strong>Core Foundations</strong>: Understanding LLMs from a developer's perspective, including API integration, cost management, and ethical considerations for responsible AI development.</p>
<p><strong>Practical Techniques</strong>: Master essential prompting patterns for code generation, debugging, documentation, and data transformation—with examples across multiple programming languages.</p>
<p><strong>Advanced Methods</strong>: Learn sophisticated techniques like chain-of-thought prompting, self-correction strategies, and prompt orchestration for complex workflows.</p>
<p><strong>Real-World Projects</strong>: Build three comprehensive applications:
- A smart code assistant for automating repetitive development tasks
- An LLM-powered ML model explainer for making complex models understandable
- A training debugger that analyzes ML experiments and suggests optimizations</p>
<p><strong>Production-Ready Practices</strong>: Discover how to build robust, testable, and cost-effective LLM-powered systems that integrate seamlessly with existing development workflows.</p>
<h3 id="why-this-book-is-different">Why This Book Is Different</h3>
<p>Unlike academic treatments of prompt engineering, this book focuses relentlessly on practical application. Every concept is illustrated with working code, real-world scenarios, and actionable insights that developers can immediately apply. The author's 15 years of software development experience ensures that the advice is grounded in the realities of building and maintaining production systems.</p>
<p>The book emphasizes building a strong foundation in problem-solving and understanding the "why" behind each technique—empowering readers to adapt and innovate as the rapidly evolving field of AI continues to advance.</p>
<p>Whether you're looking to automate routine tasks, build intelligent features, or simply understand how to work effectively with LLMs, this book provides the practical knowledge and hands-on experience needed to succeed in the age of AI-assisted development.</p></div><div class="front-matter" id="about-author"><h1 id="about-the-author">About the Author</h1>
<p><strong>Sunny Shivam</strong> is a seasoned software developer and data science enthusiast with over 15 years of experience building intelligent systems and scalable applications. He holds an M.Tech in Data Science from BITS Pilani, where he developed deep expertise in machine learning, artificial intelligence, and data-driven solution design.</p>
<p>Throughout his career, Sunny has worked across diverse industries, from early-stage startups to enterprise organizations, consistently bridging the gap between cutting-edge research and practical implementation. His passion lies in making complex technologies accessible to developers, enabling teams to leverage the power of AI and machine learning in real-world applications.</p>
<p>As an early adopter of large language models in development workflows, Sunny has hands-on experience integrating LLMs into production systems, optimizing prompts for various use cases, and building robust AI-powered tools. He has mentored numerous developers in adopting prompt engineering practices and has contributed to open-source projects focused on developer productivity and AI tooling.</p></div>
    <div class="toc">
        <h1>Contents</h1>
        <ul>
    
        <li>
            <div class="toc-entry">
                <a href="#chapter1">
                    <span class="chapter-number">1.</span>
                    <span class="chapter-title">Introduction to Prompt Engineering: The Developer's New Skillset</span>
                </a>
                <span class="dots"></span>
                <span class="page-number">6</span>
            </div>
            <ul>
        
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter1_section1">What is Prompt Engineering and why it matters for developers</a>
                    <span class="dots"></span>
                    <span class="page-number">6</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter1_section2">LLMs as programmable interfaces</a>
                    <span class="dots"></span>
                    <span class="page-number">7</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter1_section3">Ethical considerations and responsible use for developers</a>
                    <span class="dots"></span>
                    <span class="page-number">8</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter1_section4">Setting up your development environment</a>
                    <span class="dots"></span>
                    <span class="page-number">9</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter1_section5">Version control for prompts in development workflows</a>
                    <span class="dots"></span>
                    <span class="page-number">10</span>
                </div>
            </li>
            
            </ul>
        </li>
        
        <li>
            <div class="toc-entry">
                <a href="#chapter2">
                    <span class="chapter-number">2.</span>
                    <span class="chapter-title">Understanding LLMs: A Developer's Perspective</span>
                </a>
                <span class="dots"></span>
                <span class="page-number">12</span>
            </div>
            <ul>
        
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter2_section1">Brief overview of LLM capabilities and limitations</a>
                    <span class="dots"></span>
                    <span class="page-number">12</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter2_section2">Popular LLM APIs</a>
                    <span class="dots"></span>
                    <span class="page-number">14</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter2_section3">Basic API calls and handling responses</a>
                    <span class="dots"></span>
                    <span class="page-number">16</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter2_section4">Troubleshooting common API issues</a>
                    <span class="dots"></span>
                    <span class="page-number">18</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter2_section5">Cost considerations and token management</a>
                    <span class="dots"></span>
                    <span class="page-number">20</span>
                </div>
            </li>
            
            </ul>
        </li>
        
        <li>
            <div class="toc-entry">
                <a href="#chapter3">
                    <span class="chapter-number">3.</span>
                    <span class="chapter-title">The Art and Science of Prompt Construction</span>
                </a>
                <span class="dots"></span>
                <span class="page-number">25</span>
            </div>
            <ul>
        
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter3_section1">Anatomy of a Prompt: Instructions, Context, Input Data, Output Format</a>
                    <span class="dots"></span>
                    <span class="page-number">25</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter3_section2">Core Principles: Clarity, Specificity, Conciseness, Role-playing, Constraints</a>
                    <span class="dots"></span>
                    <span class="page-number">28</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter3_section3">Basic Techniques: Zero-shot, Few-shot, Instruction-based</a>
                    <span class="dots"></span>
                    <span class="page-number">32</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter3_section4">Testing and evaluating prompt effectiveness</a>
                    <span class="dots"></span>
                    <span class="page-number">35</span>
                </div>
            </li>
            
            </ul>
        </li>
        
        <li>
            <div class="toc-entry">
                <a href="#chapter4">
                    <span class="chapter-number">4.</span>
                    <span class="chapter-title">Essential Prompting Patterns for Developers</span>
                </a>
                <span class="dots"></span>
                <span class="page-number">38</span>
            </div>
            <ul>
        
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter4_section1">Code Generation: Generating functions, classes, scripts</a>
                    <span class="dots"></span>
                    <span class="page-number">38</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter4_section2">Code Explanation & Documentation</a>
                    <span class="dots"></span>
                    <span class="page-number">42</span>
                </div>
            </li>
            
            </ul>
        </li>
        
        <li>
            <div class="toc-entry">
                <a href="#chapter5">
                    <span class="chapter-number">5.</span>
                    <span class="chapter-title">Advanced Prompting Techniques for Enhanced Control</span>
                </a>
                <span class="dots"></span>
                <span class="page-number">55</span>
            </div>
            <ul>
        
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter5_section1">Chain-of-Thought (CoT): Guiding LLMs through multi-step reasoning</a>
                    <span class="dots"></span>
                    <span class="page-number">55</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter5_section2">Self-Correction & Iterative Prompting</a>
                    <span class="dots"></span>
                    <span class="page-number">58</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter5_section3">Controlling Output: Temperature, Top-P/Top-K</a>
                    <span class="dots"></span>
                    <span class="page-number">62</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter5_section4">Persona-Based Prompting</a>
                    <span class="dots"></span>
                    <span class="page-number">65</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter5_section5">Prompt chaining and orchestration techniques</a>
                    <span class="dots"></span>
                    <span class="page-number">68</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter5_section6">Error handling strategies</a>
                    <span class="dots"></span>
                    <span class="page-number">70</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter5_section7">Evaluating LLM output quality</a>
                    <span class="dots"></span>
                    <span class="page-number">72</span>
                </div>
            </li>
            
            </ul>
        </li>
        
        <li>
            <div class="toc-entry">
                <a href="#chapter6">
                    <span class="chapter-number">6.</span>
                    <span class="chapter-title">Building Effective Developer Tooling for LLM Applications</span>
                </a>
                <span class="dots"></span>
                <span class="page-number">75</span>
            </div>
            <ul>
        
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter6_section1">Prompt libraries and reuse patterns</a>
                    <span class="dots"></span>
                    <span class="page-number">75</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter6_section2">Debugging tools for LLM applications</a>
                    <span class="dots"></span>
                    <span class="page-number">78</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter6_section3">Performance profiling and optimization</a>
                    <span class="dots"></span>
                    <span class="page-number">82</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter6_section4">Integration with existing development workflows</a>
                    <span class="dots"></span>
                    <span class="page-number">85</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter6_section5">Testing frameworks for LLM-powered features</a>
                    <span class="dots"></span>
                    <span class="page-number"></span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter6_section6">Cost optimization techniques (token counting, caching responses)</a>
                    <span class="dots"></span>
                    <span class="page-number"></span>
                </div>
            </li>
            
            </ul>
        </li>
        
        <li>
            <div class="toc-entry">
                <a href="#chapter7">
                    <span class="chapter-number">7.</span>
                    <span class="chapter-title">Hands-on Project 1: Building a Smart Code Assistant</span>
                </a>
                <span class="dots"></span>
                <span class="page-number">88</span>
            </div>
            <ul>
        
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter7_section1">Scenario: Automating common coding tasks</a>
                    <span class="dots"></span>
                    <span class="page-number">88</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter7_section2">Problem: Manual, repetitive coding tasks</a>
                    <span class="dots"></span>
                    <span class="page-number">90</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter7_section3">Solution: A Python script using LLMs</a>
                    <span class="dots"></span>
                    <span class="page-number">92</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter7_section4">Focus: Practical application of prompts</a>
                    <span class="dots"></span>
                    <span class="page-number">95</span>
                </div>
            </li>
            
            </ul>
        </li>
        
        <li>
            <div class="toc-entry">
                <a href="#chapter8">
                    <span class="chapter-number">8.</span>
                    <span class="chapter-title">Hands-on Project 2: LLM-Powered ML Model Explainer</span>
                </a>
                <span class="dots"></span>
                <span class="page-number">98</span>
            </div>
            <ul>
        
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter8_section1">Scenario: Understanding complex machine learning models</a>
                    <span class="dots"></span>
                    <span class="page-number">98</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter8_section2">Problem: Difficulty interpreting ML model architecture</a>
                    <span class="dots"></span>
                    <span class="page-number">100</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter8_section3">Solution: Building an interactive tool</a>
                    <span class="dots"></span>
                    <span class="page-number">102</span>
                </div>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter8_section4">Focus: Using prompts to translate technical ML concepts</a>
                    <span class="dots"></span>
                    <span class="page-number">105</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter8_section5">Implementation: Creating a Python tool</a>
                    <span class="dots"></span>
                    <span class="page-number">108</span>
                </div>
            </li>
            
            </ul>
        </li>
        
        <li>
            <div class="toc-entry">
                <a href="#chapter9">
                    <span class="chapter-number">9.</span>
                    <span class="chapter-title">Hands-on Project 3: ML Training Debugger and Optimizer</span>
                </a>
                <span class="dots"></span>
                <span class="page-number">112</span>
            </div>
            <ul>
        
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter9_section1">Scenario: Debugging issues in ML model training</a>
                    <span class="dots"></span>
                    <span class="page-number">112</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter9_section2">Problem: Interpreting training logs</a>
                    <span class="dots"></span>
                    <span class="page-number">115</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter9_section3">Solution: A tool that analyzes training metrics</a>
                    <span class="dots"></span>
                    <span class="page-number">118</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter9_section4">Focus: Using LLMs to troubleshoot ML training</a>
                    <span class="dots"></span>
                    <span class="page-number">120</span>
                </div>
            </li>
            
            <li class="section-entry">
                <div class="toc-entry">
                    <a href="#chapter9_section5">Implementation: Building a system</a>
                    <span class="dots"></span>
                    <span class="page-number">122</span>
                </div>
            </li>
            
            </ul>
        </li>
        
        </ul>
    </div>
    <div class="chapter" id="chapter-content-1"><h1 id="chapter1">Chapter 1: Introduction to Prompt Engineering: The Developer's New Skillset</h1>
<h2 id="chapter1_section1">What is Prompt Engineering and Why It Matters for Developers</h2>
<p>Prompt engineering is the practice of crafting effective inputs (prompts) to large language models (LLMs) to obtain desired outputs. For developers, it represents a paradigm shift in how we interact with software tools. Unlike traditional programming, where we write explicit instructions in code that machines follow precisely, prompt engineering involves communicating with AI systems in natural language to achieve computational goals.</p>
<p>The importance of prompt engineering for developers cannot be overstated:</p>
<ol>
<li>
<p><strong>New Interface to Computing Resources</strong>: Prompts are becoming a universal interface to powerful computational capabilities that would otherwise require complex programming or specialized knowledge.</p>
</li>
<li>
<p><strong>Productivity Multiplier</strong>: Well-crafted prompts can dramatically accelerate development tasks like code generation, debugging, documentation, and data transformation.</p>
</li>
<li>
<p><strong>Competitive Advantage</strong>: As LLMs become integrated into development workflows, proficiency in prompt engineering provides a significant edge in the job market.</p>
</li>
<li>
<p><strong>Bridge Between Technical and Non-Technical Domains</strong>: Prompt engineering enables developers to work more effectively with non-technical stakeholders by transforming natural language requirements into working solutions more directly.</p>
</li>
</ol>
<h2 id="chapter1_section2">LLMs as Programmable Interfaces</h2>
<p>Large Language Models represent a fundamentally new type of programmable interface with unique characteristics:</p>
<h3 id="from-apis-to-llms-a-shift-in-abstraction">From APIs to LLMs: A Shift in Abstraction</h3>
<p>Traditional APIs require developers to:
- Learn specific endpoints and parameters
- Format data according to strict schemas
- Handle errors through defined error codes
- Work within rigid constraints of what the API can do</p>
<p>In contrast, LLMs offer:
- Natural language interaction
- Flexibility in input formatting
- Graceful handling of ambiguity
- The ability to perform a vastly wider range of tasks through a single interface</p>
<h3 id="the-programming-model-of-llms">The Programming Model of LLMs</h3>
<pre class="codehilite"><code class="language-python"># Traditional API call
response = requests.post(
    &quot;https://api.example.com/translate&quot;,
    json={&quot;text&quot;: &quot;Hello world&quot;, &quot;source&quot;: &quot;en&quot;, &quot;target&quot;: &quot;fr&quot;},
    headers={&quot;Authorization&quot;: &quot;Bearer &quot; + API_KEY}
)
result = response.json()[&quot;translated_text&quot;]

# vs. LLM-based approach
response = openai.ChatCompletion.create(
    model=&quot;gpt-4&quot;,
    messages=[
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Translate 'Hello world' from English to French.&quot;}
    ]
)
result = response.choices[0].message.content
</code></pre>

<p>This fundamental shift means that developers need to think differently about:</p>
<ul>
<li><strong>Input Design</strong>: Crafting prompts that clearly communicate intent</li>
<li><strong>Output Parsing</strong>: Extracting structured information from natural language responses</li>
<li><strong>Error Handling</strong>: Dealing with hallucinations, misunderstandings, and irrelevant outputs</li>
<li><strong>Iteration</strong>: Refining prompts based on observed outputs</li>
</ul>
<h2 id="chapter1_section3">Ethical Considerations and Responsible Use for Developers</h2>
<p>As developers integrating LLMs into applications, we carry significant responsibility for the ethical use of these tools:</p>
<h3 id="potential-ethical-issues">Potential Ethical Issues</h3>
<ol>
<li>
<p><strong>Bias Amplification</strong>: LLMs may reproduce or amplify social biases present in their training data, which can manifest in generated code, documentation, or user-facing content.</p>
</li>
<li>
<p><strong>Misinformation Risk</strong>: LLMs can generate plausible but incorrect information, including invalid code, inaccurate explanations, or false claims about technical topics.</p>
</li>
<li>
<p><strong>Intellectual Property Concerns</strong>: Generated code may raise questions about originality, licensing, and attribution.</p>
</li>
<li>
<p><strong>Security Vulnerabilities</strong>: LLMs may inadvertently suggest code with security flaws or sensitive information disclosure.</p>
</li>
</ol>
<h3 id="responsible-development-practices">Responsible Development Practices</h3>
<p>As developers working with LLMs, we should:</p>
<ul>
<li><strong>Validate Outputs</strong>: Never blindly integrate LLM-generated code without review and testing</li>
<li><strong>Set Clear Boundaries</strong>: Be explicit about what types of requests your LLM-powered application should and should not fulfill</li>
<li><strong>Provide Attribution</strong>: When appropriate, disclose the use of AI-generated content</li>
<li><strong>Design for Transparency</strong>: Make users aware when they're interacting with AI systems</li>
<li><strong>Monitor for Bias</strong>: Regularly audit system outputs for signs of harmful bias</li>
</ul>
<h2 id="chapter1_section4">Setting Up Your Development Environment</h2>
<p>To begin working with LLMs as a developer, you'll need to set up a proper environment:</p>
<h3 id="api-keys-and-access">API Keys and Access</h3>
<p>Most commercial LLM providers require authentication via API keys:</p>
<ol>
<li><strong>OpenAI (GPT-3.5, GPT-4)</strong>:</li>
<li>Create an account at <a href="https://platform.openai.com">platform.openai.com</a></li>
<li>Navigate to API keys section and generate a new key</li>
<li>
<p>Set up billing (required for API access)</p>
</li>
<li>
<p><strong>Google (Gemini)</strong>:</p>
</li>
<li>Get started at <a href="https://ai.google.dev">ai.google.dev</a></li>
<li>Create a project in Google Cloud Console</li>
<li>
<p>Enable the Gemini API and generate credentials</p>
</li>
<li>
<p><strong>Anthropic (Claude)</strong>:</p>
</li>
<li>Request access at <a href="https://anthropic.com/earlyaccess">anthropic.com/earlyaccess</a></li>
<li>Once approved, generate API keys from the console</li>
</ol>
<h3 id="essential-python-libraries">Essential Python Libraries</h3>
<pre class="codehilite"><code class="language-bash"># Core LLM interaction libraries
pip install openai google-generativeai anthropic

# Utility libraries for LLM applications
pip install langchain llama-index

# For embedding and vector operations
pip install sentence-transformers numpy

# Environment management
pip install python-dotenv
</code></pre>

<h3 id="environment-configuration">Environment Configuration</h3>
<p>Best practice is to store API keys securely using environment variables:</p>
<pre class="codehilite"><code class="language-python"># .env file (add to .gitignore)
OPENAI_API_KEY=sk-your-key-here
ANTHROPIC_API_KEY=sk-ant-your-key-here
GOOGLE_API_KEY=your-google-key-here

# In your Python code
import os
from dotenv import load_dotenv

load_dotenv()  # Load environment variables from .env file

openai_api_key = os.getenv(&quot;OPENAI_API_KEY&quot;)
</code></pre>

<h2 id="chapter1_section5">Version Control for Prompts in Development Workflows</h2>
<p>As your prompt engineering practice matures, treating prompts as first-class development artifacts becomes essential:</p>
<h3 id="why-version-control-prompts">Why Version Control Prompts?</h3>
<ol>
<li><strong>Reproducibility</strong>: Ensuring consistent LLM behavior across development, testing, and production</li>
<li><strong>Collaboration</strong>: Enabling team members to review and improve prompts</li>
<li><strong>Quality Assurance</strong>: Tracking changes to identify when and how prompt modifications affect system behavior</li>
<li><strong>Auditability</strong>: Maintaining records of prompt evolution for compliance or troubleshooting</li>
</ol>
<h3 id="practical-approaches-to-prompt-version-control">Practical Approaches to Prompt Version Control</h3>
<h4 id="1-structured-prompt-storage">1. Structured Prompt Storage</h4>
<pre class="codehilite"><code class="language-python"># prompts/code_generation.py
CODE_GENERATION_PROMPT = &quot;&quot;&quot;
You are an expert Python developer. Generate a well-documented 
function that {task_description}. Follow these guidelines:
- Use type hints
- Include docstrings in Google format
- Follow PEP 8 style conventions
- Handle edge cases appropriately
&quot;&quot;&quot;
</code></pre>

<h4 id="2-prompt-templates-with-variables">2. Prompt Templates with Variables</h4>
<pre class="codehilite"><code class="language-python"># Using a library like Jinja2 for template management
from jinja2 import Template

code_gen_template = Template(&quot;&quot;&quot;
You are an expert {{ language }} developer with {{ years_experience }}+ years of experience.
Generate a {{ purpose }} that accomplishes the following:

{{ task_description }}

Requirements:
{% for req in requirements %}
- {{ req }}
{% endfor %}
&quot;&quot;&quot;)

# Generate specific prompt
prompt = code_gen_template.render(
    language=&quot;Python&quot;,
    years_experience=5,
    purpose=&quot;data processing function&quot;,
    task_description=&quot;Parse CSV files and extract specific columns&quot;,
    requirements=[&quot;Handle malformed data&quot;, &quot;Be memory efficient&quot;, &quot;Include logging&quot;]
)
</code></pre>

<h4 id="3-prompt-versioning-strategies">3. Prompt Versioning Strategies</h4>
<p>Consider creating a formal system for prompt versioning:</p>
<pre class="codehilite"><code class="language-python"># prompts/registry.py
PROMPTS = {
    &quot;code_generation&quot;: {
        &quot;v1&quot;: &quot;Original prompt focused on basic functionality&quot;,
        &quot;v2&quot;: &quot;Added type hints and docstring requirements&quot;,
        &quot;v3&quot;: &quot;Expanded to include edge case handling guidance&quot;,
        &quot;current&quot;: &quot;v3&quot;  # Pointer to current version
    },
    &quot;code_explanation&quot;: {
        &quot;v1&quot;: &quot;Basic explanation format&quot;,
        &quot;v2&quot;: &quot;Enhanced with step-by-step breakdown&quot;,
        &quot;current&quot;: &quot;v2&quot;
    }
}
</code></pre>

<h2 id="chapter1_section6">Conclusion</h2>
<p>Prompt engineering represents a fundamental new skill for developers in the age of AI. By understanding the principles of effective prompt design, considering ethical implications, setting up a proper development environment, and treating prompts as versioned artifacts, you can harness the power of LLMs to transform your development workflow.</p>
<p>In the next chapter, we'll explore the technical underpinnings of LLMs and develop a deeper understanding of their capabilities and limitations from a developer's perspective.</p>
<h2 id="chapter1_section7">Exercises</h2>
<ol>
<li>Set up your development environment with access to at least one LLM API.</li>
<li>Write a prompt that generates a function in your preferred programming language and experiment with variations to see how the output changes.</li>
<li>Create a simple versioning scheme for your prompts and implement it in a small project.</li>
<li>Reflect on potential ethical considerations for an LLM-powered application you might want to build.</li>
</ol></div><div class="navigation"><div></div><a href="#chapter2" class="next-chapter">Next Chapter</a></div><div class="chapter" id="chapter-content-2"><h1 id="chapter2">Chapter 2: Understanding LLMs: A Developer's Perspective</h1>
<h2 id="chapter2_section1">Brief Overview of LLM Capabilities and Limitations</h2>
<p>Large Language Models (LLMs) represent a revolutionary class of AI systems with capabilities that can transform development workflows. To effectively leverage these systems, developers must understand both what they excel at and where they fall short.</p>
<h3 id="key-capabilities">Key Capabilities</h3>
<ol>
<li>
<p><strong>Natural Language Understanding</strong>: LLMs can parse and interpret complex instructions, requirements, and queries written in natural language.</p>
</li>
<li>
<p><strong>Code Generation</strong>: Modern LLMs can generate syntactically correct code across dozens of programming languages, from simple functions to complex classes and algorithms.</p>
</li>
<li>
<p><strong>Code Explanation</strong>: LLMs can analyze existing code and provide explanations of its purpose, logic, and implementation details.</p>
</li>
<li>
<p><strong>Translation</strong>: LLMs can translate between natural languages and between programming languages.</p>
</li>
<li>
<p><strong>Data Extraction and Transformation</strong>: LLMs can parse unstructured data and convert it into structured formats.</p>
</li>
<li>
<p><strong>Problem-Solving</strong>: LLMs can apply reasoning to debug code, optimize algorithms, and solve programming challenges.</p>
</li>
</ol>
<h3 id="key-limitations">Key Limitations</h3>
<ol>
<li><strong>Hallucinations</strong>: LLMs can generate content that appears plausible but is factually incorrect or nonsensical. This includes:</li>
<li>Inventing non-existent functions or libraries</li>
<li>Creating syntactically correct but logically flawed code</li>
<li>
<p>Confidently stating incorrect technical information</p>
</li>
<li>
<p><strong>Context Window Constraints</strong>: LLMs have fixed limits on how much text they can process in a single interaction:</p>
</li>
<li>GPT-4 (as of this writing): ~32,000 tokens (~24,000 words)</li>
<li>Claude 2: ~100,000 tokens</li>
<li>Gemini Pro: ~32,000 tokens</li>
</ol>
<p>These limitations affect your ability to provide context such as large code files or extensive requirements.</p>
<ol>
<li><strong>Knowledge Cutoffs</strong>: LLMs are trained on data up to a specific date, after which they have no knowledge:</li>
<li>GPT-4: Knowledge cutoff in early 2023</li>
<li>Other models have similar cutoffs</li>
</ol>
<p>This affects their knowledge of recent language features, libraries, and best practices.</p>
<ol>
<li>
<p><strong>Inconsistency</strong>: LLMs may provide different solutions to the same prompt when called multiple times, which can introduce unpredictability in development workflows.</p>
</li>
<li>
<p><strong>Security Risks</strong>: LLMs may inadvertently generate code with security vulnerabilities or suggest unsafe practices.</p>
</li>
</ol>
<h2 id="chapter2_section2">Popular LLM APIs</h2>
<p>Several providers offer LLM access through well-documented APIs. Here's a comparison of the major options:</p>
<h3 id="openai-gpt-35-gpt-4">OpenAI (GPT-3.5, GPT-4)</h3>
<p><strong>Strengths:</strong>
- Industry-leading capabilities for code-related tasks
- Comprehensive documentation and community support
- Flexible API with good tooling ecosystem</p>
<p><strong>Considerations:</strong>
- Higher pricing compared to some alternatives
- Rate limits for new accounts
- Data usage policies that may affect privacy-sensitive applications</p>
<p><strong>API Basics:</strong></p>
<pre class="codehilite"><code class="language-python">import openai

# Configure with your API key
openai.api_key = &quot;your-api-key&quot;

response = openai.ChatCompletion.create(
    model=&quot;gpt-4&quot;,
    messages=[
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant that generates Python code.&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Write a function that finds prime numbers up to n.&quot;}
    ]
)

print(response.choices[0].message.content)
</code></pre>

<h3 id="google-gemini">Google (Gemini)</h3>
<p><strong>Strengths:</strong>
- Strong performance on technical and scientific content
- Integration with Google Cloud ecosystem
- Competitive pricing</p>
<p><strong>Considerations:</strong>
- Newer API with evolving documentation
- May require Google Cloud account setup</p>
<p><strong>API Basics:</strong></p>
<pre class="codehilite"><code class="language-python">import google.generativeai as genai

# Configure with your API key
genai.configure(api_key=&quot;your-api-key&quot;)

model = genai.GenerativeModel('gemini-pro')
response = model.generate_content(&quot;Write a function that finds prime numbers up to n in Python.&quot;)

print(response.text)
</code></pre>

<h3 id="anthropic-claude">Anthropic (Claude)</h3>
<p><strong>Strengths:</strong>
- Very large context window (100K tokens)
- Design focus on safety and reduction of hallucinations
- Clear and thoughtful responses</p>
<p><strong>Considerations:</strong>
- May not match code generation capabilities of GPT-4 for complex tasks
- More limited developer ecosystem</p>
<p><strong>API Basics:</strong></p>
<pre class="codehilite"><code class="language-python">from anthropic import Anthropic

# Initialize with your API key
anthropic = Anthropic(api_key=&quot;your-api-key&quot;)

response = anthropic.completions.create(
    prompt=f&quot;Human: Write a function that finds prime numbers up to n in Python.\n\nAssistant:&quot;,
    model=&quot;claude-2&quot;,
    max_tokens_to_sample=1000
)

print(response.completion)
</code></pre>

<h3 id="open-source-and-self-hosted-options">Open Source and Self-Hosted Options</h3>
<p>For developers with privacy requirements or cost constraints, several open-source LLMs can be self-hosted:</p>
<ul>
<li><strong>Llama 2 / Llama 3</strong>: Meta's powerful open-source models</li>
<li><strong>Mixtral</strong>: Mistral AI's mixture-of-experts model with strong performance</li>
<li><strong>Code Llama</strong>: Specialized for code generation tasks</li>
<li><strong>Falcon</strong>: Technology Innovation Institute's efficient model</li>
</ul>
<p>Self-hosting requires substantial hardware resources but enables complete control over data and usage.</p>
<h2 id="chapter2_section3">Basic API Calls and Handling Responses</h2>
<p>Most LLM interactions follow a similar pattern regardless of provider:</p>
<ol>
<li>Construct a prompt (input)</li>
<li>Send to the LLM API</li>
<li>Receive and process the response</li>
<li>Handle errors and edge cases</li>
</ol>
<h3 id="core-api-interaction-pattern">Core API Interaction Pattern</h3>
<pre class="codehilite"><code class="language-python">import os
import openai
from dotenv import load_dotenv

# Load API key from .env file
load_dotenv()
openai.api_key = os.getenv(&quot;OPENAI_API_KEY&quot;)

def get_completion(prompt, model=&quot;gpt-3.5-turbo&quot;, temperature=0):
    &quot;&quot;&quot;
    Get a completion from the OpenAI API.

    Args:
        prompt: The prompt to send to the API
        model: The model to use (default: gpt-3.5-turbo)
        temperature: Controls randomness (0=deterministic, 1=creative)

    Returns:
        The completion text
    &quot;&quot;&quot;
    try:
        response = openai.ChatCompletion.create(
            model=model,
            messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
            temperature=temperature
        )
        return response.choices[0].message.content
    except openai.error.OpenAIError as e:
        print(f&quot;OpenAI API error: {e}&quot;)
        return None
    except Exception as e:
        print(f&quot;Unexpected error: {e}&quot;)
        return None

# Example usage
result = get_completion(&quot;Write a Python function to calculate the Fibonacci sequence.&quot;)
print(result)
</code></pre>

<h3 id="structured-response-handling">Structured Response Handling</h3>
<p>For development workflows, you often need structured data rather than freeform text. You can request specific JSON formats:</p>
<pre class="codehilite"><code class="language-python">def get_structured_response(prompt, format_instructions, model=&quot;gpt-4&quot;):
    &quot;&quot;&quot;
    Get a structured response from the LLM in JSON format.

    Args:
        prompt: The main prompt/question
        format_instructions: JSON format specification
        model: The model to use

    Returns:
        Parsed JSON response or None on error
    &quot;&quot;&quot;
    import json

    full_prompt = f&quot;&quot;&quot;{prompt}

    Return your response as a JSON object with the following structure:
    {format_instructions}

    Ensure your response is valid JSON.
    &quot;&quot;&quot;

    try:
        response = openai.ChatCompletion.create(
            model=model,
            messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: full_prompt}],
            temperature=0  # Use deterministic output for consistent JSON
        )

        response_text = response.choices[0].message.content

        # Extract JSON part (in case there's surrounding text)
        try:
            # Try to parse the entire response
            parsed = json.loads(response_text)
            return parsed
        except json.JSONDecodeError:
            # If that fails, try to find and extract a JSON block
            import re
            json_match = re.search(r'```json\n(.*?)\n```', response_text, re.DOTALL)
            if json_match:
                try:
                    return json.loads(json_match.group(1))
                except json.JSONDecodeError:
                    print(&quot;JSON parsing failed even after extraction&quot;)
                    return None
            else:
                print(&quot;Could not extract JSON from response&quot;)
                return None

    except Exception as e:
        print(f&quot;Error: {e}&quot;)
        return None

# Example usage
format_spec = &quot;&quot;&quot;{
    &quot;function_name&quot;: &quot;string&quot;,
    &quot;parameters&quot;: [&quot;list of parameter names&quot;],
    &quot;complexity&quot;: &quot;string (O-notation)&quot;,
    &quot;code&quot;: &quot;string (the Python code)&quot;
}&quot;&quot;&quot;

result = get_structured_response(
    &quot;Create a binary search function&quot;, 
    format_spec
)

if result:
    print(f&quot;Function: {result['function_name']}&quot;)
    print(f&quot;Complexity: {result['complexity']}&quot;)
    print(&quot;Code:&quot;)
    print(result['code'])
</code></pre>

<h2 id="chapter2_section4">Troubleshooting Common API Issues</h2>
<p>When working with LLM APIs, several common issues may arise:</p>
<h3 id="1-rate-limiting-and-quota-errors">1. Rate Limiting and Quota Errors</h3>
<p><strong>Symptoms:</strong>
- HTTP 429 "Too Many Requests" errors
- Responses indicating rate limit exceeded</p>
<p><strong>Solutions:</strong></p>
<pre class="codehilite"><code class="language-python">import time

def resilient_completion(prompt, max_retries=5, backoff_factor=2):
    &quot;&quot;&quot;Make API calls with exponential backoff for rate limiting&quot;&quot;&quot;
    retries = 0
    while retries &lt;= max_retries:
        try:
            return openai.ChatCompletion.create(
                model=&quot;gpt-3.5-turbo&quot;,
                messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}]
            )
        except openai.error.RateLimitError:
            wait_time = backoff_factor ** retries
            print(f&quot;Rate limit hit. Waiting {wait_time} seconds...&quot;)
            time.sleep(wait_time)
            retries += 1

    raise Exception(&quot;Max retries exceeded&quot;)
</code></pre>

<h3 id="2-context-length-exceeded">2. Context Length Exceeded</h3>
<p><strong>Symptoms:</strong>
- Errors about tokens or input length being too long
- Truncated responses</p>
<p><strong>Solutions:</strong></p>
<pre class="codehilite"><code class="language-python">def chunked_completion(long_text, question, chunk_size=2000, overlap=200):
    &quot;&quot;&quot;Process long documents by chunking with overlap&quot;&quot;&quot;
    chunks = []
    for i in range(0, len(long_text), chunk_size - overlap):
        chunk = long_text[i:i + chunk_size]
        chunks.append(chunk)

    responses = []
    for i, chunk in enumerate(chunks):
        prompt = f&quot;Document chunk {i+1}/{len(chunks)}:\n\n{chunk}\n\n{question}&quot;
        response = get_completion(prompt)
        responses.append(response)

    # Combine responses
    combined_prompt = f&quot;Based on these analyses of different parts of a document:\n\n&quot;
    for i, r in enumerate(responses):
        combined_prompt += f&quot;Analysis part {i+1}: {r}\n\n&quot;
    combined_prompt += f&quot;Now provide a complete answer to: {question}&quot;

    return get_completion(combined_prompt)
</code></pre>

<h3 id="3-inconsistent-response-formats">3. Inconsistent Response Formats</h3>
<p><strong>Symptoms:</strong>
- JSON parsing errors
- Missing fields in structured outputs
- Format inconsistencies</p>
<p><strong>Solutions:</strong></p>
<pre class="codehilite"><code class="language-python">def format_enforcing_prompt(prompt, expected_format):
    &quot;&quot;&quot;Create a prompt that enforces output format&quot;&quot;&quot;
    formatted_prompt = f&quot;&quot;&quot;
    {prompt}

    Your response MUST follow this exact format:
    {expected_format}

    If your response doesn't match this format exactly, it will break the system.
    Verify your response carefully before submitting.
    &quot;&quot;&quot;
    return formatted_prompt
</code></pre>

<h3 id="4-api-connection-issues">4. API Connection Issues</h3>
<p><strong>Symptoms:</strong>
- Timeouts
- Connection reset errors
- Intermittent failures</p>
<p><strong>Solutions:</strong></p>
<pre class="codehilite"><code class="language-python">import backoff

@backoff.on_exception(backoff.expo, 
                     (openai.error.APIConnectionError, openai.error.ServiceUnavailableError),
                     max_tries=5)
def reliable_completion(prompt, **kwargs):
    &quot;&quot;&quot;Make API calls with automatic retries for connection issues&quot;&quot;&quot;
    return openai.ChatCompletion.create(
        model=&quot;gpt-4&quot;,
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
        **kwargs
    )
</code></pre>

<h2 id="chapter2_section5">Cost Considerations and Token Management</h2>
<p>LLM API costs can rapidly accumulate, especially in production systems. Understanding token usage and implementing cost controls is essential:</p>
<h3 id="understanding-tokens">Understanding Tokens</h3>
<p>Tokens are the fundamental unit of text processing in LLMs:
- A token is approximately 4 characters or 0.75 words in English
- Punctuation and special characters also count as tokens
- Code often uses more tokens than natural language due to special characters and indentation</p>
<h3 id="token-counting">Token Counting</h3>
<pre class="codehilite"><code class="language-python">import tiktoken

def count_tokens(text, model=&quot;gpt-4&quot;):
    &quot;&quot;&quot;Count the number of tokens in a text string&quot;&quot;&quot;
    encoding = tiktoken.encoding_for_model(model)
    tokens = encoding.encode(text)
    return len(tokens)

def estimate_cost(prompt_tokens, completion_tokens, model=&quot;gpt-4&quot;):
    &quot;&quot;&quot;Estimate cost in USD for token usage with common models&quot;&quot;&quot;
    costs = {
        &quot;gpt-4&quot;: {&quot;prompt&quot;: 0.03, &quot;completion&quot;: 0.06},  # per 1K tokens
        &quot;gpt-3.5-turbo&quot;: {&quot;prompt&quot;: 0.0015, &quot;completion&quot;: 0.002},  # per 1K tokens
        &quot;gpt-4-32k&quot;: {&quot;prompt&quot;: 0.06, &quot;completion&quot;: 0.12},  # per 1K tokens
    }

    if model not in costs:
        raise ValueError(f&quot;Unknown model: {model}&quot;)

    prompt_cost = (prompt_tokens / 1000) * costs[model][&quot;prompt&quot;]
    completion_cost = (completion_tokens / 1000) * costs[model][&quot;completion&quot;]

    return prompt_cost + completion_cost

# Example usage
text = &quot;This is a sample prompt that will be sent to the LLM.&quot;
token_count = count_tokens(text)
print(f&quot;Token count: {token_count}&quot;)
print(f&quot;Estimated cost: ${estimate_cost(token_count, 150):.6f}&quot;)
</code></pre>

<h3 id="cost-optimization-strategies">Cost Optimization Strategies</h3>
<ol>
<li>
<p><strong>Use the Right Model for the Task</strong>
   <code>python
   def choose_optimal_model(task_complexity, input_length, budget_sensitivity):
       """Select the most cost-effective model for a given task"""
       if task_complexity == "low" and budget_sensitivity == "high":
           return "gpt-3.5-turbo"  # Cheaper, good for simpler tasks
       elif input_length &gt; 30000:
           return "gpt-4-32k"  # For very long contexts
       else:
           return "gpt-4"  # For complex reasoning tasks</code></p>
</li>
<li>
<p><strong>Prompt Optimization</strong>
   ```python
   # Before: Verbose prompt
   verbose_prompt = """
   I need you to carefully analyze this code and provide a detailed explanation
   of what it does, how it works, and identify any potential bugs or performance
   issues that might exist in the implementation. Please be thorough in your
   analysis and cover all aspects of the code.</p>
</li>
</ol>
<p>def factorial(n):
       if n == 0:
           return 1
       return n * factorial(n-1)
   """</p>
<p># After: Concise prompt
   concise_prompt = """
   Explain this code and identify any issues:</p>
<p>def factorial(n):
       if n == 0:
           return 1
       return n * factorial(n-1)
   """</p>
<p># Token reduction of ~50%
   ```</p>
<ol>
<li><strong>Response Caching</strong>
   ```python
   import hashlib
   import json
   import os
   import pickle</li>
</ol>
<p>class LLMCache:
       """Simple cache for LLM responses to avoid redundant API calls"""</p>
<pre class="codehilite"><code>   def __init__(self, cache_dir=&quot;.llm_cache&quot;):
       os.makedirs(cache_dir, exist_ok=True)
       self.cache_dir = cache_dir

   def _get_cache_key(self, prompt, model, temperature):
       &quot;&quot;&quot;Generate a unique cache key&quot;&quot;&quot;
       key_data = {
           &quot;prompt&quot;: prompt,
           &quot;model&quot;: model,
           &quot;temperature&quot;: temperature
       }
       key_str = json.dumps(key_data, sort_keys=True)
       return hashlib.md5(key_str.encode()).hexdigest()

   def get(self, prompt, model, temperature):
       &quot;&quot;&quot;Retrieve cached response if available&quot;&quot;&quot;
       cache_key = self._get_cache_key(prompt, model, temperature)
       cache_path = os.path.join(self.cache_dir, cache_key)

       if os.path.exists(cache_path):
           try:
               with open(cache_path, 'rb') as f:
                   return pickle.load(f)
           except Exception:
               return None
       return None

   def set(self, prompt, model, temperature, response):
       &quot;&quot;&quot;Cache a response&quot;&quot;&quot;
       cache_key = self._get_cache_key(prompt, model, temperature)
       cache_path = os.path.join(self.cache_dir, cache_key)

       with open(cache_path, 'wb') as f:
           pickle.dump(response, f)
</code></pre>

<p># Usage
   cache = LLMCache()</p>
<p>def cached_completion(prompt, model="gpt-3.5-turbo", temperature=0):
       """Get completion with caching"""
       # Check cache first
       cached = cache.get(prompt, model, temperature)
       if cached:
           print("Cache hit!")
           return cached</p>
<pre class="codehilite"><code>   # If not in cache, call the API
   print(&quot;Cache miss, calling API...&quot;)
   response = openai.ChatCompletion.create(
       model=model,
       messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
       temperature=temperature
   )

   result = response.choices[0].message.content

   # Cache the result
   cache.set(prompt, model, temperature, result)

   return result
</code></pre>

<p>```</p>
<ol>
<li>
<p><strong>Budget Management</strong>
   ```python
   class LLMBudgetManager:
       """Track and limit LLM API spending"""</p>
<p>def <strong>init</strong>(self, daily_budget=1.0):  # Default $1/day
       self.daily_budget = daily_budget
       self.today_spend = 0
       self.today_date = datetime.date.today()</p>
<p>def track_request(self, prompt_tokens, completion_tokens, model):
       """Track cost of a request and check against budget"""
       # Reset counter if it's a new day
       current_date = datetime.date.today()
       if current_date &gt; self.today_date:
           self.today_date = current_date
           self.today_spend = 0</p>
<pre class="codehilite"><code>   # Calculate cost
   cost = estimate_cost(prompt_tokens, completion_tokens, model)
   self.today_spend += cost

   # Check if over budget
   if self.today_spend &gt; self.daily_budget:
       return False, cost, self.today_spend

   return True, cost, self.today_spend
</code></pre>

</li>
</ol>
<p># Usage
   budget_mgr = LLMBudgetManager(daily_budget=5.0)  # $5/day limit</p>
<p>def budget_aware_completion(prompt, model="gpt-3.5-turbo"):
       """Make API calls with budget awareness"""
       # Count tokens in prompt
       prompt_tokens = count_tokens(prompt, model)</p>
<pre class="codehilite"><code>   # Estimate completion tokens (rough estimate)
   est_completion_tokens = prompt_tokens * 1.5

   # Check budget before making call
   allowed, est_cost, total_spend = budget_mgr.track_request(
       prompt_tokens, est_completion_tokens, model
   )

   if not allowed:
       print(f&quot;Request would exceed daily budget. Today's spend: ${total_spend:.2f}&quot;)
       return None

   # Make the API call
   response = openai.ChatCompletion.create(
       model=model,
       messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}]
   )

   # Update with actual tokens used
   actual_completion_tokens = response.usage.completion_tokens
   budget_mgr.track_request(prompt_tokens, actual_completion_tokens, model)

   return response.choices[0].message.content
</code></pre>

<p>```</p>
<h2 id="chapter2_section6">Conclusion</h2>
<p>Understanding LLMs from a developer's perspective involves recognizing both their transformative capabilities and inherent limitations. By mastering the APIs, troubleshooting common issues, and implementing cost-effective practices, you can effectively integrate these powerful tools into your development workflow.</p>
<p>In the next chapter, we'll explore the art and science of prompt construction, focusing on how to craft effective instructions that yield the best possible results from LLMs.</p>
<h2 id="chapter2_section7">Exercises</h2>
<ol>
<li>Set up API access to at least two different LLM providers and compare their responses to the same coding challenge.</li>
<li>Create a utility function that handles token counting, cost estimation, and response caching for LLM API calls.</li>
<li>Experiment with different error handling strategies to make your LLM interactions more resilient.</li>
<li>Compare the token usage and cost between verbose and concise versions of the same prompt.</li>
<li>Implement a simple command-line tool that uses an LLM API to help with a specific development task of your choice.</li>
</ol></div><div class="navigation"><a href="#chapter1" class="prev-chapter">Previous Chapter</a><a href="#chapter3" class="next-chapter">Next Chapter</a></div><div class="chapter" id="chapter-content-3"><h1 id="chapter3">Chapter 3: The Art and Science of Prompt Construction</h1>
<h2 id="chapter3_section1">Anatomy of a Prompt: Instructions, Context, Input Data, Output Format</h2>
<p>A well-constructed prompt is the foundation of effective interaction with Large Language Models. Understanding the key components of prompts helps developers craft instructions that yield predictable, high-quality results.</p>
<h3 id="the-four-core-components">The Four Core Components</h3>
<ol>
<li><strong>Instructions</strong>: Clear directives that tell the model what to do</li>
<li><strong>Context</strong>: Background information that helps the model understand the task</li>
<li><strong>Input Data</strong>: The specific content the model should work with</li>
<li><strong>Output Format</strong>: Specifications for how the response should be structured</li>
</ol>
<p>Let's examine each component in detail:</p>
<h3 id="1-instructions">1. Instructions</h3>
<p>Instructions are explicit directives that guide the model's behavior. They should be:
- Clear and specific
- Action-oriented
- Focused on a single task or a well-defined sequence of tasks</p>
<p><strong>Examples:</strong></p>
<pre class="codehilite"><code>Poor instruction: &quot;Help me with this code.&quot;
Better instruction: &quot;Debug this Python function that should calculate factorial but is producing incorrect results.&quot;

Poor instruction: &quot;Write some documentation.&quot;
Better instruction: &quot;Generate comprehensive JSDoc comments for this JavaScript utility function.&quot;
</code></pre>

<h3 id="2-context">2. Context</h3>
<p>Context provides the background information that helps the model understand the scope, purpose, and constraints of the task. Effective context includes:
- Relevant background information
- Project-specific considerations
- Technical requirements or constraints
- Target audience information</p>
<p><strong>Examples:</strong></p>
<pre class="codehilite"><code>Limited context: &quot;We need API documentation.&quot;
Better context: &quot;We're building a REST API for an e-commerce platform using Express.js. The API will be used by frontend developers who are familiar with React but have limited backend experience. Documentation should be comprehensive yet accessible.&quot;
</code></pre>

<h3 id="3-input-data">3. Input Data</h3>
<p>Input data is the specific content the model needs to process. This might be:
- Code to analyze or modify
- Text to transform
- Data to structure or extract information from
- Problems to solve</p>
<p><strong>Examples:</strong></p>
<pre class="codehilite"><code>Vague input: &quot;Fix my sorting function.&quot;
Better input: 
&quot;Fix the following sorting function that should sort an array of objects by their 'priority' property in descending order:

function sortByPriority(items) {
    return items.sort((a, b) =&gt; a.priority - b.priority);
}
&quot;
</code></pre>

<h3 id="4-output-format">4. Output Format</h3>
<p>Output format specifies how the response should be structured. Clear formatting instructions help ensure the model's response is immediately usable. This might include:
- Specific structural requirements (JSON, XML, etc.)
- Formatting conventions (markdown, HTML, etc.)
- Response sections or components
- Length constraints</p>
<p><strong>Examples:</strong></p>
<pre class="codehilite"><code>Unspecified format: &quot;Give me information about common sorting algorithms.&quot;
Better format specification: &quot;Compare quick sort, merge sort, and bubble sort. Format your response as a markdown table with columns for: Algorithm Name, Average Time Complexity, Space Complexity, Stability, and Best Use Case.&quot;
</code></pre>

<h3 id="putting-it-all-together">Putting It All Together</h3>
<p>Here's an example of a well-constructed prompt that incorporates all four components:</p>
<pre class="codehilite"><code># INSTRUCTIONS
Review the following Python function that calculates Fibonacci numbers and identify any performance issues or bugs. Then provide an optimized version.

# CONTEXT
This function will be used in a web application that needs to calculate Fibonacci numbers up to the 50th number. Performance is critical as this will be called frequently.

# INPUT DATA
```python
def fibonacci(n):
    if n &lt;= 0:
        return 0
    elif n == 1:
        return 1
    else:
        return fibonacci(n-1) + fibonacci(n-2)
</code></pre>

<h1 id="output-format">OUTPUT FORMAT</h1>
<p>Provide your response in the following structure:
1. Issues Identified (bullet points)
2. Optimized Solution (code block with comments)
3. Complexity Analysis (time and space)</p>
<pre class="codehilite"><code>## Core Principles: Clarity, Specificity, Conciseness, Role-playing, Constraints

Effective prompts adhere to several key principles that enhance the quality and reliability of LLM outputs:

### 1. Clarity

Clarity ensures the model understands exactly what is being asked. Unclear prompts lead to misinterpretations and irrelevant responses.

**Key practices:**
- Use simple, direct language
- Avoid ambiguity
- Define technical terms when necessary
- State the objective up front

**Example:**
</code></pre>

<p>Unclear prompt: "Make this code better."
Clear prompt: "Refactor this Python function to improve its readability and efficiency. Specifically, reduce nested conditionals and optimize the loop structure."</p>
<pre class="codehilite"><code>### 2. Specificity

Specificity narrows the scope of the model's response, leading to more focused and relevant outputs.

**Key practices:**
- Be explicit about requirements
- Specify the exact problem to solve
- Indicate desired approaches or techniques
- Mention constraints or limitations

**Example:**
</code></pre>

<p>General prompt: "Write a function to process data."
Specific prompt: "Write a Python function that takes a CSV string containing user records (fields: id, name, email, signup_date) and returns a list of dictionaries, with dates converted to datetime objects and emails validated for correct format."</p>
<pre class="codehilite"><code>### 3. Conciseness

Conciseness focuses on brevity without sacrificing necessary information. While context is important, excessive verbosity can dilute the core request.

**Key practices:**
- Remove unnecessary details
- Use direct, active language
- Focus on essential requirements
- Structure information logically

**Example:**
</code></pre>

<p>Verbose prompt: "I'm working on a project where I need to have a function that can take a string and then I need it to count how many times each word appears in the string because I want to analyze text frequency and I'm not sure how to approach this problem efficiently so I need a solution that works well for large texts too."</p>
<p>Concise prompt: "Create an efficient function that counts word frequency in a string, optimized for large texts."</p>
<pre class="codehilite"><code>### 4. Role-playing

Role-playing instructs the LLM to adopt a specific persona with relevant expertise, leading to more appropriate responses.

**Key practices:**
- Define a specific role with relevant expertise
- Specify the role's perspective or approach
- Set the relationship between the role and the audience
- Provide context for why this role is appropriate

**Example:**
</code></pre>

<p>Basic prompt: "Explain how to structure a microservice architecture."
Role-based prompt: "As an experienced system architect who has designed microservice systems for large-scale e-commerce platforms, explain the key considerations when structuring a microservice architecture for a startup that expects rapid growth."</p>
<pre class="codehilite"><code>### 5. Constraints

Constraints provide boundaries that help guide the model's response in terms of scope, format, or approach.

**Key practices:**
- Set explicit limitations
- Define what should be excluded
- Specify resource constraints
- Indicate priority criteria

**Example:**
</code></pre>

<p>Unconstrained prompt: "Write a function to validate email addresses."
Constrained prompt: "Write a JavaScript function to validate email addresses with these constraints:
- No external libraries or dependencies
- Must handle international domains
- Maximum 30 lines of code
- Prioritize readability over perfect validation"</p>
<pre class="codehilite"><code>## Basic Techniques: Zero-shot, Few-shot, Instruction-based

Different prompting techniques provide varied approaches to guiding LLM behavior, each with specific advantages for different situations:

### 1. Zero-shot Prompting

Zero-shot prompting involves asking the model to perform a task without any examples. This relies on the model's pre-trained knowledge.

**Best for:**
- Simple, common tasks
- When examples might bias the output
- Tasks the model is likely familiar with

**Example:**
</code></pre>

<p>Create a function in Python that validates whether a string is a valid IPv4 address.</p>
<pre class="codehilite"><code>**Advantages:**
- Simple and direct
- Requires minimal prompt engineering
- Tests the model's inherent capabilities

**Disadvantages:**
- May produce inconsistent results
- Less control over output format
- May fail for complex or uncommon tasks

### 2. Few-shot Prompting

Few-shot prompting provides one or more examples of the desired input-output pattern before asking the model to perform a similar task.

**Best for:**
- Tasks with specific output formats
- Establishing patterns the model should follow
- Guiding the model toward a particular approach

**Example:**
</code></pre>

<p>Convert the following function signatures from JavaScript to TypeScript:</p>
<p>Example 1:
JavaScript: function calculateTotal(prices, discount) { ... }
TypeScript: function calculateTotal(prices: number[], discount: number): number { ... }</p>
<p>Example 2:
JavaScript: function processUser(user, options) { ... }
TypeScript: function processUser(user: UserType, options: ProcessOptions): UserResult { ... }</p>
<p>Now convert this one:
JavaScript: function sortProducts(products, criteria, ascending) { ... }</p>
<pre class="codehilite"><code>**Advantages:**
- Provides clear guidance through examples
- Reduces ambiguity
- Works well for pattern-following tasks

**Disadvantages:**
- Takes up more context window space
- May limit creativity
- Can bias the model toward specific approaches

### 3. Instruction-based Prompting

Instruction-based prompting provides detailed, step-by-step directions on how the model should approach a task, often with explicit formatting requirements.

**Best for:**
- Complex multi-step tasks
- Tasks requiring specific methodologies
- Outputs needing standardized formatting

**Example:**
</code></pre>

<p>Analyze the security vulnerabilities in the following Node.js code:</p>
<pre class="codehilite"><code class="language-javascript">const express = require('express');
const app = express();

app.get('/user/:id', (req, res) =&gt; {
  const userId = req.params.id;
  const query = `SELECT * FROM users WHERE id = ${userId}`;
  db.execute(query).then(result =&gt; {
    res.json(result);
  });
});
</code></pre>

<p>Follow these steps in your analysis:
1. Identify each vulnerability and its type (e.g., SQL injection, XSS)
2. Explain why it's a vulnerability and potential exploit scenarios
3. Rate the severity (Low, Medium, High, Critical)
4. Provide a secure code alternative for each issue found
5. Suggest additional security best practices relevant to this code</p>
<p>Format your response as a structured report with clear headings for each vulnerability.</p>
<pre class="codehilite"><code>**Advantages:**
- Provides detailed guidance
- Ensures comprehensive outputs
- Creates structured, predictable responses

**Disadvantages:**
- Can be lengthy and consume tokens
- May over-constrain the model
- Requires careful crafting to avoid confusion

### Hybrid Approaches

Often, the most effective prompts combine elements from multiple techniques:
</code></pre>

<h1 id="instruction">INSTRUCTION</h1>
<p>Implement a memory-efficient data structure for a least-recently-used (LRU) cache in Python.</p>
<h1 id="examples">EXAMPLES</h1>
<p>Here's an example of how a similar data structure (Stack) might be implemented:</p>
<pre class="codehilite"><code class="language-python">class Stack:
    def __init__(self, capacity):
        self.capacity = capacity
        self.items = []

    def push(self, item):
        if len(self.items) &gt;= self.capacity:
            raise OverflowError(&quot;Stack is full&quot;)
        self.items.append(item)

    def pop(self):
        if not self.items:
            raise IndexError(&quot;Pop from empty stack&quot;)
        return self.items.pop()
</code></pre>

<h1 id="requirements">REQUIREMENTS</h1>
<p>Your LRU cache implementation should:
1. Have O(1) time complexity for lookups, insertions, and deletions
2. Support a configurable maximum size
3. Automatically remove least recently used items when full
4. Include methods: get(key), put(key, value), and remove(key)
5. Include proper docstrings and type hints</p>
<h1 id="output-format_1">OUTPUT FORMAT</h1>
<p>Provide your solution as a complete Python class with inline comments explaining key design decisions.</p>
<pre class="codehilite"><code>## Testing and Evaluating Prompt Effectiveness

The effectiveness of a prompt can be assessed across several dimensions:

### 1. Response Relevance

How well does the output address the actual request?

**Evaluation method:**
```python
def evaluate_relevance(prompt, response, criteria):
    &quot;&quot;&quot;
    Evaluate the relevance of an LLM response against specific criteria

    Args:
        prompt: The original prompt
        response: The LLM's response
        criteria: List of required topics/elements

    Returns:
        Score and missing elements
    &quot;&quot;&quot;
    score = 0
    missing = []

    for criterion in criteria:
        if criterion.lower() in response.lower():
            score += 1
        else:
            missing.append(criterion)

    relevance_score = score / len(criteria)
    return relevance_score, missing

# Example usage
prompt = &quot;Explain the differences between REST and GraphQL APIs.&quot;
response = &quot;REST APIs use standard HTTP methods and typically return fixed data structures. They may require multiple requests to fetch related data. GraphQL allows clients to specify exactly what data they need in a single request, reducing over-fetching.&quot;
criteria = [&quot;HTTP methods&quot;, &quot;endpoint structure&quot;, &quot;data fetching&quot;, &quot;versioning&quot;, &quot;caching&quot;]

score, missing = evaluate_relevance(prompt, response, criteria)
print(f&quot;Relevance score: {score:.2f}&quot;)
print(f&quot;Missing elements: {missing}&quot;)
</code></pre>

<h3 id="2-output-format-compliance">2. Output Format Compliance</h3>
<p>Does the response follow the requested format?</p>
<p><strong>Evaluation method:</strong></p>
<pre class="codehilite"><code class="language-python">import re
import json

def evaluate_format_compliance(response, format_type):
    &quot;&quot;&quot;
    Check if response complies with requested format

    Args:
        response: The LLM response text
        format_type: Type of format expected ('json', 'markdown_table', 'bullet_list', etc.)

    Returns:
        Boolean indicating compliance and reason if non-compliant
    &quot;&quot;&quot;
    if format_type == 'json':
        try:
            # Check if there's a code block with JSON
            json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
                json.loads(json_str)  # Test if valid JSON
                return True, &quot;Valid JSON found in code block&quot;

            # Try to find a JSON object even without code block
            json_pattern = re.search(r'(\{[^{]*&quot;.*&quot;[^}]*\})', response, re.DOTALL)
            if json_pattern:
                json_str = json_pattern.group(1)
                json.loads(json_str)  # Test if valid JSON
                return True, &quot;Valid JSON found&quot;

            return False, &quot;No valid JSON found in response&quot;
        except json.JSONDecodeError:
            return False, &quot;JSON parsing failed&quot;

    elif format_type == 'markdown_table':
        # Check for markdown table pattern
        has_table = bool(re.search(r'\|[\s\w]+\|[\s\w]+\|', response) and 
                         re.search(r'\|[-:]+\|[-:]+\|', response))
        return has_table, &quot;Markdown table not found&quot; if not has_table else &quot;Markdown table found&quot;

    elif format_type == 'bullet_list':
        # Check for bulleted list pattern
        has_bullets = bool(re.findall(r'^\s*[-*]\s+\w+', response, re.MULTILINE))
        return has_bullets, &quot;Bullet list not found&quot; if not has_bullets else &quot;Bullet list found&quot;

    return False, &quot;Format type not supported for evaluation&quot;

# Example usage
response = &quot;&quot;&quot;
Here's the data you requested:

```json
{
    &quot;name&quot;: &quot;API Comparison&quot;,
    &quot;technologies&quot;: [&quot;REST&quot;, &quot;GraphQL&quot;, &quot;gRPC&quot;],
    &quot;metrics&quot;: {
        &quot;performance&quot;: [85, 92, 97],
        &quot;learning_curve&quot;: [75, 68, 45]
    }
}
</code></pre>

<p>"""</p>
<p>is_compliant, reason = evaluate_format_compliance(response, 'json')
print(f"Format compliant: {is_compliant}, Reason: {reason}")</p>
<pre class="codehilite"><code>### 3. Factual Accuracy

Does the response contain correct information?

**Evaluation method:**
```python
def evaluate_factual_accuracy(response, fact_checks):
    &quot;&quot;&quot;
    Check response for factual accuracy against known truth

    Args:
        response: The LLM response
        fact_checks: Dictionary of facts to check {fact_description: truth_value}

    Returns:
        Accuracy score and incorrect facts
    &quot;&quot;&quot;
    correct = 0
    incorrect = []

    for fact, truth in fact_checks.items():
        # Simple presence check - could be enhanced with NLP techniques
        fact_present = fact.lower() in response.lower()

        if fact_present == truth:
            correct += 1
        else:
            incorrect.append(fact)

    accuracy = correct / len(fact_checks) if fact_checks else 0
    return accuracy, incorrect

# Example usage
response = &quot;JavaScript is a dynamically typed language that supports first-class functions and prototypal inheritance. It was created in 1995 by Brendan Eich.&quot;

facts = {
    &quot;JavaScript is dynamically typed&quot;: True,
    &quot;JavaScript uses classical inheritance&quot;: False,
    &quot;JavaScript was created by Brendan Eich&quot;: True,
    &quot;JavaScript was created in 1990&quot;: False,
    &quot;JavaScript supports first-class functions&quot;: True
}

accuracy, incorrect = evaluate_factual_accuracy(response, facts)
print(f&quot;Factual accuracy: {accuracy:.2f}&quot;)
print(f&quot;Incorrect facts: {incorrect}&quot;)
</code></pre>

<h3 id="4-ab-testing-prompts">4. A/B Testing Prompts</h3>
<p>Systematically compare different prompt variations to find the most effective approach.</p>
<p><strong>Evaluation method:</strong></p>
<pre class="codehilite"><code class="language-python">import random

class PromptABTester:
    &quot;&quot;&quot;A simple tool for A/B testing different prompt formulations&quot;&quot;&quot;

    def __init__(self, llm_function, evaluation_function):
        &quot;&quot;&quot;
        Args:
            llm_function: Function that sends prompt to LLM and returns response
            evaluation_function: Function that scores response quality (0-1)
        &quot;&quot;&quot;
        self.llm_function = llm_function
        self.evaluation_function = evaluation_function
        self.results = {}

    def test_prompt_variations(self, prompt_variations, trials=3):
        &quot;&quot;&quot;
        Test multiple prompt variations with repeated trials

        Args:
            prompt_variations: Dict of {variation_name: prompt_text}
            trials: Number of times to test each variation

        Returns:
            DataFrame with results
        &quot;&quot;&quot;
        import pandas as pd

        all_results = []

        for name, prompt in prompt_variations.items():
            self.results[name] = []

            for i in range(trials):
                response = self.llm_function(prompt)
                score = self.evaluation_function(response)

                self.results[name].append(score)
                all_results.append({
                    'variation': name,
                    'trial': i+1,
                    'score': score,
                    'prompt': prompt,
                    'response': response
                })

        results_df = pd.DataFrame(all_results)

        # Calculate aggregate statistics
        summary = results_df.groupby('variation')['score'].agg(['mean', 'std', 'min', 'max'])

        return results_df, summary

    def get_best_prompt(self):
        &quot;&quot;&quot;Return the prompt variation with highest average score&quot;&quot;&quot;
        avg_scores = {name: sum(scores)/len(scores) for name, scores in self.results.items()}
        best_variation = max(avg_scores, key=avg_scores.get)
        return best_variation, avg_scores[best_variation]

# Example usage
def mock_llm(prompt):
    &quot;&quot;&quot;Mock LLM function for demonstration&quot;&quot;&quot;
    # This would be replaced by actual LLM API call
    responses = [
        &quot;This is a detailed response that covers all requirements.&quot;,
        &quot;This is a partial response that misses some key points.&quot;,
        &quot;This response is thorough and well-structured.&quot;
    ]
    return random.choice(responses)

def mock_evaluator(response):
    &quot;&quot;&quot;Mock evaluation function for demonstration&quot;&quot;&quot;
    # This would be replaced by actual evaluation logic
    if &quot;detailed&quot; in response or &quot;thorough&quot; in response:
        return 0.9
    return 0.6

# Create test variations
prompt_variations = {
    &quot;basic&quot;: &quot;Explain how virtual memory works in operating systems.&quot;,
    &quot;detailed&quot;: &quot;Explain how virtual memory works in operating systems. Include paging, segmentation, and address translation.&quot;,
    &quot;role_based&quot;: &quot;As an OS kernel engineer, explain how virtual memory works to a junior developer.&quot;
}

# Run the test
tester = PromptABTester(mock_llm, mock_evaluator)
results, summary = tester.test_prompt_variations(prompt_variations, trials=5)
best_prompt, best_score = tester.get_best_prompt()

print(f&quot;Best prompt variation: {best_prompt} (Score: {best_score:.2f})&quot;)
print(&quot;\nSummary statistics:&quot;)
print(summary)
</code></pre>

<h2 id="chapter3_section2">Conclusion</h2>
<p>Prompt construction is both an art and a science. By understanding the anatomy of effective prompts, applying core principles, and leveraging appropriate prompting techniques, developers can achieve consistently high-quality results from LLMs. Regular testing and evaluation help refine prompts over time, leading to increasingly reliable and useful outputs.</p>
<p>In the next chapter, we'll explore essential prompting patterns specifically tailored for common developer tasks, from code generation to documentation and debugging.</p>
<h2 id="chapter3_section3">Exercises</h2>
<ol>
<li>
<p>Take a simple prompt you've used before and improve it by explicitly incorporating the four components: instructions, context, input data, and output format.</p>
</li>
<li>
<p>Compare zero-shot, few-shot, and instruction-based approaches on the same task (e.g., generating a function to validate email addresses). Which performed best and why?</p>
</li>
<li>
<p>Create an A/B testing framework to evaluate prompt effectiveness for a specific use case (e.g., code explanation, bug finding).</p>
</li>
<li>
<p>Design a prompt that demonstrates all five core principles: clarity, specificity, conciseness, role-playing, and constraints.</p>
</li>
<li>
<p>Create a systematic evaluation method for one type of LLM task you commonly use (e.g., code generation, text summarization) with at least three different evaluation criteria.</p>
</li>
</ol></div><div class="navigation"><a href="#chapter2" class="prev-chapter">Previous Chapter</a><a href="#chapter4" class="next-chapter">Next Chapter</a></div><div class="chapter" id="chapter-content-4"><h1 id="chapter4">Chapter 4: Essential Prompting Patterns for Developers</h1>
<p>In this chapter, we'll explore practical prompting patterns that developers can immediately apply to common tasks. These patterns serve as templates that you can adapt to your specific needs, saving time and improving consistency in your interactions with LLMs.</p>
<h2 id="chapter4_section1">Code Generation: Generating Functions, Classes, Scripts</h2>
<h3 id="function-generation-pattern">Function Generation Pattern</h3>
<p>Use this pattern to generate well-structured, documented functions for specific programming tasks.</p>
<pre class="codehilite"><code>Create a [language] function named [function_name] that [purpose].

Input parameters:
- [param1_name] ([type]): [description]
- [param2_name] ([type]): [description]

Requirements:
- [requirement1]
- [requirement2]

Include appropriate error handling, type hints, and documentation.
</code></pre>

<p><strong>Example - Python Data Processing Function:</strong></p>
<pre class="codehilite"><code>Create a Python function named 'parse_log_entries' that extracts structured data from application log files.

Input parameters:
- log_path (str): Path to the log file
- error_levels (list, optional): Specific error levels to filter for (e.g., [&quot;ERROR&quot;, &quot;WARNING&quot;])
- start_date (datetime, optional): Only process entries after this date

Requirements:
- Return a list of dictionaries with keys: timestamp, level, message, service
- Handle malformed log lines gracefully
- Support both plain text and gzip compressed logs
- Add type hints and comprehensive docstrings

Include appropriate error handling and performance optimization for large files.
</code></pre>

<h3 id="class-generation-pattern">Class Generation Pattern</h3>
<p>Use this pattern to generate complete classes with properties, methods, and appropriate design patterns.</p>
<pre class="codehilite"><code>Design a [language] class named [ClassName] that [purpose].

Properties:
- [property1_name] ([type]): [description]
- [property2_name] ([type]): [description]

Methods:
- [method1_name]([params]): [description]
- [method2_name]([params]): [description]

Requirements:
- [requirement1]
- [requirement2]

Additional context:
[any relevant information about usage, environment, etc.]
</code></pre>

<p><strong>Example - TypeScript Component Class:</strong></p>
<pre class="codehilite"><code>Design a TypeScript class named 'DataTable' that implements a reusable, sortable table component for a React application.

Properties:
- data (Array&lt;Record&lt;string, any&gt;&gt;): Source data to display
- columns (ColumnConfig[]): Column configuration including headers, field mappings and formatting
- sortState (SortConfig): Current sort column and direction
- pageSize (number): Number of records per page
- currentPage (number): Current page index

Methods:
- render(): JSX.Element - Render the table with current configuration
- sort(columnId: string, direction: 'asc'|'desc'): void - Sort table data
- nextPage(): void - Navigate to next page
- previousPage(): void - Navigate to previous page
- onRowClick(handler: (row: Record&lt;string, any&gt;) =&gt; void): void - Set row click handler

Requirements:
- Implement pagination with customizable page size
- Support client-side sorting for all common data types
- Allow custom cell renderers for complex data
- Follow React best practices for performance optimization
- Include proper TypeScript interfaces and types

Additional context:
The component will be used in a dashboard application that displays various data views to users with different permission levels.
</code></pre>

<h3 id="script-generation-pattern">Script Generation Pattern</h3>
<p>Use this pattern to generate complete scripts for automation tasks, data processing, or system operations.</p>
<pre class="codehilite"><code>Create a [language] script that [purpose].

Inputs:
- [input1]: [description]
- [input2]: [description]

Expected output:
[description of what the script should produce]

Requirements:
- [requirement1]
- [requirement2]

Environment context:
[relevant environment information]
</code></pre>

<p><strong>Example - Python ETL Script:</strong></p>
<pre class="codehilite"><code>Create a Python script that performs ETL (Extract, Transform, Load) operations on website analytics data.

Inputs:
- analytics.csv: Daily website traffic data (columns: date, page_path, visitors, bounce_rate, avg_time_on_page)
- product_mapping.json: JSON file mapping page paths to product categories

Expected output:
- A PostgreSQL database table with aggregated metrics by product category and date
- A summary report in CSV format showing week-over-week changes

Requirements:
- Handle missing or malformed data gracefully
- Implement logging for the ETL process
- Support incremental loads (only process new data)
- Include command-line arguments for configuration
- Optimize for memory efficiency with large input files

Environment context:
- Python 3.9+
- Will run as a scheduled task in Linux environment
- PostgreSQL 13 database
</code></pre>

<h2 id="chapter4_section2">Code Explanation &amp; Documentation</h2>
<h3 id="code-comprehension-pattern">Code Comprehension Pattern</h3>
<p>Use this pattern when you need to understand unfamiliar or complex code.</p>
<pre class="codehilite"><code>Explain the following [language] code, focusing on:
1. Overall purpose
2. Key algorithms or data structures used
3. Control flow and execution path
4. Any potential edge cases or bugs
5. Performance characteristics

```[code block]```
</code></pre>

<p><strong>Example:</strong></p>
<pre class="codehilite"><code>Explain the following JavaScript code, focusing on:
1. Overall purpose
2. Key algorithms or data structures used
3. Control flow and execution path
4. Any potential edge cases or bugs
5. Performance characteristics

```javascript
function findDuplicateTransactions(transactions) {
  const sorted = [...transactions].sort((a, b) =&gt; {
    return new Date(a.time) - new Date(b.time);
  });

  const potential = {};
  sorted.forEach(t =&gt; {
    const key = `${t.sourceAccount}_${t.targetAccount}_${t.category}_${t.amount}`;
    if (!potential[key]) potential[key] = [];
    potential[key].push(t);
  });

  const duplicates = [];
  Object.values(potential).forEach(group =&gt; {
    if (group.length &lt;= 1) return;

    const result = [];
    for (let i = 0; i &lt; group.length; i++) {
      if (result.length === 0) {
        result.push(group[i]);
        continue;
      }

      const last = result[result.length - 1];
      const current = group[i];
      const timeDiff = Math.abs(new Date(current.time) - new Date(last.time));
      const minutesDiff = timeDiff / (1000 * 60);

      if (minutesDiff &lt;= 5) {
        result.push(current);
      } else if (result.length &gt; 1) {
        duplicates.push([...result]);
        result.length = 0;
        result.push(current);
      } else {
        result.length = 0;
        result.push(current);
      }
    }

    if (result.length &gt; 1) {
      duplicates.push(result);
    }
  });

  return duplicates;
}
</code></pre>

<pre class="codehilite"><code>### Documentation Generation Pattern

Use this pattern to generate comprehensive documentation for existing code.
</code></pre>

<p>Generate [format] documentation for the following [language] [code_type].
Include:
- Purpose and functionality
- Parameter descriptions
- Return value details
- Usage examples
- Edge cases and exceptions</p>
<p><code>[code block]</code></p>
<pre class="codehilite"><code>**Example:**
</code></pre>

<p>Generate JSDoc documentation for the following JavaScript function.
Include:
- Purpose and functionality
- Parameter descriptions
- Return value details
- Usage examples
- Edge cases and exceptions</p>
<pre class="codehilite"><code class="language-javascript">function throttle(fn, delay) {
  let lastCall = 0;
  let timeoutId = null;

  return function(...args) {
    const now = Date.now();
    const remaining = delay - (now - lastCall);

    if (remaining &lt;= 0) {
      if (timeoutId) {
        clearTimeout(timeoutId);
        timeoutId = null;
      }

      lastCall = now;
      return fn.apply(this, args);
    } else if (!timeoutId) {
      timeoutId = setTimeout(() =&gt; {
        lastCall = Date.now();
        timeoutId = null;
        fn.apply(this, args);
      }, remaining);
    }
  };
}
</code></pre>

<pre class="codehilite"><code>## Debugging &amp; Error Resolution

### Error Diagnosis Pattern

Use this pattern when you encounter error messages and need help troubleshooting.
</code></pre>

<p>I'm getting the following error when running my [language] code:</p>
<pre class="codehilite"><code>[error message]
</code></pre>

<p>Here's the relevant code:</p>
<p><code>[code block]</code></p>
<p>Please:
1. Explain what's causing this error
2. Suggest a solution with code examples
3. Explain how to prevent similar errors in the future</p>
<pre class="codehilite"><code>**Example:**
</code></pre>

<p>I'm getting the following error when running my Python code:</p>
<pre class="codehilite"><code>TypeError: unsupported operand type(s) for +: 'int' and 'str'
</code></pre>

<p>Here's the relevant code:</p>
<pre class="codehilite"><code class="language-python">def calculate_total_cost(items):
    total = 0
    for item in items:
        total = total + item['price']
    return total

inventory = [
    {'id': 1, 'name': 'Widget A', 'price': 10},
    {'id': 2, 'name': 'Widget B', 'price': '15'},
    {'id': 3, 'name': 'Widget C', 'price': 20},
]

print(calculate_total_cost(inventory))
</code></pre>

<p>Please:
1. Explain what's causing this error
2. Suggest a solution with code examples
3. Explain how to prevent similar errors in the future</p>
<pre class="codehilite"><code>### Code Review Pattern

Use this pattern to identify potential issues before they cause runtime errors.
</code></pre>

<p>Review the following [language] code for:
1. Bugs or logic errors
2. Performance issues
3. Security vulnerabilities 
4. Style or best practice violations
5. Edge cases that aren't handled</p>
<p><code>[code block]</code></p>
<p>Suggest improvements with code examples.</p>
<pre class="codehilite"><code>**Example:**
</code></pre>

<p>Review the following Python code for:
1. Bugs or logic errors
2. Performance issues
3. Security vulnerabilities 
4. Style or best practice violations
5. Edge cases that aren't handled</p>
<pre class="codehilite"><code class="language-python">def authenticate_user():
    username = request.form.get('username')
    password = request.form.get('password')

    query = &quot;SELECT * FROM users WHERE username='{}' AND password='{}'&quot;.format(
        username, password
    )

    result = db.execute(query)
    user = result.fetchone()

    if user:
        session['user_id'] = user[0]
        session['is_admin'] = user[3]
        return redirect('/dashboard')
    else:
        attempts = session.get('login_attempts', 0) + 1
        session['login_attempts'] = attempts

        if attempts &gt; 3:
            time.sleep(3)  # Add delay after multiple failures

        return render_template('login.html', error=&quot;Invalid credentials&quot;)
</code></pre>

<p>Suggest improvements with code examples.</p>
<pre class="codehilite"><code>## Text Transformation

### Text Summarization Pattern

Use this pattern to condense documentation, comments, or other text while preserving key information.
</code></pre>

<p>Summarize the following [text_type] in [target_length] while preserving the key technical details:</p>
<p>[text to summarize]</p>
<pre class="codehilite"><code>**Example:**
</code></pre>

<p>Summarize the following API documentation in 3-4 paragraphs while preserving the key technical details:</p>
<p>The User Authentication API provides endpoints for registering, authenticating, and managing user accounts. It supports both traditional username/password authentication as well as OAuth2 integrations with popular providers.</p>
<p>The main endpoint /api/v1/auth/login accepts POST requests with either username/password combinations or OAuth tokens. For username/password authentication, the request body must include "username" and "password" fields in JSON format. For OAuth, include "provider" and "access_token" fields. The API returns a JWT token with a configurable expiration (default: 24 hours).</p>
<p>Token validation can be performed against the /api/v1/auth/validate endpoint, which accepts GET requests with the Authorization header set to "Bearer {token}". This endpoint returns user details if the token is valid or a 401 status if invalid or expired.</p>
<p>Account registration is handled through the /api/v1/auth/register endpoint, accepting POST requests with required fields "username", "email", and "password". Additional optional fields include "full_name", "phone", and "preferences" (as a JSON object). Password requirements can be configured but default to minimum 8 characters with at least one number, one uppercase letter, and one special character.</p>
<p>Password reset functionality is provided via two endpoints: /api/v1/auth/request-reset (POST with "email" field) sends a time-limited reset token to the user's email, and /api/v1/auth/reset-password (POST with "token" and "new_password" fields) performs the actual password change.</p>
<p>Rate limiting is applied to all authentication endpoints, with default limits of 5 attempts per minute for login and 3 attempts per hour for password reset requests from the same IP address. These limits are configurable through the server's environment variables RATE_LIMIT_LOGIN and RATE_LIMIT_RESET.</p>
<p>All authentication attempts, successful or failed, are logged with timestamp, IP address, and user agent information. These logs can be accessed through the admin dashboard or directly from the database's auth_logs table.</p>
<pre class="codehilite"><code>### Text Translation Pattern

Use this pattern to translate between technical terminology, languages, or convert between technologies.
</code></pre>

<p>Translate the following [source_format] to [target_format], maintaining all functionality:</p>
<p><code>[source code or text]</code></p>
<pre class="codehilite"><code>**Example:**
</code></pre>

<p>Translate the following JavaScript React component to TypeScript, maintaining all functionality:</p>
<pre class="codehilite"><code class="language-javascript">import React, { useState, useEffect } from 'react';
import axios from 'axios';

function UserDashboard({ userId }) {
  const [userData, setUserData] = useState(null);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState(null);

  useEffect(() =&gt; {
    async function fetchUserData() {
      try {
        const response = await axios.get(`/api/users/${userId}`);
        setUserData(response.data);
        setLoading(false);
      } catch (err) {
        setError('Failed to load user data');
        setLoading(false);
      }
    }

    fetchUserData();
  }, [userId]);

  function handleRefresh() {
    setLoading(true);
    fetchUserData();
  }

  if (loading) return &lt;div className=&quot;loading&quot;&gt;Loading...&lt;/div&gt;;
  if (error) return &lt;div className=&quot;error&quot;&gt;{error}&lt;/div&gt;;

  return (
    &lt;div className=&quot;dashboard&quot;&gt;
      &lt;h1&gt;Welcome, {userData.name}&lt;/h1&gt;
      &lt;div className=&quot;stats&quot;&gt;
        &lt;div className=&quot;stat-item&quot;&gt;
          &lt;span className=&quot;stat-label&quot;&gt;Projects&lt;/span&gt;
          &lt;span className=&quot;stat-value&quot;&gt;{userData.projects.length}&lt;/span&gt;
        &lt;/div&gt;
        &lt;div className=&quot;stat-item&quot;&gt;
          &lt;span className=&quot;stat-label&quot;&gt;Tasks&lt;/span&gt;
          &lt;span className=&quot;stat-value&quot;&gt;{userData.tasks.filter(t =&gt; !t.completed).length}&lt;/span&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;button onClick={handleRefresh}&gt;Refresh Data&lt;/button&gt;
    &lt;/div&gt;
  );
}

export default UserDashboard;
</code></pre>

<pre class="codehilite"><code>### Format Conversion Pattern

Use this pattern to transform between data formats (JSON, XML, CSV, etc.) or to structure text in specific formats.
</code></pre>

<p>Convert the following [source_format] to [target_format]:</p>
<p><code>[source data]</code></p>
<p>Requirements:
- [requirement1]
- [requirement2]</p>
<pre class="codehilite"><code>**Example:**
</code></pre>

<p>Convert the following JSON data to a CSV format:</p>
<pre class="codehilite"><code class="language-json">{
  &quot;employees&quot;: [
    {
      &quot;id&quot;: 101,
      &quot;name&quot;: &quot;John Smith&quot;,
      &quot;department&quot;: &quot;Engineering&quot;,
      &quot;title&quot;: &quot;Senior Developer&quot;,
      &quot;skills&quot;: [&quot;Python&quot;, &quot;React&quot;, &quot;MongoDB&quot;],
      &quot;projects&quot;: [
        {&quot;id&quot;: &quot;P-100&quot;, &quot;name&quot;: &quot;API Gateway&quot;},
        {&quot;id&quot;: &quot;P-201&quot;, &quot;name&quot;: &quot;Data Pipeline&quot;}
      ]
    },
    {
      &quot;id&quot;: 102,
      &quot;name&quot;: &quot;Alice Johnson&quot;,
      &quot;department&quot;: &quot;Product&quot;,
      &quot;title&quot;: &quot;Product Manager&quot;,
      &quot;skills&quot;: [&quot;Agile&quot;, &quot;Roadmapping&quot;, &quot;User Research&quot;],
      &quot;projects&quot;: [
        {&quot;id&quot;: &quot;P-100&quot;, &quot;name&quot;: &quot;API Gateway&quot;},
        {&quot;id&quot;: &quot;P-105&quot;, &quot;name&quot;: &quot;Mobile App&quot;}
      ]
    }
  ]
}
</code></pre>

<p>Requirements:
- Include headers in the first row
- Format skills as comma-separated values within a single field
- Include project IDs and names in separate columns
- Create one row per employee-project combination</p>
<pre class="codehilite"><code>## Data Extraction: Pulling Structured Data from Unstructured Text

### Entity Extraction Pattern

Use this pattern to identify and extract specific entities from text.
</code></pre>

<p>Extract the following entities from this [text_type]:
- [entity_type1]
- [entity_type2]
- [entity_type3]</p>
<p>Return the results as [format].</p>
<p>[text]</p>
<pre class="codehilite"><code>**Example:**
</code></pre>

<p>Extract the following entities from this error log:
- Error codes
- Timestamps
- File paths
- IP addresses</p>
<p>Return the results as a JSON object with arrays of each entity type.</p>
<p>[2023-05-15T09:23:45.123Z] ERROR [server.js:125] Failed to authenticate user from 192.168.1.105 - Error code AUTH-401
[2023-05-15T09:25:12.456Z] WARN [auth/middleware.js:85] Rate limit exceeded for IP 192.168.1.105
[2023-05-15T09:30:22.789Z] ERROR [database/connection.js:209] Failed to connect to database after 5 retries - Error code DB-503
[2023-05-15T09:31:01.234Z] INFO [server.js:50] Server restarting with updated configuration from /etc/app/config.json
[2023-05-15T09:32:45.678Z] ERROR [api/users.js:75] Invalid request parameters from 192.168.1.210 - Error code API-400</p>
<pre class="codehilite"><code>### Tabular Data Extraction Pattern

Use this pattern to convert text descriptions or semi-structured information into structured tables.
</code></pre>

<p>Extract the following information from the text into a [table_format] with columns [column1, column2, ...]:</p>
<p>[text]</p>
<pre class="codehilite"><code>**Example:**
</code></pre>

<p>Extract the following information from the text into a markdown table with columns Method, Endpoint, Required Parameters, and Description:</p>
<p>Our REST API provides the following endpoints for managing user profiles:</p>
<p>GET /api/users - Returns a list of all users. Supports optional query parameters 'limit' (default 20) and 'offset' (default 0) for pagination.</p>
<p>GET /api/users/{id} - Returns details for a specific user. The 'id' parameter is required and must be a valid user identifier.</p>
<p>POST /api/users - Creates a new user. Requires 'email', 'username', and 'password' in the request body. Optional fields include 'fullName', 'role', and 'preferences'.</p>
<p>PUT /api/users/{id} - Updates an existing user. The 'id' parameter is required. At least one update field must be provided in the request body.</p>
<p>DELETE /api/users/{id} - Removes a user. The 'id' parameter is required. This operation cannot be undone.</p>
<p>PATCH /api/users/{id}/status - Updates only the user's status. Requires 'id' parameter and 'status' field in the request body with value 'active' or 'inactive'.</p>
<pre class="codehilite"><code>### Parsing Unstructured Data Pattern

Use this pattern to extract structured information from free-form text like emails, documents, or requirements.
</code></pre>

<p>Parse the following [text_type] and extract:
- [information1]
- [information2]
- [information3]</p>
<p>Format the output as [format]:</p>
<p>[text]</p>
<pre class="codehilite"><code>**Example:**
</code></pre>

<p>Parse the following customer support email and extract:
- Product name and version
- Issue category
- Steps to reproduce
- Customer contact information
- Priority level (if mentioned)</p>
<p>Format the output as JSON:</p>
<p>Subject: Urgent: Dashboard crashes when filtering by date range in Analytics Pro 4.2</p>
<p>Hello Support Team,</p>
<p>I'm experiencing a critical issue with Analytics Pro version 4.2.1 on Windows 10. 
Whenever I try to filter dashboard data using a date range, the application completely
crashes and I have to restart it.</p>
<p>Steps to reproduce:
1. Open the main dashboard
2. Click on "Date Filter" in the top right
3. Select "Custom Range" from the dropdown
4. Choose any start and end dates
5. Click "Apply Filter"</p>
<p>After clicking "Apply Filter," the screen freezes for about 5 seconds, then the
application crashes with no error message.</p>
<p>This is blocking our monthly reporting process which we need to complete by end of day
tomorrow, so I'd consider this a high priority issue.</p>
<p>Please contact me at john.smith@example.com or 555-123-4567 if you need more information.</p>
<p>Thanks,
John Smith
Senior Data Analyst
Acme Corporation</p>
<pre class="codehilite"><code>## Examples in Multiple Programming Languages

### Python - Function Generation
</code></pre>

<p>Create a Python function named 'parse_log_file' that extracts and analyzes error messages from a log file.</p>
<p>Input parameters:
- file_path (str): Path to the log file
- error_types (list, optional): List of error types to focus on (e.g., ['ERROR', 'CRITICAL'])
- start_date (str, optional): Only parse logs after this date (format: 'YYYY-MM-DD')</p>
<p>Requirements:
- Return a dictionary with error types as keys and lists of error messages as values
- Include timestamps and context information with each error
- Handle compressed (.gz) log files automatically
- Use generators for memory efficiency with large files
- Include proper type hints and docstrings</p>
<pre class="codehilite"><code>### JavaScript - Debugging
</code></pre>

<p>I'm getting the following error when running my JavaScript React application:</p>
<pre class="codehilite"><code>TypeError: Cannot read properties of undefined (reading 'map')
</code></pre>

<p>Here's the relevant code:</p>
<pre class="codehilite"><code class="language-javascript">function ProductList({ categoryId }) {
  const [products, setProducts] = useState(null);
  const [loading, setLoading] = useState(true);

  useEffect(() =&gt; {
    async function fetchProducts() {
      try {
        const response = await api.getProductsByCategory(categoryId);
        setProducts(response.data);
      } catch (err) {
        console.error('Failed to fetch products:', err);
      } finally {
        setLoading(false);
      }
    }

    fetchProducts();
  }, [categoryId]);

  if (loading) return &lt;LoadingSpinner /&gt;;

  return (
    &lt;div className=&quot;product-grid&quot;&gt;
      {products.map(product =&gt; (
        &lt;ProductCard key={product.id} product={product} /&gt;
      ))}
    &lt;/div&gt;
  );
}
</code></pre>

<p>Please:
1. Explain what's causing this error
2. Suggest a solution with code examples
3. Explain how to prevent similar errors in the future</p>
<pre class="codehilite"><code>### Java - Code Explanation
</code></pre>

<p>Explain the following Java code, focusing on:
1. Overall purpose
2. Key algorithms or design patterns used
3. Thread safety considerations
4. Potential performance bottlenecks</p>
<pre class="codehilite"><code class="language-java">public class ConnectionPool {
    private static ConnectionPool instance;
    private final List&lt;Connection&gt; availableConnections = new ArrayList&lt;&gt;();
    private final List&lt;Connection&gt; usedConnections = new ArrayList&lt;&gt;();
    private final int MAX_CONNECTIONS;

    private ConnectionPool(String url, String user, String password, int maxConnections) {
        this.MAX_CONNECTIONS = maxConnections;
        try {
            for (int i = 0; i &lt; maxConnections; i++) {
                availableConnections.add(
                    DriverManager.getConnection(url, user, password)
                );
            }
        } catch (SQLException e) {
            logger.severe(&quot;Failed to initialize connection pool: &quot; + e.getMessage());
        }
    }

    public static synchronized ConnectionPool getInstance(String url, String user, 
                                                       String password, int maxConnections) {
        if (instance == null) {
            instance = new ConnectionPool(url, user, password, maxConnections);
        }
        return instance;
    }

    public synchronized Connection getConnection() throws SQLException {
        if (availableConnections.isEmpty()) {
            if (usedConnections.size() &lt; MAX_CONNECTIONS) {
                String url = usedConnections.get(0).getMetaData().getURL();
                availableConnections.add(DriverManager.getConnection(url, &quot;&quot;, &quot;&quot;));
            } else {
                throw new SQLException(&quot;Connection limit reached, no available connections!&quot;);
            }
        }

        Connection connection = availableConnections.remove(availableConnections.size() - 1);
        usedConnections.add(connection);
        return connection;
    }

    public synchronized void releaseConnection(Connection connection) {
        usedConnections.remove(connection);
        availableConnections.add(connection);
    }
}
</code></pre>

<pre class="codehilite"><code>### C# - Class Generation
</code></pre>

<p>Design a C# class named 'FileProcessor' that handles batch processing of documents in different formats.</p>
<p>Properties:
- ProcessedCount (int): Number of files successfully processed
- FailedCount (int): Number of files that failed processing
- ProcessingOptions (ProcessingOptions): Configuration settings for processing
- OnProgressUpdate (event): Event that fires when progress changes</p>
<p>Methods:
- ProcessDirectory(string path, bool recursive): Process all compatible files in a directory
- ProcessFile(string filePath): Process a single file
- GetSupportedFormats(): List<string> - Return list of supported file formats
- CancelProcessing(): Cancel any ongoing processing operation
- GenerateReport(): ProcessingSummary - Create summary of processing operations</p>
<p>Requirements:
- Support PDF, DOCX, and TXT files with different processing strategies
- Implement proper error handling and logging
- Use async/await for file operations
- Follow C# naming conventions and best practices
- Make the class extensible for adding new file format handlers
```</p>
<h2 id="chapter4_section3">Conclusion</h2>
<p>These essential prompting patterns form the foundation of effective LLM use in development workflows. By adapting these patterns to your specific needs, you can quickly generate reliable, high-quality outputs for a wide range of development tasks across different programming languages and environments.</p>
<p>In the next chapter, we'll explore advanced prompting techniques that provide even greater control over LLM responses, including Chain-of-Thought reasoning, self-correction strategies, and parameter tuning.</p>
<h2 id="chapter4_section4">Exercises</h2>
<ol>
<li>
<p>Create a function generation prompt for a programming language of your choice that implements a data validation utility.</p>
</li>
<li>
<p>Take a complex function or class from your codebase and use the code explanation pattern to generate documentation for it.</p>
</li>
<li>
<p>Find a bug in your code and use the error diagnosis pattern to get help fixing it.</p>
</li>
<li>
<p>Use the format conversion pattern to transform a dataset between two different formats (e.g., JSON to CSV or XML to JSON).</p>
</li>
<li>
<p>Create a prompt that extracts structured information from unstructured API documentation or release notes.</p>
</li>
</ol></div><div class="navigation"><a href="#chapter3" class="prev-chapter">Previous Chapter</a><a href="#chapter5" class="next-chapter">Next Chapter</a></div><div class="chapter" id="chapter-content-5"><h1 id="chapter5">Chapter 5: Advanced Prompting Techniques for Enhanced Control</h1>
<p>As you grow more experienced with prompt engineering, you'll want to move beyond basic prompting patterns to achieve more precise and reliable results. This chapter explores advanced techniques that give you greater control over LLM outputs, especially for complex development tasks.</p>
<h2 id="chapter5_section1">Chain-of-Thought (CoT): Guiding LLMs through Multi-Step Reasoning</h2>
<p>Chain-of-Thought prompting encourages LLMs to break down complex problems into logical steps before reaching a conclusion. This technique is particularly valuable for debugging, algorithm design, and other tasks that require structured reasoning.</p>
<h3 id="the-cot-principle">The CoT Principle</h3>
<p>The core idea behind CoT is to instruct the LLM to:
1. Decompose a complex problem into distinct steps
2. Reason through each step explicitly
3. Build toward the final solution incrementally</p>
<p>This mirrors how expert programmers approach difficult problems, leading to more accurate and explainable results.</p>
<h3 id="basic-cot-template">Basic CoT Template</h3>
<pre class="codehilite"><code>I need to solve the following problem: [problem description]

Please think through this step-by-step:
1. First, analyze the problem and clarify what we need to accomplish
2. Identify the key components or subproblems
3. Solve each subproblem
4. Combine the solutions
5. Verify the result

For each step, explain your reasoning before moving to the next step.
</code></pre>

<h3 id="example-debugging-complex-logic-with-cot">Example: Debugging Complex Logic with CoT</h3>
<pre class="codehilite"><code>I need to find the bug in this function that's supposed to find the longest palindromic substring in a string:

```python
def longest_palindrome(s):
    if not s:
        return &quot;&quot;

    longest = s[0]

    for i in range(len(s)):
        # Check odd-length palindromes
        temp = expand_from_center(s, i, i)
        if len(temp) &gt; len(longest):
            longest = temp

        # Check even-length palindromes
        temp = expand_from_center(s, i, i+1)
        if len(temp) &gt; len(longest):
            longest = temp

    return longest

def expand_from_center(s, left, right):
    while left &gt;= 0 and right &lt; len(s) and s[left] == s[right]:
        left += 1
        right -= 1

    return s[left+1:right]
</code></pre>

<p>Please think through this step-by-step:
1. First, understand what the function should do and how it's supposed to work
2. Trace through the logic of both functions
3. Identify any logical errors
4. Explain the bug(s)
5. Provide a corrected implementation</p>
<p>For each step, explain your reasoning before moving to the next step.</p>
<pre class="codehilite"><code>### Advanced CoT: Managing Complex Development Tasks

For more complex development tasks, structure your CoT to mirror software development best practices:
</code></pre>

<p>I need to develop a solution for [complex task]. </p>
<p>Please approach this step-by-step:</p>
<ol>
<li>Requirements analysis:</li>
<li>Clarify the exact requirements</li>
<li>
<p>Identify edge cases and constraints</p>
</li>
<li>
<p>System design:</p>
</li>
<li>Propose an overall architecture</li>
<li>Identify key components and their interactions</li>
<li>
<p>Choose appropriate data structures and algorithms</p>
</li>
<li>
<p>Implementation planning:</p>
</li>
<li>Break down the solution into implementable units</li>
<li>
<p>Determine the sequence of implementation</p>
</li>
<li>
<p>Implementation:</p>
</li>
<li>Write the code with clear comments</li>
<li>
<p>Explain design choices</p>
</li>
<li>
<p>Testing strategy:</p>
</li>
<li>Outline test cases covering normal operation and edge cases</li>
<li>Consider potential failure points</li>
</ol>
<p>For each step, provide your reasoning before moving to the next step.</p>
<pre class="codehilite"><code>### Implementing CoT in Code Generation Workflows

While CoT prompting can be used directly in conversation with an LLM, you can also embed it in automated code generation workflows:

```python
def generate_complex_solution(problem_description, language=&quot;python&quot;):
    &quot;&quot;&quot;Generate solution for complex programming problems using CoT.&quot;&quot;&quot;

    cot_prompt = f&quot;&quot;&quot;
    I need a solution for the following problem in {language}: 
    {problem_description}

    Please solve this step-by-step:
    1. Analyze the problem requirements
    2. Identify the key algorithms or data structures needed
    3. Design the solution approach
    4. Implement the code with appropriate comments
    5. Analyze the time and space complexity

    For each step, explain your reasoning before moving to the next step.
    &quot;&quot;&quot;

    response = llm_client.generate(cot_prompt)

    # Extract the final code solution from the response
    # (Implementation depends on your specific LLM and response format)
    solution_code = extract_code_from_response(response)

    return {
        &quot;full_reasoning&quot;: response,
        &quot;code_solution&quot;: solution_code
    }
</code></pre>

<h2 id="chapter5_section2">Self-Correction &amp; Iterative Prompting</h2>
<p>Self-correction techniques encourage LLMs to review and refine their own outputs, mimicking the way developers iterate on their code through debugging and refactoring.</p>
<h3 id="basic-self-correction-template">Basic Self-Correction Template</h3>
<pre class="codehilite"><code>Please solve the following problem: [problem description]

After providing your solution, critically evaluate it for:
1. Correctness
2. Edge cases
3. Efficiency
4. Readability

Then provide an improved version based on your evaluation.
</code></pre>

<h3 id="example-self-correcting-code-implementation">Example: Self-Correcting Code Implementation</h3>
<pre class="codehilite"><code>Implement a function in Python that finds all anagrams of a given word in a list of words.

After providing your solution, critically evaluate it for:
1. Correctness
2. Edge cases (empty strings, different letter cases, etc.)
3. Time and space complexity
4. Readability and Pythonic style

Then provide an improved version based on your evaluation.
</code></pre>

<h3 id="multi-round-iterative-refinement">Multi-Round Iterative Refinement</h3>
<p>For complex problems, we can use multi-round refinement where each iteration focuses on a specific aspect of improvement:</p>
<pre class="codehilite"><code class="language-python">def iterative_code_refinement(initial_prompt, iterations=3):
    &quot;&quot;&quot;Generate and iteratively refine code through multiple LLM interactions.&quot;&quot;&quot;

    # Initial solution
    current_solution = llm_client.generate(initial_prompt)
    code = extract_code(current_solution)

    refinement_aspects = [
        &quot;correctness and edge cases&quot;,
        &quot;performance optimization&quot;,
        &quot;code readability and best practices&quot;
    ]

    for i, aspect in enumerate(refinement_aspects[:iterations]):
        refinement_prompt = f&quot;&quot;&quot;
        Here is a code solution:

        ```
        {code}
        ```

        Please review this code focusing specifically on {aspect}.
        Identify any issues, explain them, and provide an improved version of the code.
        &quot;&quot;&quot;

        refinement_response = llm_client.generate(refinement_prompt)
        improved_code = extract_code(refinement_response)

        # Update current solution if improved code was provided
        if improved_code:
            code = improved_code

        print(f&quot;Completed refinement round {i+1}/{iterations}: {aspect}&quot;)

    return code
</code></pre>

<h3 id="self-debug-pattern">Self-Debug Pattern</h3>
<p>The self-debug pattern specifically targets error correction by having the LLM analyze and fix issues in its own output:</p>
<pre class="codehilite"><code>Generate a [language] function that [task description].

Then act as a code reviewer and:
1. Test the function with various inputs
2. Identify any bugs or edge cases that aren't handled
3. Fix the identified issues
4. Explain the bugs and your fixes
</code></pre>

<h3 id="example-of-self-debug-pattern">Example of Self-Debug Pattern</h3>
<pre class="codehilite"><code>Generate a JavaScript function that parses a URL string and returns an object containing its components (protocol, host, path, query parameters, etc.).

Then act as a code reviewer and:
1. Test the function with various inputs (including URLs with and without protocols, query parameters, fragments, etc.)
2. Identify any bugs or edge cases that aren't handled
3. Fix the identified issues
4. Explain the bugs and your fixes
</code></pre>

<h2 id="chapter5_section3">Controlling Output: Parameter Tuning</h2>
<p>LLM APIs provide various parameters to control the nature of the generated outputs. Understanding these parameters is crucial for fine-tuning responses to specific development needs.</p>
<h3 id="temperature-controlling-randomness-vs-determinism">Temperature: Controlling Randomness vs. Determinism</h3>
<p>Temperature (typically 0-1) controls the randomness of predictions:</p>
<ul>
<li><strong>Temperature = 0</strong>: More deterministic, focused responses</li>
<li><strong>Temperature = 0.7</strong>: Balanced creativity and coherence</li>
<li><strong>Temperature = 1</strong>: More random, diverse, and creative outputs</li>
</ul>
<h4 id="when-to-use-different-temperature-settings">When to Use Different Temperature Settings</h4>
<table>
<thead>
<tr>
<th>Temperature</th>
<th>Best For</th>
<th>Development Use Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.0 - 0.1</td>
<td>Deterministic outputs, factual responses</td>
<td>Code generation, debugging, technical explanations</td>
</tr>
<tr>
<td>0.2 - 0.5</td>
<td>Slightly varied but focused responses</td>
<td>Documentation generation, code comments, refactoring suggestions</td>
</tr>
<tr>
<td>0.6 - 0.8</td>
<td>Creative but coherent responses</td>
<td>Generating alternative approaches, brainstorming solutions</td>
</tr>
<tr>
<td>0.9 - 1.0</td>
<td>Highly diverse and unexpected outputs</td>
<td>Creative problem solving, generating test cases, finding edge cases</td>
</tr>
</tbody>
</table>
<pre class="codehilite"><code class="language-python">def generate_code(prompt, creativity_level=&quot;low&quot;):
    &quot;&quot;&quot;Generate code with appropriate temperature based on creativity needs.&quot;&quot;&quot;

    # Map creativity levels to temperature values
    temp_mapping = {
        &quot;none&quot;: 0.0,     # Purely deterministic
        &quot;low&quot;: 0.2,      # Slight variations
        &quot;medium&quot;: 0.5,   # Balanced
        &quot;high&quot;: 0.8      # Creative approaches
    }

    temperature = temp_mapping.get(creativity_level, 0.0)

    response = llm_client.generate(
        prompt,
        temperature=temperature
    )

    return response
</code></pre>

<h3 id="top-p-nucleus-sampling">Top-P (Nucleus Sampling)</h3>
<p>Top-P (typically 0-1) controls how the model selects tokens from the probability distribution:</p>
<ul>
<li><strong>Top-P = 0.1</strong>: Only the most likely tokens (more focused)</li>
<li><strong>Top-P = 0.5</strong>: More variety but still relatively constrained</li>
<li><strong>Top-P = 0.9</strong>: Wider range of possible outputs</li>
</ul>
<p>For most code-related tasks, a lower Top-P (0.1-0.3) is preferable as it leads to more precise outputs.</p>
<h3 id="frequency-and-presence-penalties">Frequency and Presence Penalties</h3>
<p>These parameters discourage repetition and can be useful in longer generations:</p>
<ul>
<li><strong>Frequency penalty</strong>: Reduces likelihood of repeating the same tokens</li>
<li><strong>Presence penalty</strong>: Reduces likelihood of repeating topics or themes</li>
</ul>
<p>For code generation, moderate frequency penalties (0.1-0.3) can help avoid redundant code structures.</p>
<h3 id="max-tokens-and-stopping-sequences">Max Tokens and Stopping Sequences</h3>
<ul>
<li><strong>Max tokens</strong>: Limits the length of the response</li>
<li><strong>Stopping sequences</strong>: Specific strings that tell the model to stop generating</li>
</ul>
<pre class="codehilite"><code class="language-python">def generate_function(function_spec):
    &quot;&quot;&quot;Generate just a function without additional explanation.&quot;&quot;&quot;

    prompt = f&quot;Write a function that {function_spec}. Provide only the code without explanation.&quot;

    response = llm_client.generate(
        prompt,
        max_tokens=500,
        stop=[&quot;\n\n&quot;, &quot;```&quot;, &quot;def &quot;, &quot;function &quot;]  # Stop after function definition
    )

    return response
</code></pre>

<h3 id="parameter-selection-framework">Parameter Selection Framework</h3>
<pre class="codehilite"><code class="language-python">def select_optimal_parameters(task_type, complexity):
    &quot;&quot;&quot;Select optimal LLM parameters based on task requirements.&quot;&quot;&quot;

    params = {
        &quot;temperature&quot;: 0.0,
        &quot;top_p&quot;: 1.0,
        &quot;frequency_penalty&quot;: 0.0,
        &quot;presence_penalty&quot;: 0.0
    }

    # Adjust based on task type
    if task_type == &quot;code_generation&quot;:
        params[&quot;temperature&quot;] = 0.0
        params[&quot;top_p&quot;] = 0.1
    elif task_type == &quot;code_explanation&quot;:
        params[&quot;temperature&quot;] = 0.1
        params[&quot;top_p&quot;] = 0.3
    elif task_type == &quot;refactoring&quot;:
        params[&quot;temperature&quot;] = 0.2
        params[&quot;frequency_penalty&quot;] = 0.3
    elif task_type == &quot;creative_solution&quot;:
        params[&quot;temperature&quot;] = 0.7
        params[&quot;top_p&quot;] = 0.9

    # Adjust based on complexity
    if complexity == &quot;high&quot;:
        params[&quot;temperature&quot;] = min(params[&quot;temperature&quot;] + 0.1, 1.0)
        params[&quot;top_p&quot;] = min(params[&quot;top_p&quot;] + 0.1, 1.0)
    elif complexity == &quot;low&quot;:
        params[&quot;temperature&quot;] = max(params[&quot;temperature&quot;] - 0.1, 0.0)
        params[&quot;top_p&quot;] = max(params[&quot;top_p&quot;] - 0.1, 0.0)

    return params
</code></pre>

<h2 id="chapter5_section4">Persona-Based Prompting</h2>
<p>Persona-based prompts instruct the LLM to adopt a specific expertise or perspective, leading to more specialized and appropriate outputs.</p>
<h3 id="the-persona-template">The Persona Template</h3>
<pre class="codehilite"><code>Act as a [role/persona] with expertise in [specific skills/domains]. Your task is to [specific task].

Key characteristics of this role:
- [characteristic 1]
- [characteristic 2]
- [characteristic 3]

Now, please [specific request].
</code></pre>

<h3 id="developer-personas-for-different-tasks">Developer Personas for Different Tasks</h3>
<h4 id="senior-developer-persona">Senior Developer Persona</h4>
<pre class="codehilite"><code>Act as a senior software developer with 15+ years of experience in production environments and expertise in system design, performance optimization, and best practices. Your task is to review the following code.

Key characteristics of your role:
- Focus on maintainability and scalability
- Attention to edge cases and error handling
- Awareness of performance implications
- Experience with enterprise coding standards

Now, please review this code and suggest improvements:

```[code block]```
</code></pre>

<h4 id="security-expert-persona">Security Expert Persona</h4>
<pre class="codehilite"><code>Act as a cybersecurity expert specializing in application security with experience performing security audits and penetration testing. Your task is to identify security vulnerabilities in the following code.

Key characteristics of your role:
- Deep knowledge of OWASP Top 10 vulnerabilities
- Experience with secure coding practices
- Understanding of common attack vectors
- Ability to suggest practical security mitigations

Now, please review this code for security vulnerabilities:

```[code block]```
</code></pre>

<h4 id="code-optimization-persona">Code Optimization Persona</h4>
<pre class="codehilite"><code>Act as a performance optimization specialist who focuses on making code run efficiently. Your expertise includes algorithmic optimization, memory management, and profiling techniques. Your task is to optimize the following function.

Key characteristics of your role:
- Deep understanding of time and space complexity
- Experience with profiling tools and bottleneck identification
- Knowledge of language-specific optimization techniques
- Focus on measurable performance improvements

Now, please optimize this code:

```[code block]```
</code></pre>

<h3 id="creating-composite-personas">Creating Composite Personas</h3>
<p>For complex tasks, you can create composite personas that combine multiple areas of expertise:</p>
<pre class="codehilite"><code>Act as a technical lead at a financial technology company who has expertise in both secure coding practices and high-performance systems. You specialize in designing backend systems that handle financial transactions and must balance security, compliance, and performance. Your task is to review and improve the following payment processing code.

Key characteristics of your role:
- Knowledge of financial compliance requirements (PCI DSS)
- Experience with secure transaction processing
- Expertise in optimizing high-throughput systems
- Background in designing fault-tolerant architectures

Now, please review and improve this payment processing code:

```[code block]```
</code></pre>

<h3 id="implementing-persona-selection-in-applications">Implementing Persona Selection in Applications</h3>
<p>For practical applications, you can create a library of personas and select the appropriate one based on the task:</p>
<pre class="codehilite"><code class="language-python">PERSONA_LIBRARY = {
    &quot;senior_dev&quot;: {
        &quot;intro&quot;: &quot;Act as a senior software developer with 15+ years of experience...&quot;,
        &quot;characteristics&quot;: [
            &quot;Focus on maintainability and scalability&quot;,
            &quot;Attention to edge cases and error handling&quot;,
            &quot;Awareness of performance implications&quot;
        ]
    },
    &quot;security_expert&quot;: {
        &quot;intro&quot;: &quot;Act as a cybersecurity expert specializing in application security...&quot;,
        &quot;characteristics&quot;: [
            &quot;Deep knowledge of OWASP Top 10 vulnerabilities&quot;,
            &quot;Experience with secure coding practices&quot;,
            &quot;Understanding of common attack vectors&quot;
        ]
    },
    # Additional personas...
}

def generate_with_persona(persona_key, task, content=None):
    &quot;&quot;&quot;Generate content using a specific persona.&quot;&quot;&quot;

    if persona_key not in PERSONA_LIBRARY:
        raise ValueError(f&quot;Unknown persona: {persona_key}&quot;)

    persona = PERSONA_LIBRARY[persona_key]

    prompt = f&quot;{persona['intro']}\n\nKey characteristics of your role:&quot;

    for characteristic in persona[&quot;characteristics&quot;]:
        prompt += f&quot;\n- {characteristic}&quot;

    prompt += f&quot;\n\nNow, please {task}&quot;

    if content:
        prompt += f&quot;:\n\n```\n{content}\n```&quot;

    response = llm_client.generate(prompt)
    return response
</code></pre>

<h2 id="chapter5_section5">Prompt Chaining and Orchestration Techniques</h2>
<p>Complex development tasks often require multiple LLM calls, with the output of one prompt feeding into another. Prompt chaining creates sophisticated workflows that combine multiple prompting techniques.</p>
<h3 id="basic-prompt-chain-pattern">Basic Prompt Chain Pattern</h3>
<pre class="codehilite"><code class="language-python">def multi_stage_code_development(task_description):
    &quot;&quot;&quot;Generate code through multiple stages of refinement.&quot;&quot;&quot;

    # Stage 1: Design the solution approach
    design_prompt = f&quot;&quot;&quot;
    I need to develop a solution for: {task_description}

    Please provide a high-level design with:
    1. Key components/functions needed
    2. Data structures to use
    3. Overall algorithm or approach
    4. Potential edge cases to handle

    Just focus on the design, not the implementation.
    &quot;&quot;&quot;

    design = llm_client.generate(design_prompt)

    # Stage 2: Implement based on the design
    implementation_prompt = f&quot;&quot;&quot;
    Based on the following design:

    {design}

    Implement the complete solution in code. Include comments explaining key parts.
    &quot;&quot;&quot;

    implementation = llm_client.generate(implementation_prompt)
    code = extract_code(implementation)

    # Stage 3: Test case generation
    test_prompt = f&quot;&quot;&quot;
    For the following code:

    ```
    {code}
    ```

    Generate comprehensive test cases that cover:
    1. Normal operation
    2. Edge cases
    3. Error conditions

    Provide the test cases as executable code.
    &quot;&quot;&quot;

    test_cases = llm_client.generate(test_prompt)

    return {
        &quot;design&quot;: design,
        &quot;implementation&quot;: code,
        &quot;tests&quot;: extract_code(test_cases)
    }
</code></pre>

<h3 id="advanced-orchestration-the-specialist-pattern">Advanced Orchestration: The Specialist Pattern</h3>
<p>The specialist pattern uses different prompts/personas for different aspects of a complex task:</p>
<pre class="codehilite"><code class="language-python">def develop_feature_with_specialists(feature_spec):
    &quot;&quot;&quot;Develop a complete feature using specialist personas for each aspect.&quot;&quot;&quot;

    specialists = {
        &quot;architect&quot;: &quot;system design and architecture&quot;,
        &quot;implementer&quot;: &quot;clean, efficient implementation&quot;,
        &quot;security_expert&quot;: &quot;security best practices&quot;,
        &quot;tester&quot;: &quot;comprehensive testing&quot;,
        &quot;documenter&quot;: &quot;clear documentation&quot;
    }

    results = {}
    accumulated_context = feature_spec

    # Step through specialist chain
    for role, expertise in specialists.items():
        prompt = f&quot;&quot;&quot;
        Act as a specialist in {expertise}.

        Project context so far:
        {accumulated_context}

        Your task is to contribute the {role} perspective to this feature.
        &quot;&quot;&quot;

        response = llm_client.generate(prompt)
        results[role] = response

        # Add this specialist's contribution to the accumulated context
        accumulated_context += f&quot;\n\n{role.upper()} CONTRIBUTION:\n{response}&quot;

    return results
</code></pre>

<h3 id="parallel-prompting-with-aggregation">Parallel Prompting with Aggregation</h3>
<p>For some tasks, you can get multiple independent perspectives and then combine them:</p>
<pre class="codehilite"><code class="language-python">import asyncio

async def get_multiple_perspectives(code_to_review, perspectives=None):
    &quot;&quot;&quot;Get multiple review perspectives on the same code and aggregate results.&quot;&quot;&quot;

    if perspectives is None:
        perspectives = [&quot;readability&quot;, &quot;performance&quot;, &quot;security&quot;, &quot;maintainability&quot;]

    async def get_perspective(aspect):
        prompt = f&quot;&quot;&quot;
        Review the following code focusing ONLY on {aspect}:

        ```
        {code_to_review}
        ```

        Provide specific issues and recommendations related to {aspect}.
        &quot;&quot;&quot;

        return {
            &quot;aspect&quot;: aspect,
            &quot;review&quot;: await llm_client.generate_async(prompt)
        }

    # Get all perspectives in parallel
    review_tasks = [get_perspective(aspect) for aspect in perspectives]
    reviews = await asyncio.gather(*review_tasks)

    # Create aggregation prompt
    aggregation_prompt = f&quot;&quot;&quot;
    I have received the following code reviews from different perspectives:

    &quot;&quot;&quot;

    for review in reviews:
        aggregation_prompt += f&quot;&quot;&quot;
        {review['aspect'].upper()} REVIEW:
        {review['review']}

        &quot;&quot;&quot;

    aggregation_prompt += &quot;&quot;&quot;
    Synthesize these reviews into a consolidated summary of:
    1. The most critical issues to address
    2. Recommended improvements in priority order
    3. Positive aspects of the code worth preserving
    &quot;&quot;&quot;

    consolidated_review = await llm_client.generate_async(aggregation_prompt)

    return {
        &quot;individual_reviews&quot;: reviews,
        &quot;consolidated_review&quot;: consolidated_review
    }
</code></pre>

<h2 id="chapter5_section6">Error Handling Strategies for Inadequate LLM Responses</h2>
<p>When working with LLMs, you'll inevitably encounter responses that don't meet your requirements. Implementing robust error handling is crucial for production applications.</p>
<h3 id="response-validation-pattern">Response Validation Pattern</h3>
<p>Always validate LLM outputs before using them in your application:</p>
<pre class="codehilite"><code class="language-python">def validate_code_response(code, language=&quot;python&quot;):
    &quot;&quot;&quot;Validate that an LLM-generated code snippet is valid.&quot;&quot;&quot;

    # Basic syntactic validation
    if language == &quot;python&quot;:
        try:
            ast.parse(code)
            return True, &quot;Valid Python syntax&quot;
        except SyntaxError as e:
            return False, f&quot;Python syntax error: {str(e)}&quot;
    elif language == &quot;javascript&quot;:
        # Use appropriate JS parser here
        pass

    # You could add additional validation like:
    # - Checking that specific functions exist
    # - Verifying that requirements are met
    # - Testing with example inputs

    return True, &quot;Passed validation&quot;
</code></pre>

<h3 id="retry-with-enhanced-context">Retry with Enhanced Context</h3>
<p>When an LLM response is inadequate, retry with additional context:</p>
<pre class="codehilite"><code class="language-python">def get_working_solution(problem_statement, max_attempts=3):
    &quot;&quot;&quot;Get a working solution by retrying with improved context.&quot;&quot;&quot;

    prompt = f&quot;Write a function that {problem_statement}. Include proper error handling.&quot;

    for attempt in range(1, max_attempts + 1):
        print(f&quot;Attempt {attempt}/{max_attempts}&quot;)

        response = llm_client.generate(prompt)
        code = extract_code(response)

        valid, message = validate_code_response(code)
        if valid:
            return code

        # If invalid, enhance the prompt with error information
        prompt = f&quot;&quot;&quot;
        You provided the following solution:

        ```
        {code}
        ```

        However, there was an issue: {message}

        Please provide a corrected solution that addresses this problem.
        Original task: Write a function that {problem_statement}
        &quot;&quot;&quot;

    # If we exhaust attempts, raise an exception
    raise Exception(f&quot;Failed to generate valid code after {max_attempts} attempts&quot;)
</code></pre>

<h3 id="fallback-chain-strategy">Fallback Chain Strategy</h3>
<p>Implement a chain of fallbacks when an LLM response doesn't meet requirements:</p>
<pre class="codehilite"><code class="language-python">def generate_with_fallbacks(prompt, validators=None):
    &quot;&quot;&quot;Generate content with a series of fallback strategies.&quot;&quot;&quot;

    if validators is None:
        validators = [basic_validator]

    # First attempt: Standard generation
    response = llm_client.generate(prompt)

    for validator in validators:
        valid, message = validator(response)
        if valid:
            return response

    # Fallback 1: Retry with more specific instructions
    enhanced_prompt = f&quot;&quot;&quot;
    Previous attempt did not meet the requirements because: {message}

    Let me clarify what's needed:
    {prompt}

    Please ensure your response meets all requirements.
    &quot;&quot;&quot;

    response = llm_client.generate(enhanced_prompt)

    for validator in validators:
        valid, message = validator(response)
        if valid:
            return response

    # Fallback 2: Try with different parameters
    response = llm_client.generate(
        enhanced_prompt,
        temperature=0.0,  # Switch to deterministic mode
        max_tokens=2000   # Allow more space for complete response
    )

    for validator in validators:
        valid, message = validator(response)
        if valid:
            return response

    # Fallback 3: Try a different model (e.g., more capable)
    response = llm_client.generate(
        enhanced_prompt,
        model=&quot;more-capable-model&quot;  # e.g., gpt-4 instead of gpt-3.5-turbo
    )

    for validator in validators:
        valid, message = validator(response)
        if valid:
            return response

    # If all fallbacks fail, return the best effort with a warning
    return {
        &quot;response&quot;: response,
        &quot;warning&quot;: &quot;Response may not meet all requirements&quot;,
        &quot;validation_message&quot;: message
    }
</code></pre>

<h2 id="chapter5_section7">Evaluating LLM Output Quality Programmatically</h2>
<p>Systematic evaluation of LLM outputs is essential for maintaining quality and improving prompt design over time.</p>
<h3 id="code-execution-evaluation">Code Execution Evaluation</h3>
<p>For code generation tasks, the ultimate test is whether the code executes correctly:</p>
<pre class="codehilite"><code class="language-python">import subprocess
import tempfile
import os

def evaluate_code_execution(code, test_input=None, expected_output=None, timeout=5):
    &quot;&quot;&quot;Evaluate code by executing it and checking the output.&quot;&quot;&quot;

    with tempfile.NamedTemporaryFile(suffix='.py', delete=False) as temp:
        temp_name = temp.name
        temp.write(code.encode('utf-8'))

    try:
        # Execute the code
        if test_input:
            # If we have test input, provide it via stdin
            process = subprocess.run(
                ['python', temp_name],
                input=test_input.encode('utf-8'),
                capture_output=True,
                timeout=timeout
            )
        else:
            process = subprocess.run(
                ['python', temp_name],
                capture_output=True,
                timeout=timeout
            )

        stdout = process.stdout.decode('utf-8')
        stderr = process.stderr.decode('utf-8')

        # Check if execution was successful
        if process.returncode != 0:
            return False, f&quot;Execution failed with error: {stderr}&quot;

        # If expected output is provided, check against it
        if expected_output is not None:
            if stdout.strip() == expected_output.strip():
                return True, &quot;Output matches expected result&quot;
            else:
                return False, f&quot;Output doesn't match expected result.\nExpected: {expected_output}\nActual: {stdout}&quot;

        return True, stdout

    except subprocess.TimeoutExpired:
        return False, f&quot;Execution timed out after {timeout} seconds&quot;

    finally:
        # Clean up the temporary file
        if os.path.exists(temp_name):
            os.unlink(temp_name)
</code></pre>

<h3 id="unit-test-generation-and-execution">Unit Test Generation and Execution</h3>
<p>Generate unit tests and use them to validate LLM-generated code:</p>
<pre class="codehilite"><code class="language-python">def evaluate_with_generated_tests(code_solution, problem_description):
    &quot;&quot;&quot;Evaluate code by generating and running tests for it.&quot;&quot;&quot;

    # Generate test cases based on problem description
    test_generation_prompt = f&quot;&quot;&quot;
    For the following problem:
    {problem_description}

    Generate comprehensive pytest unit tests that cover normal cases, edge cases, and error cases.
    Focus only on the tests, assuming the solution is implemented in a function called 'solution'.
    &quot;&quot;&quot;

    test_code = llm_client.generate(test_generation_prompt)
    test_code = extract_code(test_code)

    # Combine solution and tests in a temporary file
    full_code = f&quot;&quot;&quot;
{code_solution}

# Generated tests
{test_code}
    &quot;&quot;&quot;

    # Execute the tests
    with tempfile.NamedTemporaryFile(suffix='.py', delete=False) as temp:
        temp_name = temp.name
        temp.write(full_code.encode('utf-8'))

    try:
        process = subprocess.run(
            ['pytest', temp_name, '-v'],
            capture_output=True
        )

        stdout = process.stdout.decode('utf-8')
        stderr = process.stderr.decode('utf-8')

        # Parse test results
        if process.returncode == 0:
            return True, &quot;All tests passed&quot;, stdout
        else:
            return False, &quot;Some tests failed&quot;, stdout + &quot;\n&quot; + stderr

    finally:
        if os.path.exists(temp_name):
            os.unlink(temp_name)
</code></pre>

<h3 id="functional-requirements-verification">Functional Requirements Verification</h3>
<p>Verify that the generated solution meets all functional requirements:</p>
<pre class="codehilite"><code class="language-python">def verify_requirements_coverage(code, requirements):
    &quot;&quot;&quot;Check if code likely addresses all specified requirements.&quot;&quot;&quot;

    evaluation_prompt = f&quot;&quot;&quot;
    Given the following code:

    ```
    {code}
    ```

    And these requirements:

    &quot;&quot;&quot;

    for i, req in enumerate(requirements, 1):
        evaluation_prompt += f&quot;{i}. {req}\n&quot;

    evaluation_prompt += &quot;&quot;&quot;
    For each requirement, determine if the code addresses it:
    1. Respond with YES if the requirement is clearly addressed
    2. Respond with PARTIAL if the requirement is partly addressed
    3. Respond with NO if the requirement is not addressed

    Format your response as a JSON object with requirement numbers as keys and assessment as values,
    with an additional 'explanation' field for each requirement.
    &quot;&quot;&quot;

    response = llm_client.generate(evaluation_prompt)

    try:
        # Extract JSON from the response
        import re
        import json

        json_match = re.search(r'{.*}', response, re.DOTALL)
        if json_match:
            assessment = json.loads(json_match.group(0))

            # Calculate coverage percentage
            covered = sum(1 for v in assessment.values() if isinstance(v, dict) and v.get('assessment') == 'YES')
            partial = sum(0.5 for v in assessment.values() if isinstance(v, dict) and v.get('assessment') == 'PARTIAL')
            total_requirements = len(requirements)

            coverage_pct = (covered + partial) / total_requirements * 100 if total_requirements &gt; 0 else 0

            return {
                &quot;coverage_percentage&quot;: coverage_pct,
                &quot;detailed_assessment&quot;: assessment,
                &quot;missing_requirements&quot;: [
                    req for i, req in enumerate(requirements, 1) 
                    if str(i) in assessment and assessment[str(i)].get('assessment') == 'NO'
                ]
            }
    except Exception as e:
        return {
            &quot;error&quot;: f&quot;Failed to parse assessment: {str(e)}&quot;,
            &quot;raw_response&quot;: response
        }
</code></pre>

<h3 id="comprehensive-evaluation-framework">Comprehensive Evaluation Framework</h3>
<p>For production applications, implement a comprehensive evaluation framework:</p>
<pre class="codehilite"><code class="language-python">class LLMCodeEvaluator:
    &quot;&quot;&quot;Framework for evaluating LLM-generated code quality.&quot;&quot;&quot;

    def __init__(self, code, language=&quot;python&quot;):
        self.code = code
        self.language = language
        self.evaluation_results = {}

    def run_all_evaluations(self):
        &quot;&quot;&quot;Run all available evaluations.&quot;&quot;&quot;
        self.evaluate_syntax()
        self.evaluate_style()
        self.evaluate_complexity()
        self.evaluate_security()

        if self.language == &quot;python&quot;:
            self.run_python_specific_evaluations()

        return self.get_summary()

    def evaluate_syntax(self):
        &quot;&quot;&quot;Check for syntax errors.&quot;&quot;&quot;
        if self.language == &quot;python&quot;:
            try:
                ast.parse(self.code)
                self.evaluation_results[&quot;syntax&quot;] = {
                    &quot;pass&quot;: True,
                    &quot;message&quot;: &quot;No syntax errors detected&quot;
                }
            except SyntaxError as e:
                self.evaluation_results[&quot;syntax&quot;] = {
                    &quot;pass&quot;: False,
                    &quot;message&quot;: f&quot;Syntax error: {str(e)}&quot;
                }
        # Add handlers for other languages

    def evaluate_style(self):
        &quot;&quot;&quot;Evaluate code style compliance.&quot;&quot;&quot;
        # For Python, could use tools like flake8, black
        # For JS, could use ESLint
        # Here's a simplified example using an LLM for style evaluation:

        style_prompt = f&quot;&quot;&quot;
        Review this {self.language} code for style issues:

        ```
        {self.code}
        ```

        Identify any style issues according to common {self.language} conventions.
        Return your response as a JSON with 'issues' (array) and 'score' (0-10).
        &quot;&quot;&quot;

        response = llm_client.generate(style_prompt)
        # Parse response and extract style evaluation
        # (Implementation details omitted)

        self.evaluation_results[&quot;style&quot;] = {
            &quot;pass&quot;: style_score &gt; 7,
            &quot;score&quot;: style_score,
            &quot;issues&quot;: style_issues
        }

    # Additional evaluation methods...

    def get_summary(self):
        &quot;&quot;&quot;Generate overall evaluation summary.&quot;&quot;&quot;
        total_checks = len(self.evaluation_results)
        passed_checks = sum(1 for result in self.evaluation_results.values() if result.get(&quot;pass&quot;))

        return {
            &quot;overall_score&quot;: passed_checks / total_checks if total_checks &gt; 0 else 0,
            &quot;passed_checks&quot;: passed_checks,
            &quot;total_checks&quot;: total_checks,
            &quot;detailed_results&quot;: self.evaluation_results
        }
</code></pre>

<h2 id="chapter5_section8">Conclusion</h2>
<p>Advanced prompting techniques give developers unprecedented control over LLM outputs. By mastering Chain-of-Thought reasoning, self-correction, parameter tuning, persona-based prompting, and effective error handling strategies, you can create more reliable, higher-quality solutions for complex development tasks.</p>
<p>The techniques in this chapter build upon the foundation established earlier and represent the current state of the art in prompt engineering for software development. As you apply these methods in your work, you'll develop an intuitive sense for which techniques work best for different types of tasks.</p>
<p>In the next chapter, we'll put these advanced techniques into practice by building a complete Smart Code Assistant that can help with a wide range of development tasks.</p>
<h2 id="chapter5_section9">Exercises</h2>
<ol>
<li>
<p>Create a Chain-of-Thought prompt for solving a complex algorithmic problem, and compare the results with a simple prompt for the same problem.</p>
</li>
<li>
<p>Implement a self-correction workflow for a code generation task that includes multiple rounds of refinement.</p>
</li>
<li>
<p>Experiment with different temperature settings for the same code generation task, and document how the outputs differ.</p>
</li>
<li>
<p>Design three different personas for code review, focusing on different aspects (e.g., security, performance, readability), and compare their feedback on the same piece of code.</p>
</li>
<li>
<p>Build a simple prompt chaining system that generates code, tests, and documentation for a specific function in sequence.</p>
</li>
</ol></div><div class="navigation"><a href="#chapter4" class="prev-chapter">Previous Chapter</a><a href="#chapter6" class="next-chapter">Next Chapter</a></div><div class="chapter" id="chapter-content-6"><h1 id="chapter6">Chapter 6: Building Effective Developer Tooling for LLM Applications</h1>
<p>In the previous chapters, we've explored the fundamentals of prompt engineering and various techniques to create effective prompts. Now, it's time to take our skills to the next level by implementing robust developer tooling for LLM applications. As LLMs become integral parts of modern software systems, proper tooling becomes essential for maintainability, scalability, and reliability.</p>
<h2 id="chapter6_section1">6.1 Prompt Libraries and Reuse Patterns</h2>
<h3 id="611-the-need-for-prompt-management">6.1.1 The Need for Prompt Management</h3>
<p>As your project grows, managing prompts becomes increasingly challenging. Without proper organization, you might face:</p>
<ul>
<li>Duplicate prompts across different parts of your application</li>
<li>Inconsistent prompting styles and formats</li>
<li>Difficulty in tracking which prompts work best for specific tasks</li>
<li>Challenges in version control and prompt evolution</li>
</ul>
<h3 id="612-building-a-prompt-library">6.1.2 Building a Prompt Library</h3>
<p>Let's create a simple yet effective prompt library in Python:</p>
<pre class="codehilite"><code class="language-python"># A basic prompt library implementation

class PromptTemplate:
    def __init__(self, template, required_variables=None):
        self.template = template
        self.required_variables = required_variables or []

    def format(self, **kwargs):
        # Check if all required variables are provided
        missing_vars = [var for var in self.required_variables if var not in kwargs]
        if missing_vars:
            raise ValueError(f&quot;Missing required variables: {', '.join(missing_vars)}&quot;)

        # Format the template with the provided variables
        return self.template.format(**kwargs)

class PromptLibrary:
    def __init__(self):
        self.prompts = {}

    def add_prompt(self, name, template, required_variables=None):
        self.prompts[name] = PromptTemplate(template, required_variables)

    def get_prompt(self, name, **kwargs):
        if name not in self.prompts:
            raise KeyError(f&quot;Prompt '{name}' not found in the library&quot;)
        return self.prompts[name].format(**kwargs)
</code></pre>

<p>Usage example:</p>
<pre class="codehilite"><code class="language-python"># Initialize the library
prompt_lib = PromptLibrary()

# Add prompts with templates
prompt_lib.add_prompt(
    &quot;code_explanation&quot;,
    &quot;Explain the following {language} code:\n\n```{language}\n{code}\n```\n\nProvide a detailed explanation including:&quot;,
    [&quot;language&quot;, &quot;code&quot;]
)

prompt_lib.add_prompt(
    &quot;bug_fix&quot;,
    &quot;Fix the following {language} code that has a bug:\n\n```{language}\n{code}\n```\n\nError message: {error}\n\nProvide the corrected code and explain the fix.&quot;,
    [&quot;language&quot;, &quot;code&quot;, &quot;error&quot;]
)

# Use the prompt template
python_code = &quot;def factorial(n):\n    if n == 0:\n        return 1\n    return n * factorial(n-1)&quot;
formatted_prompt = prompt_lib.get_prompt(&quot;code_explanation&quot;, language=&quot;python&quot;, code=python_code)
</code></pre>

<h3 id="613-advanced-prompt-organization-patterns">6.1.3 Advanced Prompt Organization Patterns</h3>
<p>For larger projects, consider organizing prompts hierarchically:</p>
<pre class="codehilite"><code class="language-python"># Domain-specific prompt libraries
class CodePromptLibrary(PromptLibrary):
    def __init__(self):
        super().__init__()
        self._initialize_code_prompts()

    def _initialize_code_prompts(self):
        self.add_prompt(&quot;generate_function&quot;, &quot;Write a {language} function that {requirement}.&quot;, [&quot;language&quot;, &quot;requirement&quot;])
        self.add_prompt(&quot;optimize_code&quot;, &quot;Optimize the following {language} code for {optimization_goal}:\n\n```{language}\n{code}\n```&quot;, [&quot;language&quot;, &quot;optimization_goal&quot;, &quot;code&quot;])
        # More code-related prompts...
</code></pre>

<h2 id="chapter6_section2">6.2 Debugging Tools for LLM Applications</h2>
<h3 id="621-prompt-debugging">6.2.1 Prompt Debugging</h3>
<p>Debugging LLM applications presents unique challenges compared to traditional software. Let's implement a simple prompt debugger:</p>
<pre class="codehilite"><code class="language-python">import json
from datetime import datetime

class PromptDebugger:
    def __init__(self, log_file=None):
        self.log_file = log_file
        self.history = []

    def log_interaction(self, prompt, response, metadata=None):
        interaction = {
            &quot;timestamp&quot;: datetime.now().isoformat(),
            &quot;prompt&quot;: prompt,
            &quot;response&quot;: response,
            &quot;metadata&quot;: metadata or {}
        }
        self.history.append(interaction)

        if self.log_file:
            with open(self.log_file, 'a') as f:
                f.write(json.dumps(interaction) + &quot;\n&quot;)

        return interaction

    def analyze_token_usage(self, interaction):
        if 'token_usage' in interaction['metadata']:
            usage = interaction['metadata']['token_usage']
            return f&quot;Prompt tokens: {usage.get('prompt_tokens', 'N/A')}, &quot; \
                   f&quot;Completion tokens: {usage.get('completion_tokens', 'N/A')}, &quot; \
                   f&quot;Total tokens: {usage.get('total_tokens', 'N/A')}&quot;
        return &quot;Token usage data not available&quot;

    def compare_interactions(self, interaction1_idx, interaction2_idx):
        if interaction1_idx &gt;= len(self.history) or interaction2_idx &gt;= len(self.history):
            return &quot;Invalid interaction indices&quot;

        int1 = self.history[interaction1_idx]
        int2 = self.history[interaction2_idx]

        # Compare prompts
        prompt_diff = self._simple_diff(int1['prompt'], int2['prompt'])

        # Compare responses (simplified)
        response_similarity = self._calculate_similarity(int1['response'], int2['response'])

        return {
            &quot;prompt_differences&quot;: prompt_diff,
            &quot;response_similarity&quot;: f&quot;{response_similarity:.2f}%&quot;
        }

    def _simple_diff(self, text1, text2):
        # A very simple diff implementation
        # In a real application, use a proper diff library
        if text1 == text2:
            return &quot;No differences&quot;

        # Basic character-by-character comparison
        diffs = []
        for i, (c1, c2) in enumerate(zip(text1, text2)):
            if c1 != c2:
                diffs.append(f&quot;Pos {i}: '{c1}' vs '{c2}'&quot;)

        if len(text1) != len(text2):
            diffs.append(f&quot;Length difference: {len(text1)} vs {len(text2)}&quot;)

        return diffs[:10]  # Only show first 10 differences

    def _calculate_similarity(self, text1, text2):
        # Simple similarity calculation
        # In a real application, use a more sophisticated algorithm
        common_chars = sum(1 for c1, c2 in zip(text1, text2) if c1 == c2)
        total_length = max(len(text1), len(text2))
        return (common_chars / total_length) * 100 if total_length &gt; 0 else 100
</code></pre>

<p>Example usage with OpenAI's API:</p>
<pre class="codehilite"><code class="language-python">import openai

debugger = PromptDebugger(log_file=&quot;llm_debug_log.jsonl&quot;)

def query_llm(prompt, model=&quot;gpt-3.5-turbo&quot;):
    response = openai.ChatCompletion.create(
        model=model,
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}]
    )

    content = response.choices[0].message.content

    # Log the interaction with metadata
    debugger.log_interaction(
        prompt=prompt,
        response=content,
        metadata={
            &quot;model&quot;: model,
            &quot;token_usage&quot;: response.usage._asdict() if hasattr(response, &quot;usage&quot;) else None,
            &quot;finish_reason&quot;: response.choices[0].finish_reason
        }
    )

    return content
</code></pre>

<h3 id="622-visualizing-llm-behavior">6.2.2 Visualizing LLM Behavior</h3>
<p>To understand how changes in prompts affect LLM responses, visualization tools can be invaluable:</p>
<pre class="codehilite"><code class="language-python">import matplotlib.pyplot as plt
import numpy as np

def visualize_token_usage(debugger, last_n=10):
    &quot;&quot;&quot;Visualize token usage for the last N interactions&quot;&quot;&quot;
    if len(debugger.history) == 0:
        return &quot;No history available&quot;

    # Get data for the last n interactions
    history = debugger.history[-last_n:]

    prompt_tokens = []
    completion_tokens = []
    labels = []

    for i, interaction in enumerate(history):
        metadata = interaction.get('metadata', {})
        usage = metadata.get('token_usage', {})

        prompt_tokens.append(usage.get('prompt_tokens', 0))
        completion_tokens.append(usage.get('completion_tokens', 0))
        labels.append(f&quot;Query {i+1}&quot;)

    # Create stacked bar chart
    width = 0.35
    fig, ax = plt.subplots(figsize=(12, 7))

    ax.bar(labels, prompt_tokens, width, label='Prompt Tokens')
    ax.bar(labels, completion_tokens, width, bottom=prompt_tokens, label='Completion Tokens')

    ax.set_ylabel('Token Count')
    ax.set_title('Token Usage by Query')
    ax.legend()

    plt.tight_layout()
    plt.xticks(rotation=45)
    plt.savefig('token_usage.png')
    plt.close()

    return &quot;Token usage visualization saved as 'token_usage.png'&quot;
</code></pre>

<h2 id="chapter6_section3">6.3 Performance Profiling and Optimization</h2>
<h3 id="631-measuring-llm-application-performance">6.3.1 Measuring LLM Application Performance</h3>
<p>Performance in LLM applications involves several metrics:</p>
<pre class="codehilite"><code class="language-python">import time
import statistics
from functools import wraps

class LLMProfiler:
    def __init__(self):
        self.metrics = {
            &quot;latency&quot;: [],
            &quot;token_throughput&quot;: [],
            &quot;success_rate&quot;: {&quot;success&quot;: 0, &quot;failure&quot;: 0},
            &quot;cost&quot;: []
        }

    def profile_request(self, func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            error = None
            response = None

            try:
                response = func(*args, **kwargs)
                self.metrics[&quot;success_rate&quot;][&quot;success&quot;] += 1
            except Exception as e:
                error = e
                self.metrics[&quot;success_rate&quot;][&quot;failure&quot;] += 1

            end_time = time.time()
            latency = end_time - start_time
            self.metrics[&quot;latency&quot;].append(latency)

            # Calculate token throughput if possible
            if response and hasattr(response, &quot;usage&quot;):
                total_tokens = response.usage.total_tokens
                tokens_per_second = total_tokens / latency if latency &gt; 0 else 0
                self.metrics[&quot;token_throughput&quot;].append(tokens_per_second)

                # Calculate approximate cost (example for GPT-3.5-turbo)
                # Rates as of 2023, adjust as needed
                prompt_cost = response.usage.prompt_tokens * 0.0015 / 1000  # $0.0015 per 1K tokens
                completion_cost = response.usage.completion_tokens * 0.002 / 1000  # $0.002 per 1K tokens
                total_cost = prompt_cost + completion_cost
                self.metrics[&quot;cost&quot;].append(total_cost)

            if error:
                raise error

            return response

        return wrapper

    def get_summary(self):
        latency_stats = {
            &quot;min&quot;: min(self.metrics[&quot;latency&quot;]) if self.metrics[&quot;latency&quot;] else None,
            &quot;max&quot;: max(self.metrics[&quot;latency&quot;]) if self.metrics[&quot;latency&quot;] else None,
            &quot;avg&quot;: statistics.mean(self.metrics[&quot;latency&quot;]) if self.metrics[&quot;latency&quot;] else None,
            &quot;p95&quot;: self._percentile(self.metrics[&quot;latency&quot;], 95),
            &quot;p99&quot;: self._percentile(self.metrics[&quot;latency&quot;], 99)
        }

        throughput_stats = {
            &quot;avg&quot;: statistics.mean(self.metrics[&quot;token_throughput&quot;]) if self.metrics[&quot;token_throughput&quot;] else None
        }

        success_rate = (
            self.metrics[&quot;success_rate&quot;][&quot;success&quot;] / 
            (self.metrics[&quot;success_rate&quot;][&quot;success&quot;] + self.metrics[&quot;success_rate&quot;][&quot;failure&quot;])
            if (self.metrics[&quot;success_rate&quot;][&quot;success&quot;] + self.metrics[&quot;success_rate&quot;][&quot;failure&quot;]) &gt; 0
            else 0
        ) * 100

        total_cost = sum(self.metrics[&quot;cost&quot;])

        return {
            &quot;latency_ms&quot;: {k: v*1000 if v is not None else None for k, v in latency_stats.items()},
            &quot;throughput_tokens_per_sec&quot;: throughput_stats,
            &quot;success_rate_percent&quot;: success_rate,
            &quot;total_cost_usd&quot;: total_cost,
            &quot;request_count&quot;: len(self.metrics[&quot;latency&quot;])
        }

    def _percentile(self, data, percentile):
        if not data:
            return None
        sorted_data = sorted(data)
        index = int(len(sorted_data) * (percentile / 100))
        return sorted_data[index]
</code></pre>

<h3 id="632-optimizing-prompt-performance">6.3.2 Optimizing Prompt Performance</h3>
<p>We can optimize prompts in several ways:</p>
<ol>
<li><strong>Prompt Compression Techniques:</strong></li>
</ol>
<pre class="codehilite"><code class="language-python">def compress_prompt(prompt, max_length=None):
    &quot;&quot;&quot;Compress a prompt by removing redundancies while preserving meaning&quot;&quot;&quot;
    # Simple compression techniques
    compressed = prompt

    # Remove redundant whitespace
    compressed = &quot; &quot;.join(compressed.split())

    # Replace common verbose phrases
    replacements = {
        &quot;Please provide a detailed explanation of&quot;: &quot;Explain&quot;,
        &quot;I would like you to&quot;: &quot;&quot;,
        &quot;It would be great if you could&quot;: &quot;&quot;,
        &quot;Can you please&quot;: &quot;&quot;,
    }

    for verbose, concise in replacements.items():
        compressed = compressed.replace(verbose, concise)

    # If a maximum length is specified, truncate while preserving key instructions
    if max_length and len(compressed) &gt; max_length:
        # This is a simplistic approach - a real implementation would be more sophisticated
        lines = compressed.split('. ')
        result = []
        current_length = 0

        # Always include the first line (assumed to contain the main instruction)
        result.append(lines[0])
        current_length += len(lines[0])

        # Add as many additional lines as fit within max_length
        for line in lines[1:]:
            if current_length + len(line) + 2 &lt;= max_length:  # +2 for the '. '
                result.append(line)
                current_length += len(line) + 2
            else:
                break

        compressed = '. '.join(result)
        if not compressed.endswith('.'):
            compressed += '.'

    return compressed
</code></pre>

<ol>
<li><strong>Caching LLM Responses:</strong></li>
</ol>
<pre class="codehilite"><code class="language-python">import hashlib
import json
import os
import pickle

class LLMResponseCache:
    def __init__(self, cache_dir=&quot;llm_cache&quot;, ttl_seconds=86400):
        &quot;&quot;&quot;Initialize the cache with a directory and time-to-live&quot;&quot;&quot;
        self.cache_dir = cache_dir
        self.ttl_seconds = ttl_seconds
        os.makedirs(cache_dir, exist_ok=True)

    def _get_cache_key(self, prompt, model, temperature):
        &quot;&quot;&quot;Create a unique cache key from the request parameters&quot;&quot;&quot;
        key_data = {
            &quot;prompt&quot;: prompt,
            &quot;model&quot;: model,
            &quot;temperature&quot;: temperature
        }
        key_string = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_string.encode()).hexdigest()

    def _get_cache_path(self, key):
        &quot;&quot;&quot;Get the file path for a cache key&quot;&quot;&quot;
        return os.path.join(self.cache_dir, f&quot;{key}.pkl&quot;)

    def get(self, prompt, model, temperature=0):
        &quot;&quot;&quot;Retrieve a response from the cache if it exists and is still valid&quot;&quot;&quot;
        key = self._get_cache_key(prompt, model, temperature)
        cache_path = self._get_cache_path(key)

        if not os.path.exists(cache_path):
            return None

        # Check if cache has expired
        cache_age = time.time() - os.path.getmtime(cache_path)
        if cache_age &gt; self.ttl_seconds:
            os.remove(cache_path)  # Remove expired cache
            return None

        try:
            with open(cache_path, 'rb') as f:
                return pickle.load(f)
        except:
            return None

    def set(self, prompt, model, temperature, response):
        &quot;&quot;&quot;Store a response in the cache&quot;&quot;&quot;
        key = self._get_cache_key(prompt, model, temperature)
        cache_path = self._get_cache_path(key)

        with open(cache_path, 'wb') as f:
            pickle.dump(response, f)
</code></pre>

<p>Example usage of caching:</p>
<pre class="codehilite"><code class="language-python">cache = LLMResponseCache()

def query_llm_with_cache(prompt, model=&quot;gpt-3.5-turbo&quot;, temperature=0):
    # Try to get from cache first
    cached_response = cache.get(prompt, model, temperature)
    if cached_response:
        print(&quot;Cache hit!&quot;)
        return cached_response

    # If not in cache, make the API call
    print(&quot;Cache miss, calling API...&quot;)
    response = openai.ChatCompletion.create(
        model=model,
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
        temperature=temperature
    )

    # Store in cache for future use
    cache.set(prompt, model, temperature, response)

    return response
</code></pre>

<h2 id="chapter6_section4">6.4 Integration with Existing Development Workflows</h2>
<h3 id="641-command-line-tools-for-prompt-engineering">6.4.1 Command Line Tools for Prompt Engineering</h3>
<p>Creating a simple CLI tool for prompt engineering:</p>
<pre class="codehilite"><code class="language-python">#!/usr/bin/env python
import argparse
import sys
import json
import openai
from pathlib import Path

def setup_argparser():
    parser = argparse.ArgumentParser(description=&quot;LLM Prompt Engineering CLI Tool&quot;)
    subparsers = parser.add_subparsers(dest=&quot;command&quot;, help=&quot;Commands&quot;)

    # Query LLM command
    query_parser = subparsers.add_parser(&quot;query&quot;, help=&quot;Query an LLM with a prompt&quot;)
    query_parser.add_argument(&quot;--prompt&quot;, &quot;-p&quot;, help=&quot;The prompt to send&quot;)
    query_parser.add_argument(&quot;--prompt-file&quot;, &quot;-f&quot;, help=&quot;File containing the prompt&quot;)
    query_parser.add_argument(&quot;--model&quot;, &quot;-m&quot;, default=&quot;gpt-3.5-turbo&quot;, help=&quot;LLM model to use&quot;)
    query_parser.add_argument(&quot;--output&quot;, &quot;-o&quot;, help=&quot;Save output to file&quot;)
    query_parser.add_argument(&quot;--temperature&quot;, &quot;-t&quot;, type=float, default=0, help=&quot;Temperature setting&quot;)

    # Test prompt variations command
    test_parser = subparsers.add_parser(&quot;test-variations&quot;, help=&quot;Test different prompt variations&quot;)
    test_parser.add_argument(&quot;--variations-file&quot;, required=True, help=&quot;JSON file with prompt variations&quot;)
    test_parser.add_argument(&quot;--model&quot;, &quot;-m&quot;, default=&quot;gpt-3.5-turbo&quot;, help=&quot;LLM model to use&quot;)
    test_parser.add_argument(&quot;--output-dir&quot;, &quot;-o&quot;, default=&quot;./results&quot;, help=&quot;Directory to save results&quot;)

    # Batch processing command
    batch_parser = subparsers.add_parser(&quot;batch&quot;, help=&quot;Process a batch of prompts&quot;)
    batch_parser.add_argument(&quot;--batch-file&quot;, required=True, help=&quot;JSON file with prompts to process&quot;)
    batch_parser.add_argument(&quot;--model&quot;, &quot;-m&quot;, default=&quot;gpt-3.5-turbo&quot;, help=&quot;LLM model to use&quot;)
    batch_parser.add_argument(&quot;--output-dir&quot;, &quot;-o&quot;, default=&quot;./results&quot;, help=&quot;Directory to save results&quot;)

    return parser

def main():
    parser = setup_argparser()
    args = parser.parse_args()

    if args.command is None:
        parser.print_help()
        return

    # Initialize OpenAI API (assuming OPENAI_API_KEY environment variable is set)
    if not openai.api_key:
        print(&quot;Error: OpenAI API key not found. Set the OPENAI_API_KEY environment variable.&quot;)
        sys.exit(1)

    if args.command == &quot;query&quot;:
        handle_query_command(args)
    elif args.command == &quot;test-variations&quot;:
        handle_test_variations_command(args)
    elif args.command == &quot;batch&quot;:
        handle_batch_command(args)

def handle_query_command(args):
    # Get prompt from arguments or file
    if args.prompt:
        prompt = args.prompt
    elif args.prompt_file:
        with open(args.prompt_file, 'r') as f:
            prompt = f.read()
    else:
        print(&quot;Error: Either --prompt or --prompt-file must be specified&quot;)
        sys.exit(1)

    # Query the LLM
    response = openai.ChatCompletion.create(
        model=args.model,
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
        temperature=args.temperature
    )

    # Process the response
    content = response.choices[0].message.content

    # Output handling
    if args.output:
        with open(args.output, 'w') as f:
            f.write(content)
        print(f&quot;Response saved to {args.output}&quot;)
    else:
        print(&quot;\n--- Response ---\n&quot;)
        print(content)
        print(&quot;\n---------------\n&quot;)

    # Print usage statistics
    print(f&quot;Token usage: {response.usage.total_tokens} tokens&quot;)
    print(f&quot;  - Prompt: {response.usage.prompt_tokens} tokens&quot;)
    print(f&quot;  - Completion: {response.usage.completion_tokens} tokens&quot;)

# Additional handler functions omitted for brevity

if __name__ == &quot;__main__&quot;:
    main()
</code></pre>

<h3 id="642-integrating-with-vs-code-extensions">6.4.2 Integrating with VS Code Extensions</h3>
<p>For VS Code integration, consider creating a simple extension that enables developers to interact with LLMs directly from their editor.</p>
<p>Key features might include:
- Prompt templates accessible via snippets
- Highlighted code selection to LLM processing
- Preview window for LLM responses
- Context-aware suggestions based on the current file</p>
<h2 id="chapter6_section5">6.5 Testing Frameworks for LLM-Powered Features</h2>
<h3 id="651-unit-testing-llm-prompts">6.5.1 Unit Testing LLM Prompts</h3>
<p>Creating a testing framework for prompts:</p>
<pre class="codehilite"><code class="language-python">import unittest
from unittest.mock import patch
import json

class PromptTest(unittest.TestCase):
    &quot;&quot;&quot;Base class for testing prompt templates and their expected responses&quot;&quot;&quot;

    def setUp(self):
        # Set up mock for OpenAI API
        self.openai_patcher = patch('openai.ChatCompletion.create')
        self.mock_openai = self.openai_patcher.start()

    def tearDown(self):
        self.openai_patcher.stop()

    def assert_prompt_contains(self, prompt, required_elements):
        &quot;&quot;&quot;Assert that a prompt contains all required elements&quot;&quot;&quot;
        for element in required_elements:
            self.assertIn(element, prompt, f&quot;Prompt should contain '{element}'&quot;)

    def assert_prompt_format(self, prompt, expected_format):
        &quot;&quot;&quot;Assert that a prompt follows the expected format structure&quot;&quot;&quot;
        # This is a simplified check - real implementation would be more sophisticated
        sections = expected_format.split(&quot;[section]&quot;)
        last_pos = 0

        for section in sections[1:]:  # Skip the first empty section
            section = section.strip()
            pos = prompt.find(section, last_pos)
            self.assertGreater(pos, -1, f&quot;Prompt missing expected section: '{section}'&quot;)
            last_pos = pos + len(section)

    def mock_llm_response(self, response_content, usage=None):
        &quot;&quot;&quot;Helper to set up a mock LLM response&quot;&quot;&quot;
        if usage is None:
            usage = {&quot;prompt_tokens&quot;: 10, &quot;completion_tokens&quot;: 20, &quot;total_tokens&quot;: 30}

        # Create a response object structure similar to OpenAI's
        response = type('obj', (object,), {
            'choices': [
                type('obj', (object,), {
                    'message': type('obj', (object,), {'content': response_content}),
                    'finish_reason': 'stop'
                })
            ],
            'usage': type('obj', (object,), usage),
            'model': 'gpt-3.5-turbo'
        })

        self.mock_openai.return_value = response

# Example test class
class TestCodeGenerationPrompts(PromptTest):
    def test_python_function_prompt(self):
        from my_prompt_lib import get_function_generation_prompt

        # Test specific prompt generation
        prompt = get_function_generation_prompt(
            language=&quot;python&quot;,
            function_name=&quot;calculate_discount&quot;,
            description=&quot;Calculate the final price after applying a discount percentage&quot;,
            parameters=[&quot;price&quot;, &quot;discount_percentage&quot;]
        )

        # Verify prompt structure
        self.assert_prompt_contains(prompt, [&quot;python&quot;, &quot;calculate_discount&quot;, &quot;price&quot;, &quot;discount_percentage&quot;])
        self.assert_prompt_format(prompt, &quot;[section]Task[section]Parameters[section]Requirements&quot;)

        # Mock the LLM response
        expected_code = &quot;def calculate_discount(price, discount_percentage):\n    return price * (1 - discount_percentage / 100)&quot;
        self.mock_llm_response(expected_code)

        # Test the full flow from prompt to response
        from my_llm_service import generate_code
        response = generate_code(prompt)

        # Verify the response handling
        self.assertEqual(response, expected_code)
</code></pre>

<h3 id="652-integration-testing-for-llm-applications">6.5.2 Integration Testing for LLM Applications</h3>
<p>For integration tests:</p>
<pre class="codehilite"><code class="language-python">class LLMIntegrationTest(unittest.TestCase):
    &quot;&quot;&quot;Base class for integration testing of LLM-powered features&quot;&quot;&quot;

    def setUp(self):
        # Real API calls but with a special test API key
        # Could use a staging/test environment for the API
        import os
        os.environ[&quot;OPENAI_API_KEY&quot;] = os.environ.get(&quot;OPENAI_TEST_API_KEY&quot;)

        # Set lower temperature for more consistent results in tests
        self.default_test_params = {
            &quot;temperature&quot;: 0.0,
            &quot;max_tokens&quot;: 100  # Limit tokens for faster tests
        }

    def assert_response_matches_criteria(self, response, criteria):
        &quot;&quot;&quot;Assert that an LLM response meets a set of criteria&quot;&quot;&quot;
        for criterion, expected in criteria.items():
            if criterion == &quot;contains&quot;:
                for phrase in expected:
                    self.assertIn(phrase, response, f&quot;Response should contain '{phrase}'&quot;)
            elif criterion == &quot;excludes&quot;:
                for phrase in expected:
                    self.assertNotIn(phrase, response, f&quot;Response should not contain '{phrase}'&quot;)
            elif criterion == &quot;length_range&quot;:
                min_len, max_len = expected
                self.assertTrue(min_len &lt;= len(response) &lt;= max_len, 
                               f&quot;Response length {len(response)} outside range {min_len}-{max_len}&quot;)
            # Add more criteria types as needed
</code></pre>

<h2 id="chapter6_section6">6.6 Cost Optimization Techniques</h2>
<h3 id="661-token-counting-and-budget-management">6.6.1 Token Counting and Budget Management</h3>
<p>Implement a token budget manager:</p>
<pre class="codehilite"><code class="language-python">import tiktoken

class TokenBudgetManager:
    &quot;&quot;&quot;Manages token usage and budgets for LLM applications&quot;&quot;&quot;

    def __init__(self, model_name=&quot;gpt-3.5-turbo&quot;, monthly_budget=None):
        self.model_name = model_name
        self.monthly_budget = monthly_budget
        self.encoding = tiktoken.encoding_for_model(model_name)

        # Cost per 1K tokens (adjust based on current pricing)
        self.cost_per_1k_tokens = {
            &quot;gpt-3.5-turbo&quot;: {&quot;input&quot;: 0.0015, &quot;output&quot;: 0.002},
            &quot;gpt-4&quot;: {&quot;input&quot;: 0.03, &quot;output&quot;: 0.06}
        }.get(model_name, {&quot;input&quot;: 0.0015, &quot;output&quot;: 0.002})

        self.current_usage = {
            &quot;input_tokens&quot;: 0,
            &quot;output_tokens&quot;: 0,
            &quot;total_cost&quot;: 0.0
        }

    def count_tokens(self, text):
        &quot;&quot;&quot;Count the number of tokens in a text string&quot;&quot;&quot;
        if not text:
            return 0
        token_ids = self.encoding.encode(text)
        return len(token_ids)

    def estimate_cost(self, input_text, estimated_output_length=None):
        &quot;&quot;&quot;Estimate the cost of an LLM request&quot;&quot;&quot;
        input_tokens = self.count_tokens(input_text)

        # If output length not provided, estimate based on input length
        if estimated_output_length is None:
            estimated_output_tokens = input_tokens * 1.5  # Simple heuristic
        else:
            estimated_output_tokens = self.count_tokens(estimated_output_length)

        input_cost = (input_tokens / 1000) * self.cost_per_1k_tokens[&quot;input&quot;]
        output_cost = (estimated_output_tokens / 1000) * self.cost_per_1k_tokens[&quot;output&quot;]

        return {
            &quot;input_tokens&quot;: input_tokens,
            &quot;estimated_output_tokens&quot;: estimated_output_tokens,
            &quot;input_cost&quot;: input_cost,
            &quot;estimated_output_cost&quot;: output_cost,
            &quot;total_estimated_cost&quot;: input_cost + output_cost
        }

    def track_usage(self, input_text, output_text):
        &quot;&quot;&quot;Track actual token usage and cost&quot;&quot;&quot;
        input_tokens = self.count_tokens(input_text)
        output_tokens = self.count_tokens(output_text)

        input_cost = (input_tokens / 1000) * self.cost_per_1k_tokens[&quot;input&quot;]
        output_cost = (output_tokens / 1000) * self.cost_per_1k_tokens[&quot;output&quot;]
        total_cost = input_cost + output_cost

        # Update running totals
        self.current_usage[&quot;input_tokens&quot;] += input_tokens
        self.current_usage[&quot;output_tokens&quot;] += output_tokens
        self.current_usage[&quot;total_cost&quot;] += total_cost

        return {
            &quot;input_tokens&quot;: input_tokens,
            &quot;output_tokens&quot;: output_tokens,
            &quot;input_cost&quot;: input_cost,
            &quot;output_cost&quot;: output_cost,
            &quot;total_cost&quot;: total_cost,
            &quot;running_totals&quot;: self.current_usage.copy()
        }

    def check_budget_status(self):
        &quot;&quot;&quot;Check status against monthly budget&quot;&quot;&quot;
        if self.monthly_budget is None:
            return {&quot;has_budget&quot;: False}

        remaining_budget = self.monthly_budget - self.current_usage[&quot;total_cost&quot;]
        usage_percentage = (self.current_usage[&quot;total_cost&quot;] / self.monthly_budget) * 100

        return {
            &quot;has_budget&quot;: True,
            &quot;monthly_budget&quot;: self.monthly_budget,
            &quot;current_usage&quot;: self.current_usage[&quot;total_cost&quot;],
            &quot;remaining_budget&quot;: remaining_budget,
            &quot;usage_percentage&quot;: usage_percentage,
            &quot;status&quot;: &quot;OK&quot; if usage_percentage &lt; 90 else &quot;WARNING&quot; if usage_percentage &lt; 100 else &quot;EXCEEDED&quot;
        }
</code></pre>

<h3 id="662-implementing-smart-caching">6.6.2 Implementing Smart Caching</h3>
<p>We've already covered a basic caching implementation earlier. Here's an enhancement with smarter invalidation strategies:</p>
<pre class="codehilite"><code class="language-python">class SemanticCache:
    &quot;&quot;&quot;A cache that uses semantic similarity to match similar prompts&quot;&quot;&quot;

    def __init__(self, embedding_model=&quot;text-embedding-ada-002&quot;, similarity_threshold=0.95):
        self.cache = {}  # Maps embedding hash to (response, original_prompt)
        self.embedding_model = embedding_model
        self.similarity_threshold = similarity_threshold

    def _get_embedding(self, text):
        &quot;&quot;&quot;Get embedding vector for text&quot;&quot;&quot;
        response = openai.Embedding.create(
            model=self.embedding_model,
            input=text
        )
        return response[&quot;data&quot;][0][&quot;embedding&quot;]

    def _compute_similarity(self, embedding1, embedding2):
        &quot;&quot;&quot;Compute cosine similarity between embeddings&quot;&quot;&quot;
        import numpy as np

        # Convert to numpy arrays for vector operations
        vec1 = np.array(embedding1)
        vec2 = np.array(embedding2)

        # Compute cosine similarity
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)

        return dot_product / (norm1 * norm2)

    def get(self, prompt, model):
        &quot;&quot;&quot;Try to retrieve a cached response based on semantic similarity&quot;&quot;&quot;
        try:
            prompt_embedding = self._get_embedding(prompt)

            best_match = None
            highest_similarity = 0

            for cache_key, (cached_response, original_prompt, cached_model) in self.cache.items():
                # Skip if models don't match
                if model != cached_model:
                    continue

                # Calculate similarity with the cached prompt
                similarity = self._compute_similarity(prompt_embedding, cache_key)

                if similarity &gt; highest_similarity:
                    highest_similarity = similarity
                    best_match = (cached_response, original_prompt, similarity)

            # Return the best match if it meets the threshold
            if best_match and highest_similarity &gt;= self.similarity_threshold:
                return {&quot;response&quot;: best_match[0], 
                        &quot;original_prompt&quot;: best_match[1],
                        &quot;similarity&quot;: highest_similarity}

            return None

        except Exception as e:
            print(f&quot;Error in semantic cache: {e}&quot;)
            return None

    def set(self, prompt, response, model):
        &quot;&quot;&quot;Store a response in the cache&quot;&quot;&quot;
        try:
            prompt_embedding = self._get_embedding(prompt)
            # Use the embedding vector as key
            embedding_key = tuple(prompt_embedding)  # Convert to tuple so it's hashable
            self.cache[embedding_key] = (response, prompt, model)
        except Exception as e:
            print(f&quot;Error setting semantic cache: {e}&quot;)
</code></pre>

<h2 id="chapter6_section7">6.7 Conclusion</h2>
<p>In this chapter, we've explored various tools and techniques for building effective developer tooling for LLM applications. From prompt libraries and reuse patterns to debugging tools, performance optimization, testing frameworks, and cost management, we've covered the essential components needed to develop robust LLM-powered applications.</p>
<p>As you implement these tools in your projects, remember that the field of prompt engineering is rapidly evolving. Stay flexible and be prepared to adapt your tools and approaches as new best practices emerge. In the next chapter, we'll put these tools into practice with a hands-on project building a Smart Code Assistant.</p>
<h2 id="chapter6_section8">6.8 Further Reading</h2>
<ul>
<li>"Design Patterns for LLM Applications" by Various Authors</li>
<li>"Efficient Natural Language Processing" by Various Authors</li>
<li>"Software Engineering for AI-Powered Systems" by Various Authors</li>
<li>OpenAI API Documentation</li>
<li>LangChain and LlamaIndex Documentation for advanced tooling options</li>
</ul></div><div class="navigation"><a href="#chapter5" class="prev-chapter">Previous Chapter</a><a href="#chapter7" class="next-chapter">Next Chapter</a></div><div class="chapter" id="chapter-content-7"><h1 id="chapter7">Chapter 7: Hands-on Project 1: Building a Smart Code Assistant</h1>
<p>In the previous chapters, we've explored various prompt engineering techniques, patterns, and tools for developing LLM-powered applications. Now it's time to put that knowledge into practice by building a practical tool that can help streamline your daily coding tasks. In this chapter, we'll create a Smart Code Assistant that leverages LLMs to automate common coding tasks.</p>
<h2 id="chapter7_section1">7.1 Project Overview</h2>
<h3 id="711-problem-statement">7.1.1 Problem Statement</h3>
<p>Software development involves numerous repetitive tasks that consume valuable time and mental energy:</p>
<ul>
<li>Writing boilerplate code for new classes, functions, or modules</li>
<li>Creating comprehensive documentation for existing code</li>
<li>Refactoring code for better readability or performance</li>
<li>Understanding unfamiliar code or complex algorithms</li>
<li>Writing unit tests for existing code</li>
<li>Converting code between different programming languages</li>
</ul>
<p>While integrated development environments (IDEs) offer some assistance, they often lack the flexibility and contextual understanding that LLMs can provide.</p>
<h3 id="712-solution-the-smart-code-assistant">7.1.2 Solution: The Smart Code Assistant</h3>
<p>We'll build a Python-based Smart Code Assistant that leverages the power of LLMs to:</p>
<ol>
<li>Generate boilerplate code based on natural language specifications</li>
<li>Analyze and explain existing code</li>
<li>Suggest refactoring improvements for small functions</li>
<li>Generate unit tests for given functions</li>
<li>Assist with code documentation</li>
<li>Provide language conversion between Python, JavaScript, and Java</li>
</ol>
<h3 id="713-technical-requirements">7.1.3 Technical Requirements</h3>
<ul>
<li>Python 3.8+ environment</li>
<li>OpenAI API key (or equivalent for another LLM provider)</li>
<li>Command-line interface for easy integration with existing workflows</li>
<li>Simple, modular architecture for future extensions</li>
<li>Option to save results to files or copy to clipboard</li>
</ul>
<h2 id="chapter7_section2">7.2 Setting Up the Project</h2>
<p>Let's begin by setting up our project structure and installing the necessary dependencies.</p>
<h3 id="721-project-structure">7.2.1 Project Structure</h3>
<pre class="codehilite"><code>smart_code_assistant/
├── __init__.py
├── main.py              # Entry point for the command-line interface
├── code_assistant.py    # Core functionality
├── prompt_library.py    # Prompt templates
├── utils/
│   ├── __init__.py
│   ├── clipboard.py     # Clipboard utilities
│   ├── file_utils.py    # File handling utilities
│   └── token_counter.py # Token counting utilities
├── config.py            # Configuration handling
├── requirements.txt     # Project dependencies
└── README.md            # Project documentation
</code></pre>

<h3 id="722-installing-dependencies">7.2.2 Installing Dependencies</h3>
<p>Create a <code>requirements.txt</code> file with the following dependencies:</p>
<pre class="codehilite"><code>openai&gt;=1.0.0
typer&gt;=0.9.0
rich&gt;=13.5.0
pyperclip&gt;=1.8.2
tiktoken&gt;=0.5.0
python-dotenv&gt;=1.0.0
</code></pre>

<p>Install these dependencies using pip:</p>
<pre class="codehilite"><code class="language-bash">pip install -r requirements.txt
</code></pre>

<h3 id="723-configuration-setup">7.2.3 Configuration Setup</h3>
<p>Create a <code>config.py</code> file to handle API keys and other settings:</p>
<pre class="codehilite"><code class="language-python">import os
import dotenv
from pathlib import Path

# Load environment variables from .env file
dotenv.load_dotenv()

# API configuration
OPENAI_API_KEY = os.getenv(&quot;OPENAI_API_KEY&quot;)
DEFAULT_MODEL = os.getenv(&quot;DEFAULT_MODEL&quot;, &quot;gpt-3.5-turbo&quot;)
MAX_TOKENS = int(os.getenv(&quot;MAX_TOKENS&quot;, &quot;2048&quot;))
TEMPERATURE = float(os.getenv(&quot;TEMPERATURE&quot;, &quot;0.7&quot;))

# Application paths
APP_DIR = Path.home() / &quot;.smart_code_assistant&quot;
CACHE_DIR = APP_DIR / &quot;cache&quot;
OUTPUT_DIR = APP_DIR / &quot;output&quot;

# Ensure directories exist
APP_DIR.mkdir(exist_ok=True)
CACHE_DIR.mkdir(exist_ok=True)
OUTPUT_DIR.mkdir(exist_ok=True)

# Default languages supported
SUPPORTED_LANGUAGES = [
    &quot;python&quot;, &quot;javascript&quot;, &quot;typescript&quot;, &quot;java&quot;, &quot;c++&quot;, &quot;csharp&quot;, &quot;go&quot;, &quot;rust&quot;
]
</code></pre>

<p>Create a <code>.env</code> file in the project root to store your OpenAI API key:</p>
<pre class="codehilite"><code>OPENAI_API_KEY=your_api_key_here
DEFAULT_MODEL=gpt-3.5-turbo
</code></pre>

<h2 id="chapter7_section3">7.3 Building the Core Components</h2>
<h3 id="731-prompt-library">7.3.1 Prompt Library</h3>
<p>First, let's create our prompt library with templates for different coding tasks. Create a <code>prompt_library.py</code> file:</p>
<pre class="codehilite"><code class="language-python">class PromptTemplate:
    def __init__(self, template, required_params=None):
        self.template = template
        self.required_params = required_params or []

    def format(self, **kwargs):
        # Ensure all required parameters are provided
        missing = [param for param in self.required_params if param not in kwargs]
        if missing:
            raise ValueError(f&quot;Missing required parameters: {', '.join(missing)}&quot;)

        # Format the template with the provided parameters
        return self.template.format(**kwargs)


class CodePromptLibrary:
    def __init__(self):
        self.prompts = {}
        self._initialize_prompts()

    def _initialize_prompts(self):
        # Code generation prompts
        self.prompts[&quot;generate_function&quot;] = PromptTemplate(
            &quot;&quot;&quot;You are an expert software developer. Write a {language} function that {description}.

Requirements:
{requirements}

Your function should be well-documented with comments explaining the logic.
Only return the code with no additional explanations.
&quot;&quot;&quot;,
            [&quot;language&quot;, &quot;description&quot;, &quot;requirements&quot;]
        )

        # Code explanation prompts
        self.prompts[&quot;explain_code&quot;] = PromptTemplate(
            &quot;&quot;&quot;Explain the following {language} code in detail:

```{language}
{code}
</code></pre>

<p>Include in your explanation:
1. What the code does
2. The key components and their purpose
3. Any algorithms or patterns used
4. Potential edge cases or limitations
""",
            ["language", "code"]
        )</p>
<pre class="codehilite"><code>    # Refactoring prompts
    self.prompts[&quot;refactor_code&quot;] = PromptTemplate(
        &quot;&quot;&quot;Refactor the following {language} code to improve its {focus}:
</code></pre>

<pre class="codehilite"><code>{code}
</code></pre>

<p>Provide the refactored code and explain what improvements you made.
Focus specifically on improving {focus} while maintaining the same functionality.
""",
            ["language", "code", "focus"]
        )</p>
<pre class="codehilite"><code>    # Unit test generation prompts
    self.prompts[&quot;generate_tests&quot;] = PromptTemplate(
        &quot;&quot;&quot;Write comprehensive unit tests for the following {language} function:
</code></pre>

<pre class="codehilite"><code>{code}
</code></pre>

<p>The tests should:
1. Cover normal cases, edge cases, and potential errors
2. Be well-structured and properly named
3. Use {test_framework} as the testing framework
4. Include comments explaining the purpose of each test case
""",
            ["language", "code", "test_framework"]
        )</p>
<pre class="codehilite"><code>    # Documentation generation prompts
    self.prompts[&quot;generate_docs&quot;] = PromptTemplate(
        &quot;&quot;&quot;Generate comprehensive documentation for the following {language} code:
</code></pre>

<pre class="codehilite"><code>{code}
</code></pre>

<p>The documentation should:
1. Follow {doc_style} documentation style
2. Include parameter descriptions, return values, and exceptions
3. Provide a clear overview of what the code does and how to use it
4. Include usage examples where appropriate
""",
            ["language", "code", "doc_style"]
        )</p>
<pre class="codehilite"><code>    # Code conversion prompts
    self.prompts[&quot;convert_code&quot;] = PromptTemplate(
        &quot;&quot;&quot;Convert the following {source_language} code to {target_language} while maintaining the same functionality:
</code></pre>

<pre class="codehilite"><code>{code}
</code></pre>

<p>Ensure the converted code:
1. Follows the idiomatic conventions of {target_language}
2. Preserves the original functionality and logic
3. Includes equivalent error handling
4. Is well-commented to explain any non-trivial conversions
""",
            ["source_language", "target_language", "code"]
        )</p>
<pre class="codehilite"><code>def get_prompt(self, prompt_name, **kwargs):
    &quot;&quot;&quot;Get a formatted prompt by name with the provided parameters&quot;&quot;&quot;
    if prompt_name not in self.prompts:
        raise ValueError(f&quot;Unknown prompt: {prompt_name}&quot;)

    return self.prompts[prompt_name].format(**kwargs)
</code></pre>

<pre class="codehilite"><code>### 7.3.2 Core Code Assistant Implementation

Now, let's create the core `code_assistant.py` file that will handle interactions with the LLM:

```python
import openai
import tiktoken
import json
import time
from pathlib import Path

import config
from prompt_library import CodePromptLibrary

class SmartCodeAssistant:
    def __init__(self, api_key=None, model=None):
        &quot;&quot;&quot;Initialize the Smart Code Assistant&quot;&quot;&quot;
        self.api_key = api_key or config.OPENAI_API_KEY
        self.model = model or config.DEFAULT_MODEL
        self.prompt_library = CodePromptLibrary()

        # Configure OpenAI client
        openai.api_key = self.api_key

        # Initialize tokenizer for token counting
        self.tokenizer = tiktoken.encoding_for_model(self.model)

    def _send_request(self, prompt, temperature=None, max_tokens=None):
        &quot;&quot;&quot;Send a request to the OpenAI API&quot;&quot;&quot;
        temperature = temperature if temperature is not None else config.TEMPERATURE
        max_tokens = max_tokens if max_tokens is not None else config.MAX_TOKENS

        try:
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
                temperature=temperature,
                max_tokens=max_tokens
            )
            return response.choices[0].message.content
        except Exception as e:
            raise Exception(f&quot;Error calling OpenAI API: {str(e)}&quot;)

    def count_tokens(self, text):
        &quot;&quot;&quot;Count the number of tokens in the given text&quot;&quot;&quot;
        return len(self.tokenizer.encode(text))

    def generate_function(self, description, language=&quot;python&quot;, requirements=&quot;&quot;):
        &quot;&quot;&quot;Generate code based on a natural language description&quot;&quot;&quot;
        prompt = self.prompt_library.get_prompt(
            &quot;generate_function&quot;,
            language=language,
            description=description,
            requirements=requirements
        )
        return self._send_request(prompt, temperature=0.2)

    def explain_code(self, code, language=&quot;python&quot;):
        &quot;&quot;&quot;Explain the given code in detail&quot;&quot;&quot;
        prompt = self.prompt_library.get_prompt(
            &quot;explain_code&quot;,
            language=language,
            code=code
        )
        return self._send_request(prompt)

    def refactor_code(self, code, focus=&quot;readability&quot;, language=&quot;python&quot;):
        &quot;&quot;&quot;Refactor code to improve a specific aspect&quot;&quot;&quot;
        prompt = self.prompt_library.get_prompt(
            &quot;refactor_code&quot;,
            language=language,
            code=code,
            focus=focus
        )
        return self._send_request(prompt)

    def generate_tests(self, code, language=&quot;python&quot;, test_framework=&quot;pytest&quot;):
        &quot;&quot;&quot;Generate unit tests for the given code&quot;&quot;&quot;
        prompt = self.prompt_library.get_prompt(
            &quot;generate_tests&quot;,
            language=language,
            code=code,
            test_framework=test_framework
        )
        return self._send_request(prompt)

    def generate_docs(self, code, language=&quot;python&quot;, doc_style=&quot;Google&quot;):
        &quot;&quot;&quot;Generate documentation for the given code&quot;&quot;&quot;
        prompt = self.prompt_library.get_prompt(
            &quot;generate_docs&quot;,
            language=language,
            code=code,
            doc_style=doc_style
        )
        return self._send_request(prompt)

    def convert_code(self, code, source_language=&quot;python&quot;, target_language=&quot;javascript&quot;):
        &quot;&quot;&quot;Convert code from one language to another&quot;&quot;&quot;
        prompt = self.prompt_library.get_prompt(
            &quot;convert_code&quot;,
            source_language=source_language,
            target_language=target_language,
            code=code
        )
        return self._send_request(prompt)
</code></pre>

<h3 id="733-utility-functions">7.3.3 Utility Functions</h3>
<p>Create a utility module for file operations and clipboard interaction. First, create the <code>utils/file_utils.py</code>:</p>
<pre class="codehilite"><code class="language-python">import os
from pathlib import Path

def read_file(file_path):
    &quot;&quot;&quot;Read content from a file&quot;&quot;&quot;
    path = Path(file_path)
    if not path.exists():
        raise FileNotFoundError(f&quot;File not found: {file_path}&quot;)

    with open(path, 'r', encoding='utf-8') as file:
        return file.read()

def write_file(file_path, content):
    &quot;&quot;&quot;Write content to a file&quot;&quot;&quot;
    path = Path(file_path)

    # Create directories if they don't exist
    path.parent.mkdir(parents=True, exist_ok=True)

    with open(path, 'w', encoding='utf-8') as file:
        file.write(content)

    return path

def get_language_from_extension(file_path):
    &quot;&quot;&quot;Determine language from file extension&quot;&quot;&quot;
    extension_map = {
        '.py': 'python',
        '.js': 'javascript',
        '.ts': 'typescript',
        '.java': 'java',
        '.cpp': 'c++',
        '.cc': 'c++',
        '.c': 'c',
        '.h': 'c',
        '.hpp': 'c++',
        '.cs': 'csharp',
        '.go': 'go',
        '.rs': 'rust',
        '.rb': 'ruby',
        '.php': 'php',
        '.swift': 'swift',
        '.kt': 'kotlin'
    }

    extension = Path(file_path).suffix.lower()
    return extension_map.get(extension, 'text')
</code></pre>

<p>Now, create <code>utils/clipboard.py</code>:</p>
<pre class="codehilite"><code class="language-python">import pyperclip

def copy_to_clipboard(text):
    &quot;&quot;&quot;Copy text to clipboard&quot;&quot;&quot;
    try:
        pyperclip.copy(text)
        return True
    except Exception as e:
        print(f&quot;Failed to copy to clipboard: {e}&quot;)
        return False

def paste_from_clipboard():
    &quot;&quot;&quot;Paste text from clipboard&quot;&quot;&quot;
    try:
        return pyperclip.paste()
    except Exception as e:
        print(f&quot;Failed to paste from clipboard: {e}&quot;)
        return &quot;&quot;
</code></pre>

<p>Create <code>utils/token_counter.py</code>:</p>
<pre class="codehilite"><code class="language-python">import tiktoken
import config

def count_tokens(text, model=None):
    &quot;&quot;&quot;Count tokens in the given text&quot;&quot;&quot;
    model = model or config.DEFAULT_MODEL

    try:
        encoding = tiktoken.encoding_for_model(model)
        return len(encoding.encode(text))
    except Exception as e:
        # Fallback to approximate token count if encoding fails
        return len(text) // 4  # Rough approximation: ~4 characters per token
</code></pre>

<p>Create an empty <code>__init__.py</code> in the utils directory to make it a proper package:</p>
<pre class="codehilite"><code class="language-python"># This file makes the utils directory a Python package
</code></pre>

<h2 id="chapter7_section4">7.4 Building the Command-Line Interface</h2>
<p>Let's create a command-line interface using Typer to make our tool easily accessible. Create the <code>main.py</code> file:</p>
<pre class="codehilite"><code class="language-python">import typer
import sys
from pathlib import Path
from typing import Optional, List
from rich.console import Console
from rich.syntax import Syntax

import config
from code_assistant import SmartCodeAssistant
from utils.file_utils import read_file, write_file, get_language_from_extension
from utils.clipboard import copy_to_clipboard, paste_from_clipboard
from utils.token_counter import count_tokens

# Initialize Typer app and Rich console
app = typer.Typer(help=&quot;Smart Code Assistant - Your AI-powered coding companion&quot;)
console = Console()

# Initialize code assistant
assistant = SmartCodeAssistant()

def print_code(code, language):
    &quot;&quot;&quot;Print code with syntax highlighting&quot;&quot;&quot;
    syntax = Syntax(code, language, theme=&quot;monokai&quot;, line_numbers=True)
    console.print(syntax)

@app.command(&quot;generate&quot;)
def generate_function(
    description: str = typer.Argument(..., help=&quot;Description of the function to generate&quot;),
    language: str = typer.Option(&quot;python&quot;, &quot;--language&quot;, &quot;-l&quot;, help=&quot;Programming language&quot;),
    requirements: str = typer.Option(&quot;&quot;, &quot;--requirements&quot;, &quot;-r&quot;, help=&quot;Additional requirements&quot;),
    output_file: Optional[Path] = typer.Option(None, &quot;--output&quot;, &quot;-o&quot;, help=&quot;Output file path&quot;),
    copy: bool = typer.Option(False, &quot;--copy&quot;, &quot;-c&quot;, help=&quot;Copy result to clipboard&quot;)
):
    &quot;&quot;&quot;Generate code based on a natural language description&quot;&quot;&quot;
    console.print(f&quot;[bold blue]Generating {language} code for:[/bold blue] {description}&quot;)

    try:
        result = assistant.generate_function(description, language, requirements)

        # Print the result
        print_code(result, language)

        # Save to file if requested
        if output_file:
            write_file(output_file, result)
            console.print(f&quot;[green]Code saved to:[/green] {output_file}&quot;)

        # Copy to clipboard if requested
        if copy:
            copy_to_clipboard(result)
            console.print(&quot;[green]Code copied to clipboard![/green]&quot;)

    except Exception as e:
        console.print(f&quot;[bold red]Error:[/bold red] {str(e)}&quot;)
        raise typer.Exit(code=1)

@app.command(&quot;explain&quot;)
def explain_code(
    file: Optional[Path] = typer.Option(None, &quot;--file&quot;, &quot;-f&quot;, help=&quot;File containing code to explain&quot;),
    language: Optional[str] = typer.Option(None, &quot;--language&quot;, &quot;-l&quot;, help=&quot;Programming language&quot;),
    from_clipboard: bool = typer.Option(False, &quot;--clipboard&quot;, &quot;-c&quot;, help=&quot;Read code from clipboard&quot;),
    output_file: Optional[Path] = typer.Option(None, &quot;--output&quot;, &quot;-o&quot;, help=&quot;Output file path&quot;)
):
    &quot;&quot;&quot;Explain code in detail&quot;&quot;&quot;
    # Get the code from file or clipboard
    if file:
        code = read_file(file)
        language = language or get_language_from_extension(file)
    elif from_clipboard:
        code = paste_from_clipboard()
        if not code:
            console.print(&quot;[bold red]No code found in clipboard![/bold red]&quot;)
            raise typer.Exit(code=1)
    else:
        # Interactive mode - read from stdin
        console.print(&quot;[bold blue]Enter code to explain (Ctrl+D to finish):[/bold blue]&quot;)
        code = sys.stdin.read().strip()
        if not code:
            console.print(&quot;[bold red]No code provided![/bold red]&quot;)
            raise typer.Exit(code=1)

    language = language or &quot;python&quot;  # Default to Python if not specified

    console.print(f&quot;[bold blue]Explaining {language} code...[/bold blue]&quot;)

    try:
        explanation = assistant.explain_code(code, language)

        # Print the explanation
        console.print(&quot;[bold green]Explanation:[/bold green]&quot;)
        console.print(explanation)

        # Save to file if requested
        if output_file:
            write_file(output_file, explanation)
            console.print(f&quot;[green]Explanation saved to:[/green] {output_file}&quot;)

    except Exception as e:
        console.print(f&quot;[bold red]Error:[/bold red] {str(e)}&quot;)
        raise typer.Exit(code=1)

@app.command(&quot;refactor&quot;)
def refactor_code(
    file: Optional[Path] = typer.Option(None, &quot;--file&quot;, &quot;-f&quot;, help=&quot;File containing code to refactor&quot;),
    language: Optional[str] = typer.Option(None, &quot;--language&quot;, &quot;-l&quot;, help=&quot;Programming language&quot;),
    focus: str = typer.Option(&quot;readability&quot;, &quot;--focus&quot;, help=&quot;What to focus on improving (e.g., readability, performance)&quot;),
    from_clipboard: bool = typer.Option(False, &quot;--clipboard&quot;, &quot;-c&quot;, help=&quot;Read code from clipboard&quot;),
    output_file: Optional[Path] = typer.Option(None, &quot;--output&quot;, &quot;-o&quot;, help=&quot;Output file path&quot;),
    copy: bool = typer.Option(False, &quot;--copy&quot;, help=&quot;Copy result to clipboard&quot;)
):
    &quot;&quot;&quot;Refactor code to improve a specific aspect&quot;&quot;&quot;
    # Get the code from file or clipboard
    if file:
        code = read_file(file)
        language = language or get_language_from_extension(file)
    elif from_clipboard:
        code = paste_from_clipboard()
        if not code:
            console.print(&quot;[bold red]No code found in clipboard![/bold red]&quot;)
            raise typer.Exit(code=1)
    else:
        # Interactive mode - read from stdin
        console.print(f&quot;[bold blue]Enter code to refactor (focus: {focus}, Ctrl+D to finish):[/bold blue]&quot;)
        code = sys.stdin.read().strip()
        if not code:
            console.print(&quot;[bold red]No code provided![/bold red]&quot;)
            raise typer.Exit(code=1)

    language = language or &quot;python&quot;  # Default to Python if not specified

    console.print(f&quot;[bold blue]Refactoring {language} code to improve {focus}...[/bold blue]&quot;)

    try:
        refactored = assistant.refactor_code(code, focus, language)

        # Print the refactored code
        console.print(&quot;[bold green]Refactored code:[/bold green]&quot;)
        print_code(refactored, language)

        # Save to file if requested
        if output_file:
            write_file(output_file, refactored)
            console.print(f&quot;[green]Refactored code saved to:[/green] {output_file}&quot;)

        # Copy to clipboard if requested
        if copy:
            copy_to_clipboard(refactored)
            console.print(&quot;[green]Refactored code copied to clipboard![/green]&quot;)

    except Exception as e:
        console.print(f&quot;[bold red]Error:[/bold red] {str(e)}&quot;)
        raise typer.Exit(code=1)

@app.command(&quot;test&quot;)
def generate_tests(
    file: Optional[Path] = typer.Option(None, &quot;--file&quot;, &quot;-f&quot;, help=&quot;File containing code to test&quot;),
    language: Optional[str] = typer.Option(None, &quot;--language&quot;, &quot;-l&quot;, help=&quot;Programming language&quot;),
    framework: str = typer.Option(&quot;pytest&quot;, &quot;--framework&quot;, help=&quot;Testing framework to use&quot;),
    from_clipboard: bool = typer.Option(False, &quot;--clipboard&quot;, &quot;-c&quot;, help=&quot;Read code from clipboard&quot;),
    output_file: Optional[Path] = typer.Option(None, &quot;--output&quot;, &quot;-o&quot;, help=&quot;Output file path&quot;)
):
    &quot;&quot;&quot;Generate unit tests for code&quot;&quot;&quot;
    # Get the code from file or clipboard
    if file:
        code = read_file(file)
        language = language or get_language_from_extension(file)
    elif from_clipboard:
        code = paste_from_clipboard()
        if not code:
            console.print(&quot;[bold red]No code found in clipboard![/bold red]&quot;)
            raise typer.Exit(code=1)
    else:
        # Interactive mode - read from stdin
        console.print(f&quot;[bold blue]Enter code to generate tests for (using {framework}, Ctrl+D to finish):[/bold blue]&quot;)
        code = sys.stdin.read().strip()
        if not code:
            console.print(&quot;[bold red]No code provided![/bold red]&quot;)
            raise typer.Exit(code=1)

    language = language or &quot;python&quot;  # Default to Python if not specified

    console.print(f&quot;[bold blue]Generating {framework} tests for {language} code...[/bold blue]&quot;)

    try:
        tests = assistant.generate_tests(code, language, framework)

        # Print the tests
        console.print(&quot;[bold green]Generated tests:[/bold green]&quot;)
        print_code(tests, language)

        # Save to file if requested
        if output_file:
            write_file(output_file, tests)
            console.print(f&quot;[green]Tests saved to:[/green] {output_file}&quot;)

    except Exception as e:
        console.print(f&quot;[bold red]Error:[/bold red] {str(e)}&quot;)
        raise typer.Exit(code=1)

@app.command(&quot;docs&quot;)
def generate_docs(
    file: Optional[Path] = typer.Option(None, &quot;--file&quot;, &quot;-f&quot;, help=&quot;File containing code to document&quot;),
    language: Optional[str] = typer.Option(None, &quot;--language&quot;, &quot;-l&quot;, help=&quot;Programming language&quot;),
    style: str = typer.Option(&quot;Google&quot;, &quot;--style&quot;, help=&quot;Documentation style (Google, NumPy, JSDoc, etc.)&quot;),
    from_clipboard: bool = typer.Option(False, &quot;--clipboard&quot;, &quot;-c&quot;, help=&quot;Read code from clipboard&quot;),
    output_file: Optional[Path] = typer.Option(None, &quot;--output&quot;, &quot;-o&quot;, help=&quot;Output file path&quot;)
):
    &quot;&quot;&quot;Generate documentation for code&quot;&quot;&quot;
    # Get the code from file or clipboard
    if file:
        code = read_file(file)
        language = language or get_language_from_extension(file)
    elif from_clipboard:
        code = paste_from_clipboard()
        if not code:
            console.print(&quot;[bold red]No code found in clipboard![/bold red]&quot;)
            raise typer.Exit(code=1)
    else:
        # Interactive mode - read from stdin
        console.print(f&quot;[bold blue]Enter code to document (using {style} style, Ctrl+D to finish):[/bold blue]&quot;)
        code = sys.stdin.read().strip()
        if not code:
            console.print(&quot;[bold red]No code provided![/bold red]&quot;)
            raise typer.Exit(code=1)

    language = language or &quot;python&quot;  # Default to Python if not specified

    console.print(f&quot;[bold blue]Generating {style} documentation for {language} code...[/bold blue]&quot;)

    try:
        docs = assistant.generate_docs(code, language, style)

        # Print the documentation
        console.print(&quot;[bold green]Generated documentation:[/bold green]&quot;)
        print_code(docs, language)

        # Save to file if requested
        if output_file:
            write_file(output_file, docs)
            console.print(f&quot;[green]Documentation saved to:[/green] {output_file}&quot;)

    except Exception as e:
        console.print(f&quot;[bold red]Error:[/bold red] {str(e)}&quot;)
        raise typer.Exit(code=1)

@app.command(&quot;convert&quot;)
def convert_code(
    source_language: str = typer.Option(..., &quot;--from&quot;, &quot;-f&quot;, help=&quot;Source programming language&quot;),
    target_language: str = typer.Option(..., &quot;--to&quot;, &quot;-t&quot;, help=&quot;Target programming language&quot;),
    file: Optional[Path] = typer.Option(None, &quot;--file&quot;, help=&quot;File containing code to convert&quot;),
    from_clipboard: bool = typer.Option(False, &quot;--clipboard&quot;, &quot;-c&quot;, help=&quot;Read code from clipboard&quot;),
    output_file: Optional[Path] = typer.Option(None, &quot;--output&quot;, &quot;-o&quot;, help=&quot;Output file path&quot;),
    copy: bool = typer.Option(False, &quot;--copy&quot;, help=&quot;Copy result to clipboard&quot;)
):
    &quot;&quot;&quot;Convert code from one language to another&quot;&quot;&quot;
    # Get the code from file or clipboard
    if file:
        code = read_file(file)
    elif from_clipboard:
        code = paste_from_clipboard()
        if not code:
            console.print(&quot;[bold red]No code found in clipboard![/bold red]&quot;)
            raise typer.Exit(code=1)
    else:
        # Interactive mode - read from stdin
        console.print(f&quot;[bold blue]Enter {source_language} code to convert to {target_language} (Ctrl+D to finish):[/bold blue]&quot;)
        code = sys.stdin.read().strip()
        if not code:
            console.print(&quot;[bold red]No code provided![/bold red]&quot;)
            raise typer.Exit(code=1)

    console.print(f&quot;[bold blue]Converting code from {source_language} to {target_language}...[/bold blue]&quot;)

    try:
        converted = assistant.convert_code(code, source_language, target_language)

        # Print the converted code
        console.print(&quot;[bold green]Converted code:[/bold green]&quot;)
        print_code(converted, target_language)

        # Save to file if requested
        if output_file:
            write_file(output_file, converted)
            console.print(f&quot;[green]Converted code saved to:[/green] {output_file}&quot;)

        # Copy to clipboard if requested
        if copy:
            copy_to_clipboard(converted)
            console.print(&quot;[green]Converted code copied to clipboard![/green]&quot;)

    except Exception as e:
        console.print(f&quot;[bold red]Error:[/bold red] {str(e)}&quot;)
        raise typer.Exit(code=1)

@app.command(&quot;info&quot;)
def show_info():
    &quot;&quot;&quot;Show information about the Smart Code Assistant&quot;&quot;&quot;
    console.print(&quot;[bold blue]Smart Code Assistant[/bold blue]&quot;)
    console.print(&quot;Your AI-powered coding companion&quot;)
    console.print(&quot;\n[bold green]Available commands:[/bold green]&quot;)
    console.print(&quot;  generate    - Generate code from a description&quot;)
    console.print(&quot;  explain     - Explain code in detail&quot;)
    console.print(&quot;  refactor    - Refactor code to improve specific aspects&quot;)
    console.print(&quot;  test        - Generate unit tests for code&quot;)
    console.print(&quot;  docs        - Generate documentation for code&quot;)
    console.print(&quot;  convert     - Convert code between languages&quot;)
    console.print(&quot;  info        - Show this information&quot;)

    console.print(&quot;\n[bold green]Configuration:[/bold green]&quot;)
    console.print(f&quot;  Model: {config.DEFAULT_MODEL}&quot;)
    console.print(f&quot;  Max tokens: {config.MAX_TOKENS}&quot;)
    console.print(f&quot;  Temperature: {config.TEMPERATURE}&quot;)
    console.print(f&quot;  Supported languages: {', '.join(config.SUPPORTED_LANGUAGES)}&quot;)

if __name__ == &quot;__main__&quot;:
    app()
</code></pre>

<h2 id="chapter7_section5">7.5 Project Usage Examples</h2>
<p>Let's explore how to use our Smart Code Assistant for various tasks.</p>
<h3 id="751-generate-a-function">7.5.1 Generate a Function</h3>
<pre class="codehilite"><code class="language-bash"># Generate a binary search function in Python
python main.py generate &quot;implement a binary search algorithm for a sorted list&quot; --language python --requirements &quot;Must handle edge cases like empty lists and include proper documentation&quot;
</code></pre>

<h3 id="752-explain-code">7.5.2 Explain Code</h3>
<pre class="codehilite"><code class="language-bash"># Explain code from a file
python main.py explain --file complex_algorithm.py

# Explain code from clipboard
python main.py explain --clipboard --language javascript
</code></pre>

<h3 id="753-refactor-code">7.5.3 Refactor Code</h3>
<pre class="codehilite"><code class="language-bash"># Refactor code from a file to improve performance
python main.py refactor --file slow_function.py --focus performance --output improved_function.py

# Refactor code from clipboard to improve readability
python main.py refactor --clipboard --focus readability --language python
</code></pre>

<h3 id="754-generate-tests">7.5.4 Generate Tests</h3>
<pre class="codehilite"><code class="language-bash"># Generate tests for a function in a file
python main.py test --file my_function.py --framework pytest --output test_my_function.py

# Generate tests for code in clipboard
python main.py test --clipboard --language javascript --framework jest
</code></pre>

<h3 id="755-generate-documentation">7.5.5 Generate Documentation</h3>
<pre class="codehilite"><code class="language-bash"># Generate documentation for a file
python main.py docs --file undocumented_code.py --style Google --output documented_code.py

# Generate documentation for code in clipboard
python main.py docs --clipboard --language typescript --style JSDoc
</code></pre>

<h3 id="756-convert-code-between-languages">7.5.6 Convert Code Between Languages</h3>
<pre class="codehilite"><code class="language-bash"># Convert Python code to JavaScript
python main.py convert --from python --to javascript --file algorithm.py --output algorithm.js

# Convert JavaScript code from clipboard to Python
python main.py convert --from javascript --to python --clipboard --copy
</code></pre>

<h2 id="chapter7_section6">7.6 Enhancing the Smart Code Assistant</h2>
<p>Now that we have the core functionality in place, let's explore some ways to enhance our tool.</p>
<h3 id="761-adding-a-simple-caching-mechanism">7.6.1 Adding a Simple Caching Mechanism</h3>
<p>To avoid unnecessary API calls and reduce costs, let's implement a simple caching mechanism:</p>
<pre class="codehilite"><code class="language-python"># Add to code_assistant.py

import hashlib
import json
import os
from pathlib import Path
import time

class SimpleCache:
    def __init__(self, cache_dir=None, ttl=3600):
        &quot;&quot;&quot;Initialize the cache with a directory and time-to-live in seconds&quot;&quot;&quot;
        self.cache_dir = Path(cache_dir or config.CACHE_DIR)
        self.ttl = ttl
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def _get_cache_key(self, prompt, model):
        &quot;&quot;&quot;Create a hash key from the prompt and model&quot;&quot;&quot;
        key_str = f&quot;{prompt}:{model}&quot;
        return hashlib.md5(key_str.encode()).hexdigest()

    def _get_cache_path(self, key):
        &quot;&quot;&quot;Get the file path for a cache key&quot;&quot;&quot;
        return self.cache_dir / f&quot;{key}.json&quot;

    def get(self, prompt, model):
        &quot;&quot;&quot;Get cached response if available and not expired&quot;&quot;&quot;
        key = self._get_cache_key(prompt, model)
        cache_path = self._get_cache_path(key)

        if not cache_path.exists():
            return None

        # Check if cache has expired
        if time.time() - cache_path.stat().st_mtime &gt; self.ttl:
            os.remove(cache_path)
            return None

        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
                return cache_data[&quot;response&quot;]
        except:
            return None

    def set(self, prompt, model, response):
        &quot;&quot;&quot;Cache a response&quot;&quot;&quot;
        key = self._get_cache_key(prompt, model)
        cache_path = self._get_cache_path(key)

        cache_data = {
            &quot;prompt&quot;: prompt,
            &quot;model&quot;: model,
            &quot;response&quot;: response,
            &quot;timestamp&quot;: time.time()
        }

        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
</code></pre>

<p>Update the <code>SmartCodeAssistant</code> class to use the cache:</p>
<pre class="codehilite"><code class="language-python"># Update _send_request method in code_assistant.py

def __init__(self, api_key=None, model=None, use_cache=True):
    # ... existing code ...
    self.use_cache = use_cache
    self.cache = SimpleCache() if use_cache else None

def _send_request(self, prompt, temperature=None, max_tokens=None):
    &quot;&quot;&quot;Send a request to the OpenAI API with caching&quot;&quot;&quot;
    temperature = temperature if temperature is not None else config.TEMPERATURE
    max_tokens = max_tokens if max_tokens is not None else config.MAX_TOKENS

    # Try to get from cache if enabled
    if self.use_cache:
        cached_response = self.cache.get(prompt, self.model)
        if cached_response:
            return cached_response

    try:
        response = openai.ChatCompletion.create(
            model=self.model,
            messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
            temperature=temperature,
            max_tokens=max_tokens
        )
        content = response.choices[0].message.content

        # Store in cache if enabled
        if self.use_cache:
            self.cache.set(prompt, self.model, content)

        return content
    except Exception as e:
        raise Exception(f&quot;Error calling OpenAI API: {str(e)}&quot;)
</code></pre>

<h3 id="762-adding-progressive-enhancement-with-file-context">7.6.2 Adding Progressive Enhancement with File Context</h3>
<p>Let's enhance our code assistant to consider surrounding file context when processing partial code:</p>
<pre class="codehilite"><code class="language-python"># Add to code_assistant.py

def extract_file_context(self, file_path, target_lines=None, context_lines=5):
    &quot;&quot;&quot;Extract context from a file around the target lines&quot;&quot;&quot;
    with open(file_path, 'r', encoding='utf-8') as f:
        all_lines = f.readlines()

    if target_lines is None:
        return &quot;&quot;.join(all_lines)

    # Convert target_lines to a range if it's a single number
    if isinstance(target_lines, int):
        target_start = max(0, target_lines - 1)
        target_end = target_start + 1
    else:
        target_start = max(0, target_lines[0] - 1)
        target_end = min(len(all_lines), target_lines[1])

    # Extract the target code
    target_code = &quot;&quot;.join(all_lines[target_start:target_end])

    # Get context before target
    context_before_start = max(0, target_start - context_lines)
    context_before = &quot;&quot;.join(all_lines[context_before_start:target_start])

    # Get context after target
    context_after_end = min(len(all_lines), target_end + context_lines)
    context_after = &quot;&quot;.join(all_lines[target_end:context_after_end])

    # Compile everything with markers
    result = &quot;&quot;
    if context_before:
        result += &quot;/* Context before target code */\n&quot; + context_before

    result += &quot;/* Target code */\n&quot; + target_code

    if context_after:
        result += &quot;/* Context after target code */\n&quot; + context_after

    return result

def refactor_with_context(self, code, file_path, target_lines, focus=&quot;readability&quot;, language=None):
    &quot;&quot;&quot;Refactor code with surrounding file context&quot;&quot;&quot;
    if not language:
        language = get_language_from_extension(file_path)

    # Extract code with context
    code_with_context = self.extract_file_context(file_path, target_lines, context_lines=5)

    prompt = self.prompt_library.get_prompt(
        &quot;refactor_with_context&quot;,
        language=language,
        code=code_with_context,
        focus=focus
    )

    return self._send_request(prompt)
</code></pre>

<p>Add the new prompt template to the <code>PromptLibrary</code>:</p>
<pre class="codehilite"><code class="language-python"># Add to _initialize_prompts in prompt_library.py

self.prompts[&quot;refactor_with_context&quot;] = PromptTemplate(
    &quot;&quot;&quot;Refactor the target code in the following {language} code to improve its {focus}.
The file contains context before and after the target code to help you understand its purpose.
Only modify the code between the &quot;/* Target code */&quot; markers.

```{language}
{code}
</code></pre>

<p>Provide ONLY the refactored target code portion and explain what improvements you made.
The surrounding context is for reference only and should not be included in your response.
Focus specifically on improving {focus} while maintaining the same functionality.
""",
    ["language", "code", "focus"]
)</p>
<pre class="codehilite"><code>### 7.6.3 Adding a Project-Level Assistant

Let's extend our code assistant to understand project-level context:

```python
# Add to code_assistant.py

def analyze_project_structure(self, project_dir, max_files=20, file_extensions=None):
    &quot;&quot;&quot;Analyze project structure to provide context for code generation&quot;&quot;&quot;
    project_path = Path(project_dir)
    if not project_path.exists() or not project_path.is_dir():
        raise ValueError(f&quot;Invalid project directory: {project_dir}&quot;)

    # Default file extensions to analyze
    if file_extensions is None:
        file_extensions = [&quot;.py&quot;, &quot;.js&quot;, &quot;.ts&quot;, &quot;.java&quot;, &quot;.c&quot;, &quot;.cpp&quot;, &quot;.h&quot;, &quot;.hpp&quot;]

    # Find relevant files
    all_files = []
    for ext in file_extensions:
        all_files.extend(project_path.glob(f&quot;**/*{ext}&quot;))

    # Limit the number of files to analyze
    all_files = all_files[:max_files]

    # Extract file names and structures
    project_structure = {
        &quot;project_name&quot;: project_path.name,
        &quot;files&quot;: [],
        &quot;imports&quot;: [],
        &quot;classes&quot;: [],
        &quot;functions&quot;: []
    }

    for file_path in all_files:
        rel_path = file_path.relative_to(project_path)

        try:
            content = read_file(file_path)

            # Extract high-level info from the file
            file_info = {
                &quot;path&quot;: str(rel_path),
                &quot;extension&quot;: file_path.suffix,
                &quot;size_bytes&quot;: file_path.stat().st_size
            }

            project_structure[&quot;files&quot;].append(file_info)

            # Very simple extraction of Python imports, classes, and functions
            # In a real implementation, use AST parsing or other proper code analysis
            if file_path.suffix == &quot;.py&quot;:
                # Simple regex-based extraction
                import re

                # Find imports
                imports = re.findall(r'^import\s+(.+?)$|^from\s+(.+?)\s+import', content, re.MULTILINE)
                for imp in imports:
                    imp_name = imp[0] or imp[1]
                    if imp_name:
                        project_structure[&quot;imports&quot;].append(imp_name)

                # Find classes
                classes = re.findall(r'^class\s+([A-Za-z0-9_]+)', content, re.MULTILINE)
                for cls in classes:
                    project_structure[&quot;classes&quot;].append({
                        &quot;name&quot;: cls,
                        &quot;file&quot;: str(rel_path)
                    })

                # Find functions
                functions = re.findall(r'^def\s+([A-Za-z0-9_]+)', content, re.MULTILINE)
                for func in functions:
                    project_structure[&quot;functions&quot;].append({
                        &quot;name&quot;: func,
                        &quot;file&quot;: str(rel_path)
                    })

        except Exception as e:
            print(f&quot;Error analyzing file {rel_path}: {e}&quot;)

    return project_structure

def generate_code_with_project_context(self, description, project_dir, language=None):
    &quot;&quot;&quot;Generate code with project context&quot;&quot;&quot;
    try:
        # Analyze project structure
        project_structure = self.analyze_project_structure(project_dir)

        # Determine language from project if not specified
        if language is None:
            # Simple heuristic: use most common language in project
            extensions = [f[&quot;extension&quot;] for f in project_structure[&quot;files&quot;]]
            if extensions:
                from collections import Counter
                most_common_ext = Counter(extensions).most_common(1)[0][0]
                language = get_language_from_extension(f&quot;file{most_common_ext}&quot;)
            else:
                language = &quot;python&quot;  # Default

        # Create prompt with project context
        prompt = f&quot;&quot;&quot;You are an expert software developer. 
I want you to generate {language} code based on the following description:

{description}

The code will be part of an existing project with the following structure:
Project name: {project_structure['project_name']}
Files: {', '.join(f['path'] for f in project_structure['files'][:10])}

Key classes in the project: {', '.join(cls['name'] for cls in project_structure['classes'][:10])}
Key functions in the project: {', '.join(func['name'] for func in project_structure['functions'][:10])}
Common imports: {', '.join(project_structure['imports'][:10])}

Generate code that follows the style and conventions of this existing project.
Only return the code with minimal explanatory comments.
&quot;&quot;&quot;

        return self._send_request(prompt, temperature=0.2)

    except Exception as e:
        raise Exception(f&quot;Error generating code with project context: {str(e)}&quot;)
</code></pre>

<h2 id="chapter7_section7">7.7 Practical Use Cases</h2>
<p>Here are some practical use cases for our Smart Code Assistant:</p>
<h3 id="771-automating-repetitive-coding-tasks">7.7.1 Automating Repetitive Coding Tasks</h3>
<p><strong>Task</strong>: Creating REST API endpoint handlers</p>
<pre class="codehilite"><code class="language-bash">python main.py generate &quot;create a Flask REST API endpoint for user registration that validates email, username, and password&quot; --language python --requirements &quot;Must include input validation, error handling, and follow RESTful principles&quot;
</code></pre>

<p><strong>Task</strong>: Generating database models</p>
<pre class="codehilite"><code class="language-bash">python main.py generate &quot;create a SQLAlchemy model for a blog post with title, content, author, publication date, and tags&quot; --language python
</code></pre>

<h3 id="772-understanding-legacy-code">7.7.2 Understanding Legacy Code</h3>
<p><strong>Task</strong>: Explaining complex algorithms</p>
<pre class="codehilite"><code class="language-bash">python main.py explain --file legacy_algorithm.py
</code></pre>

<p><strong>Task</strong>: Documenting undocumented functions</p>
<pre class="codehilite"><code class="language-bash">python main.py docs --file undocumented_module.py --style Google
</code></pre>

<h3 id="773-improving-code-quality">7.7.3 Improving Code Quality</h3>
<p><strong>Task</strong>: Refactoring for performance</p>
<pre class="codehilite"><code class="language-bash">python main.py refactor --file slow_function.py --focus performance
</code></pre>

<p><strong>Task</strong>: Creating unit tests for existing code</p>
<pre class="codehilite"><code class="language-bash">python main.py test --file data_processor.py --framework pytest
</code></pre>

<h3 id="774-cross-language-development">7.7.4 Cross-Language Development</h3>
<p><strong>Task</strong>: Converting Python utility to JavaScript</p>
<pre class="codehilite"><code class="language-bash">python main.py convert --from python --to javascript --file utils.py --output utils.js
</code></pre>

<h2 id="chapter7_section8">7.8 Best Practices and Limitations</h2>
<h3 id="781-best-practices">7.8.1 Best Practices</h3>
<ol>
<li>
<p><strong>Always review the generated code</strong>: While LLMs can provide good starting points, always review the code for correctness, security issues, and alignment with your needs.</p>
</li>
<li>
<p><strong>Break down complex tasks</strong>: For better results, break complex coding tasks into smaller, more manageable pieces.</p>
</li>
<li>
<p><strong>Provide clear requirements</strong>: The more specific your descriptions and requirements are, the better the generated code will be.</p>
</li>
<li>
<p><strong>Use project context</strong>: When working on existing projects, providing project-level context will help generate more consistent and compatible code.</p>
</li>
<li>
<p><strong>Cache responses</strong>: To reduce API costs and improve response times, implement caching for frequently requested tasks.</p>
</li>
</ol>
<h3 id="782-limitations">7.8.2 Limitations</h3>
<ol>
<li>
<p><strong>Code accuracy</strong>: LLMs may generate code with logical errors or incorrect implementations, especially for complex algorithms.</p>
</li>
<li>
<p><strong>Security considerations</strong>: Generated code might contain security vulnerabilities, so always review it carefully.</p>
</li>
<li>
<p><strong>Context limits</strong>: LLMs have context window limitations, so they might struggle with understanding very large codebases or files.</p>
</li>
<li>
<p><strong>Language limitations</strong>: Performance varies across programming languages, with better results typically for popular languages like Python and JavaScript.</p>
</li>
<li>
<p><strong>API costs</strong>: Extensive use of LLMs can incur significant API costs, so monitor usage carefully.</p>
</li>
</ol>
<h2 id="chapter7_section9">7.9 Future Enhancements</h2>
<p>Our Smart Code Assistant is just the beginning. Here are some potential future enhancements:</p>
<ol>
<li>
<p><strong>IDE integration</strong>: Develop plugins for popular IDEs like VS Code, PyCharm, and IntelliJ.</p>
</li>
<li>
<p><strong>More advanced project understanding</strong>: Implement deeper static analysis of project structures and coding patterns.</p>
</li>
<li>
<p><strong>Code review capabilities</strong>: Add features to review code changes and suggest improvements.</p>
</li>
<li>
<p><strong>Custom fine-tuning</strong>: Train models on specific codebases to better match company coding styles and patterns.</p>
</li>
<li>
<p><strong>Collaborative features</strong>: Allow teams to share and rate prompt templates and responses.</p>
</li>
<li>
<p><strong>Version control integration</strong>: Integrate with Git to understand code history and changes over time.</p>
</li>
</ol>
<h2 id="chapter7_section10">7.10 Conclusion</h2>
<p>In this chapter, we've built a practical Smart Code Assistant that demonstrates how prompt engineering can be applied to solve real-world coding challenges. By leveraging LLMs, we've created a tool that can generate code, explain existing code, refactor for improvements, create tests, and assist with documentation.</p>
<p>The key takeaway is that effective prompt engineering allows us to guide LLMs to produce valuable coding assistance. By structuring prompts with clear instructions, relevant context, and specific requirements, we can obtain high-quality results across a range of coding tasks.</p>
<p>As you continue your prompt engineering journey, consider how you might extend and customize this tool for your specific development needs. The techniques and patterns demonstrated here can be applied to a wide range of software development tasks beyond what we've covered.</p>
<p>In the next chapter, we'll build on these skills to create another practical application: an LLM-powered ML Model Explainer and Debugger.</p></div><div class="navigation"><a href="#chapter6" class="prev-chapter">Previous Chapter</a><a href="#chapter8" class="next-chapter">Next Chapter</a></div><div class="chapter" id="chapter-content-8"><h1 id="chapter8">Chapter 8: Hands-on Project 2: LLM-Powered ML Model Explainer</h1>
<p>In this chapter, we'll build on the prompt engineering skills we've developed to create a sophisticated tool that addresses one of the most challenging aspects of machine learning: model interpretability. We'll develop an LLM-powered ML Model Explainer that can analyze, interpret, and explain complex machine learning models in accessible language.</p>
<h2 id="chapter8_section1">8.1 Project Overview</h2>
<h3 id="811-the-problem-ml-model-black-boxes">8.1.1 The Problem: ML Model Black Boxes</h3>
<p>Machine learning models, especially deep learning networks, are often criticized as "black boxes" due to their complexity and lack of interpretability. This presents several challenges:</p>
<p><strong>For Data Scientists and ML Engineers:</strong>
- Difficulty understanding why a model makes specific predictions
- Challenges in debugging model performance issues
- Inability to explain model behavior to stakeholders
- Struggles with model validation and trust</p>
<p><strong>For Business Stakeholders:</strong>
- Lack of confidence in automated decision-making
- Regulatory compliance requirements for explainable AI
- Need to understand model limitations and appropriate use cases
- Difficulty in communicating AI capabilities to customers</p>
<p><strong>For Developers Integrating ML Models:</strong>
- Uncertainty about when models might fail
- Challenges in debugging production issues
- Difficulty in setting appropriate confidence thresholds
- Need to understand model requirements and constraints</p>
<h3 id="812-solution-llm-powered-model-explainer">8.1.2 Solution: LLM-Powered Model Explainer</h3>
<p>We'll create a comprehensive tool that leverages LLMs to:</p>
<ol>
<li><strong>Analyze Model Architecture</strong>: Break down complex model structures into understandable components</li>
<li><strong>Explain Hyperparameters</strong>: Interpret the significance of various hyperparameter choices</li>
<li><strong>Describe Training Approaches</strong>: Explain the training methodology and its implications</li>
<li><strong>Interpret Model Behavior</strong>: Analyze predictions and feature importance</li>
<li><strong>Provide Usage Guidance</strong>: Offer recommendations for appropriate model deployment</li>
<li><strong>Generate Documentation</strong>: Create comprehensive model documentation automatically</li>
</ol>
<h3 id="813-technical-scope">8.1.3 Technical Scope</h3>
<p>Our ML Model Explainer will support:
- <strong>Frameworks</strong>: TensorFlow/Keras, PyTorch, Scikit-learn
- <strong>Model Types</strong>: Neural networks (CNN, RNN, LSTM, Transformer), tree-based models, linear models
- <strong>Analysis Types</strong>: Architecture explanation, hyperparameter interpretation, performance analysis
- <strong>Output Formats</strong>: Interactive reports, markdown documentation, API responses</p>
<h2 id="chapter8_section2">8.2 Setting Up the Project</h2>
<h3 id="821-project-structure">8.2.1 Project Structure</h3>
<pre class="codehilite"><code>ml_model_explainer/
├── __init__.py
├── main.py                 # CLI interface
├── explainer/
│   ├── __init__.py
│   ├── core.py            # Core explanation engine
│   ├── model_analyzers/   # Model-specific analyzers
│   │   ├── __init__.py
│   │   ├── base.py        # Base analyzer class
│   │   ├── keras_analyzer.py
│   │   ├── pytorch_analyzer.py
│   │   └── sklearn_analyzer.py
│   ├── prompt_templates.py # Specialized ML prompts
│   └── report_generator.py # Report generation
├── utils/
│   ├── __init__.py
│   ├── model_utils.py     # Model loading utilities
│   ├── visualization.py   # Visualization helpers
│   └── export_utils.py    # Export functionality
├── config.py
├── requirements.txt
└── examples/              # Example models and notebooks
    ├── sample_models/
    └── notebooks/
</code></pre>

<h3 id="822-dependencies">8.2.2 Dependencies</h3>
<p>Create <code>requirements.txt</code>:</p>
<pre class="codehilite"><code>openai&gt;=1.0.0
tensorflow&gt;=2.12.0
torch&gt;=2.0.0
scikit-learn&gt;=1.3.0
pandas&gt;=2.0.0
numpy&gt;=1.24.0
matplotlib&gt;=3.7.0
seaborn&gt;=0.12.0
plotly&gt;=5.15.0
typer&gt;=0.9.0
rich&gt;=13.5.0
jinja2&gt;=3.1.0
python-dotenv&gt;=1.0.0
tiktoken&gt;=0.5.0
</code></pre>

<h3 id="823-configuration">8.2.3 Configuration</h3>
<p>Create <code>config.py</code>:</p>
<pre class="codehilite"><code class="language-python">import os
from pathlib import Path
import dotenv

# Load environment variables
dotenv.load_dotenv()

# API Configuration
OPENAI_API_KEY = os.getenv(&quot;OPENAI_API_KEY&quot;)
DEFAULT_MODEL = os.getenv(&quot;DEFAULT_MODEL&quot;, &quot;gpt-4&quot;)
TEMPERATURE = float(os.getenv(&quot;TEMPERATURE&quot;, &quot;0.3&quot;))
MAX_TOKENS = int(os.getenv(&quot;MAX_TOKENS&quot;, &quot;3000&quot;))

# Application paths
APP_DIR = Path.home() / &quot;.ml_model_explainer&quot;
CACHE_DIR = APP_DIR / &quot;cache&quot;
REPORTS_DIR = APP_DIR / &quot;reports&quot;
MODELS_DIR = APP_DIR / &quot;models&quot;

# Ensure directories exist
for directory in [APP_DIR, CACHE_DIR, REPORTS_DIR, MODELS_DIR]:
    directory.mkdir(exist_ok=True)

# Supported frameworks
SUPPORTED_FRAMEWORKS = [&quot;tensorflow&quot;, &quot;pytorch&quot;, &quot;sklearn&quot;]

# Model type categories
MODEL_CATEGORIES = {
    &quot;neural_networks&quot;: [&quot;Sequential&quot;, &quot;Model&quot;, &quot;CNN&quot;, &quot;RNN&quot;, &quot;LSTM&quot;, &quot;GRU&quot;, &quot;Transformer&quot;],
    &quot;tree_based&quot;: [&quot;RandomForest&quot;, &quot;GradientBoosting&quot;, &quot;XGBoost&quot;, &quot;LightGBM&quot;],
    &quot;linear&quot;: [&quot;LinearRegression&quot;, &quot;LogisticRegression&quot;, &quot;SVM&quot;, &quot;Ridge&quot;, &quot;Lasso&quot;],
    &quot;clustering&quot;: [&quot;KMeans&quot;, &quot;DBSCAN&quot;, &quot;HierarchicalClustering&quot;],
    &quot;ensemble&quot;: [&quot;VotingClassifier&quot;, &quot;BaggingClassifier&quot;, &quot;AdaBoost&quot;]
}

# Default visualization settings
VIZ_SETTINGS = {
    &quot;figure_size&quot;: (12, 8),
    &quot;dpi&quot;: 300,
    &quot;style&quot;: &quot;seaborn-v0_8&quot;,
    &quot;color_palette&quot;: &quot;husl&quot;
}
</code></pre>

<h2 id="chapter8_section3">8.3 Building the Core Components</h2>
<h3 id="831-specialized-ml-prompt-templates">8.3.1 Specialized ML Prompt Templates</h3>
<p>Create <code>explainer/prompt_templates.py</code>:</p>
<pre class="codehilite"><code class="language-python">class MLPromptTemplates:
    def __init__(self):
        self.templates = self._initialize_templates()

    def _initialize_templates(self):
        return {
            &quot;model_architecture_analysis&quot;: &quot;&quot;&quot;
You are an expert machine learning engineer and educator. Analyze the following model architecture and provide a comprehensive explanation.

Model Information:
Framework: {framework}
Model Type: {model_type}
Architecture Summary:
{architecture_summary}

Layer Details:
{layer_details}

Please provide:
1. **High-Level Overview**: What type of model this is and its primary purpose
2. **Architecture Breakdown**: Explain each component and its role
3. **Design Rationale**: Why this architecture is suitable for the intended task
4. **Strengths and Limitations**: What this model does well and where it might struggle
5. **Computational Complexity**: Discuss the model's resource requirements

Use clear, educational language that would be accessible to both technical and non-technical stakeholders.
&quot;&quot;&quot;,

            &quot;hyperparameter_explanation&quot;: &quot;&quot;&quot;
You are an ML expert explaining model hyperparameters to a team that includes both technical and business stakeholders.

Model: {model_type}
Hyperparameters:
{hyperparameters}

Training Configuration:
{training_config}

For each hyperparameter, explain:
1. **What it controls**: The aspect of model behavior it influences
2. **Current value significance**: Why this specific value was chosen
3. **Impact of changes**: How increasing/decreasing would affect the model
4. **Tuning considerations**: Guidelines for optimization

Conclude with an overall assessment of the hyperparameter choices and their implications for model performance and behavior.
&quot;&quot;&quot;,

            &quot;training_methodology_analysis&quot;: &quot;&quot;&quot;
You are an experienced ML practitioner explaining the training approach for a machine learning model.

Training Details:
Model Type: {model_type}
Training Method: {training_method}
Dataset Information: {dataset_info}
Training Configuration: {training_config}
Performance Metrics: {performance_metrics}

Please explain:
1. **Training Approach**: The methodology used and why it's appropriate
2. **Data Preparation**: How the data was processed for training
3. **Optimization Strategy**: The learning approach and convergence strategy
4. **Validation Method**: How model performance was evaluated during training
5. **Performance Interpretation**: What the metrics tell us about model quality
6. **Potential Issues**: Any concerns or limitations evident from the training process

Provide actionable insights about the model's reliability and expected performance.
&quot;&quot;&quot;,

            &quot;prediction_explanation&quot;: &quot;&quot;&quot;
You are an AI explainability expert helping users understand a specific model prediction.

Model Information:
Type: {model_type}
Task: {task_type}
Input Features: {input_features}
Prediction: {prediction}
Confidence/Probability: {confidence}

Feature Importance (if available):
{feature_importance}

Please provide:
1. **Prediction Summary**: What the model predicted and confidence level
2. **Key Drivers**: Which features most influenced this prediction
3. **Decision Logic**: The reasoning process the model likely followed
4. **Confidence Assessment**: How reliable this prediction appears to be
5. **Alternative Scenarios**: How changing key inputs might affect the outcome
6. **Limitations**: What this prediction doesn't tell us

Make the explanation accessible to non-technical users while maintaining accuracy.
&quot;&quot;&quot;,

            &quot;model_comparison&quot;: &quot;&quot;&quot;
You are an ML consultant comparing different model approaches for a specific problem.

Models to Compare:
{model_details}

Comparison Criteria:
- Performance metrics
- Interpretability
- Computational requirements
- Robustness
- Maintenance complexity

For each model, analyze:
1. **Strengths**: What it does particularly well
2. **Weaknesses**: Where it struggles or has limitations
3. **Use Case Fit**: How well it matches the intended application
4. **Trade-offs**: What you sacrifice vs. what you gain

Conclude with a recommendation for which model to use in different scenarios.
&quot;&quot;&quot;,

            &quot;model_deployment_guidance&quot;: &quot;&quot;&quot;
You are a production ML expert providing deployment guidance for a trained model.

Model Details:
Type: {model_type}
Performance: {performance_summary}
Requirements: {requirements}
Constraints: {constraints}

Please provide guidance on:
1. **Deployment Readiness**: Is this model ready for production use?
2. **Infrastructure Requirements**: What resources and setup are needed?
3. **Monitoring Strategy**: What to track in production
4. **Risk Assessment**: Potential failure modes and mitigation strategies
5. **Maintenance Plan**: How to keep the model performing well over time
6. **Scaling Considerations**: How to handle increasing load or changing requirements

Include specific, actionable recommendations for successful deployment.
&quot;&quot;&quot;
        }

    def get_template(self, template_name):
        if template_name not in self.templates:
            raise ValueError(f&quot;Template '{template_name}' not found&quot;)
        return self.templates[template_name]

    def format_template(self, template_name, **kwargs):
        template = self.get_template(template_name)
        return template.format(**kwargs)
</code></pre>

<h3 id="832-base-model-analyzer">8.3.2 Base Model Analyzer</h3>
<p>Create <code>explainer/model_analyzers/base.py</code>:</p>
<pre class="codehilite"><code class="language-python">from abc import ABC, abstractmethod
import json
from pathlib import Path
from typing import Dict, Any, List, Optional

class BaseModelAnalyzer(ABC):
    &quot;&quot;&quot;Base class for model analyzers&quot;&quot;&quot;

    def __init__(self, model_path: Optional[str] = None):
        self.model_path = model_path
        self.model = None
        self.model_info = {}
        self.analysis_cache = {}

    @abstractmethod
    def load_model(self, model_path: str) -&gt; Any:
        &quot;&quot;&quot;Load the model from file&quot;&quot;&quot;
        pass

    @abstractmethod
    def extract_architecture(self) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Extract model architecture information&quot;&quot;&quot;
        pass

    @abstractmethod
    def extract_hyperparameters(self) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Extract model hyperparameters&quot;&quot;&quot;
        pass

    @abstractmethod
    def get_model_summary(self) -&gt; str:
        &quot;&quot;&quot;Get a string summary of the model&quot;&quot;&quot;
        pass

    @abstractmethod
    def predict_sample(self, sample_input: Any) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Make a prediction on sample input and return explanation data&quot;&quot;&quot;
        pass

    def analyze_model(self) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Perform comprehensive model analysis&quot;&quot;&quot;
        if not self.model:
            raise ValueError(&quot;Model not loaded. Call load_model() first.&quot;)

        analysis = {
            &quot;framework&quot;: self.get_framework_name(),
            &quot;model_type&quot;: self.get_model_type(),
            &quot;architecture&quot;: self.extract_architecture(),
            &quot;hyperparameters&quot;: self.extract_hyperparameters(),
            &quot;summary&quot;: self.get_model_summary(),
            &quot;complexity&quot;: self.analyze_complexity(),
            &quot;metadata&quot;: self.extract_metadata()
        }

        self.analysis_cache = analysis
        return analysis

    @abstractmethod
    def get_framework_name(self) -&gt; str:
        &quot;&quot;&quot;Return the ML framework name&quot;&quot;&quot;
        pass

    @abstractmethod
    def get_model_type(self) -&gt; str:
        &quot;&quot;&quot;Return the specific model type&quot;&quot;&quot;
        pass

    def analyze_complexity(self) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Analyze model computational complexity&quot;&quot;&quot;
        # Default implementation - can be overridden
        complexity = {
            &quot;estimated_parameters&quot;: &quot;Unknown&quot;,
            &quot;memory_usage&quot;: &quot;Unknown&quot;,
            &quot;inference_time&quot;: &quot;Unknown&quot;
        }
        return complexity

    def extract_metadata(self) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Extract additional metadata&quot;&quot;&quot;
        metadata = {
            &quot;model_path&quot;: self.model_path,
            &quot;analysis_timestamp&quot;: None,
            &quot;version_info&quot;: {}
        }
        return metadata

    def save_analysis(self, output_path: str):
        &quot;&quot;&quot;Save analysis results to file&quot;&quot;&quot;
        if not self.analysis_cache:
            raise ValueError(&quot;No analysis cached. Run analyze_model() first.&quot;)

        output_path = Path(output_path)
        with open(output_path, 'w') as f:
            json.dump(self.analysis_cache, f, indent=2, default=str)

    def load_analysis(self, analysis_path: str) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Load previously saved analysis&quot;&quot;&quot;
        with open(analysis_path, 'r') as f:
            self.analysis_cache = json.load(f)
        return self.analysis_cache
</code></pre>

<h3 id="833-kerastensorflow-analyzer">8.3.3 Keras/TensorFlow Analyzer</h3>
<p>Create <code>explainer/model_analyzers/keras_analyzer.py</code>:</p>
<pre class="codehilite"><code class="language-python">import tensorflow as tf
from tensorflow import keras
import numpy as np
from typing import Dict, Any, List, Optional
import json

from .base import BaseModelAnalyzer

class KerasModelAnalyzer(BaseModelAnalyzer):
    &quot;&quot;&quot;Analyzer for Keras/TensorFlow models&quot;&quot;&quot;

    def load_model(self, model_path: str):
        &quot;&quot;&quot;Load Keras model&quot;&quot;&quot;
        try:
            self.model = keras.models.load_model(model_path)
            self.model_path = model_path
            return self.model
        except Exception as e:
            raise ValueError(f&quot;Failed to load Keras model: {str(e)}&quot;)

    def extract_architecture(self) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Extract detailed architecture information&quot;&quot;&quot;
        if not self.model:
            raise ValueError(&quot;Model not loaded&quot;)

        architecture = {
            &quot;model_class&quot;: type(self.model).__name__,
            &quot;total_layers&quot;: len(self.model.layers),
            &quot;input_shape&quot;: self.model.input_shape if hasattr(self.model, 'input_shape') else None,
            &quot;output_shape&quot;: self.model.output_shape if hasattr(self.model, 'output_shape') else None,
            &quot;layers&quot;: []
        }

        # Extract layer information
        for i, layer in enumerate(self.model.layers):
            layer_info = {
                &quot;index&quot;: i,
                &quot;name&quot;: layer.name,
                &quot;class&quot;: type(layer).__name__,
                &quot;config&quot;: self._safe_config_extract(layer),
                &quot;input_shape&quot;: layer.input_shape if hasattr(layer, 'input_shape') else None,
                &quot;output_shape&quot;: layer.output_shape if hasattr(layer, 'output_shape') else None,
                &quot;trainable_params&quot;: layer.count_params() if hasattr(layer, 'count_params') else 0,
                &quot;activation&quot;: getattr(layer, 'activation', None)
            }

            # Add layer-specific information
            if hasattr(layer, 'units'):
                layer_info['units'] = layer.units
            if hasattr(layer, 'filters'):
                layer_info['filters'] = layer.filters
            if hasattr(layer, 'kernel_size'):
                layer_info['kernel_size'] = layer.kernel_size
            if hasattr(layer, 'strides'):
                layer_info['strides'] = layer.strides
            if hasattr(layer, 'dropout'):
                layer_info['dropout_rate'] = layer.rate

            architecture[&quot;layers&quot;].append(layer_info)

        return architecture

    def _safe_config_extract(self, layer) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Safely extract layer configuration&quot;&quot;&quot;
        try:
            config = layer.get_config()
            # Remove non-serializable items
            safe_config = {}
            for key, value in config.items():
                try:
                    json.dumps(value)  # Test if serializable
                    safe_config[key] = value
                except (TypeError, ValueError):
                    safe_config[key] = str(value)
            return safe_config
        except:
            return {&quot;error&quot;: &quot;Could not extract configuration&quot;}

    def extract_hyperparameters(self) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Extract model hyperparameters&quot;&quot;&quot;
        hyperparams = {
            &quot;optimizer&quot;: None,
            &quot;loss_function&quot;: None,
            &quot;metrics&quot;: [],
            &quot;total_parameters&quot;: self.model.count_params() if hasattr(self.model, 'count_params') else 0,
            &quot;trainable_parameters&quot;: sum([layer.count_params() for layer in self.model.layers if layer.trainable])
        }

        # Extract optimizer information if model is compiled
        if hasattr(self.model, 'optimizer') and self.model.optimizer:
            optimizer = self.model.optimizer
            hyperparams[&quot;optimizer&quot;] = {
                &quot;class&quot;: type(optimizer).__name__,
                &quot;learning_rate&quot;: float(optimizer.learning_rate) if hasattr(optimizer, 'learning_rate') else None,
                &quot;config&quot;: self._extract_optimizer_config(optimizer)
            }

        # Extract loss function
        if hasattr(self.model, 'compiled_loss') and self.model.compiled_loss:
            hyperparams[&quot;loss_function&quot;] = str(self.model.compiled_loss)

        # Extract metrics
        if hasattr(self.model, 'compiled_metrics') and self.model.compiled_metrics:
            hyperparams[&quot;metrics&quot;] = [str(metric) for metric in self.model.compiled_metrics.metrics]

        return hyperparams

    def _extract_optimizer_config(self, optimizer) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Extract optimizer configuration&quot;&quot;&quot;
        try:
            config = optimizer.get_config()
            safe_config = {}
            for key, value in config.items():
                try:
                    json.dumps(value)
                    safe_config[key] = value
                except (TypeError, ValueError):
                    safe_config[key] = str(value)
            return safe_config
        except:
            return {&quot;error&quot;: &quot;Could not extract optimizer config&quot;}

    def get_model_summary(self) -&gt; str:
        &quot;&quot;&quot;Get model summary as string&quot;&quot;&quot;
        if not self.model:
            raise ValueError(&quot;Model not loaded&quot;)

        # Capture model summary
        summary_lines = []
        self.model.summary(print_fn=lambda x: summary_lines.append(x))
        return '\n'.join(summary_lines)

    def predict_sample(self, sample_input: Any) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Make prediction and return explanation data&quot;&quot;&quot;
        if not self.model:
            raise ValueError(&quot;Model not loaded&quot;)

        # Ensure input is in correct format
        if not isinstance(sample_input, np.ndarray):
            sample_input = np.array(sample_input)

        if len(sample_input.shape) == len(self.model.input_shape) - 1:
            sample_input = np.expand_dims(sample_input, axis=0)

        # Make prediction
        prediction = self.model.predict(sample_input, verbose=0)

        prediction_info = {
            &quot;input_shape&quot;: sample_input.shape,
            &quot;output_shape&quot;: prediction.shape,
            &quot;prediction&quot;: prediction.tolist(),
            &quot;input_data&quot;: sample_input.tolist()
        }

        # Add confidence for classification tasks
        if len(prediction.shape) &gt; 1 and prediction.shape[1] &gt; 1:
            prediction_info[&quot;confidence&quot;] = float(np.max(prediction))
            prediction_info[&quot;predicted_class&quot;] = int(np.argmax(prediction))
            prediction_info[&quot;class_probabilities&quot;] = prediction[0].tolist()

        return prediction_info

    def get_framework_name(self) -&gt; str:
        return &quot;tensorflow&quot;

    def get_model_type(self) -&gt; str:
        if not self.model:
            return &quot;Unknown&quot;

        model_class = type(self.model).__name__

        # Determine model type based on architecture
        if &quot;Sequential&quot; in model_class:
            return &quot;Sequential Neural Network&quot;
        elif &quot;Model&quot; in model_class:
            return &quot;Functional Neural Network&quot;
        else:
            return f&quot;Custom {model_class}&quot;

    def analyze_complexity(self) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Analyze computational complexity&quot;&quot;&quot;
        if not self.model:
            return super().analyze_complexity()

        total_params = self.model.count_params()
        trainable_params = sum([layer.count_params() for layer in self.model.layers if layer.trainable])

        # Estimate memory usage (rough approximation)
        # Each parameter typically takes 4 bytes (float32)
        estimated_memory_mb = (total_params * 4) / (1024 * 1024)

        complexity = {
            &quot;total_parameters&quot;: total_params,
            &quot;trainable_parameters&quot;: trainable_params,
            &quot;non_trainable_parameters&quot;: total_params - trainable_params,
            &quot;estimated_memory_mb&quot;: round(estimated_memory_mb, 2),
            &quot;model_size_layers&quot;: len(self.model.layers),
            &quot;complexity_category&quot;: self._categorize_complexity(total_params)
        }

        return complexity

    def _categorize_complexity(self, param_count: int) -&gt; str:
        &quot;&quot;&quot;Categorize model complexity based on parameter count&quot;&quot;&quot;
        if param_count &lt; 1000:
            return &quot;Very Simple&quot;
        elif param_count &lt; 100000:
            return &quot;Simple&quot;
        elif param_count &lt; 1000000:
            return &quot;Moderate&quot;
        elif param_count &lt; 10000000:
            return &quot;Complex&quot;
        else:
            return &quot;Very Complex&quot;
</code></pre>

<h3 id="834-core-explainer-engine">8.3.4 Core Explainer Engine</h3>
<p>Create <code>explainer/core.py</code>:</p>
<pre class="codehilite"><code class="language-python">import openai
from typing import Dict, Any, Optional, List
import json
from pathlib import Path

import config
from .prompt_templates import MLPromptTemplates
from .model_analyzers.keras_analyzer import KerasModelAnalyzer
from .model_analyzers.pytorch_analyzer import PyTorchModelAnalyzer
from .model_analyzers.sklearn_analyzer import SklearnModelAnalyzer

class MLModelExplainer:
    &quot;&quot;&quot;Core explanation engine for ML models&quot;&quot;&quot;

    def __init__(self, api_key: Optional[str] = None, model: str = None):
        self.api_key = api_key or config.OPENAI_API_KEY
        self.model = model or config.DEFAULT_MODEL
        self.prompt_templates = MLPromptTemplates()

        # Configure OpenAI
        openai.api_key = self.api_key

        # Model analyzer mapping
        self.analyzers = {
            &quot;tensorflow&quot;: KerasModelAnalyzer,
            &quot;keras&quot;: KerasModelAnalyzer,
            &quot;pytorch&quot;: PyTorchModelAnalyzer,
            &quot;sklearn&quot;: SklearnModelAnalyzer
        }

        self.current_analyzer = None
        self.current_analysis = None

    def load_model(self, model_path: str, framework: str = None) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Load and analyze a model&quot;&quot;&quot;
        if framework is None:
            framework = self._detect_framework(model_path)

        if framework not in self.analyzers:
            raise ValueError(f&quot;Unsupported framework: {framework}&quot;)

        # Initialize appropriate analyzer
        self.current_analyzer = self.analyzers[framework](model_path)
        self.current_analyzer.load_model(model_path)

        # Perform initial analysis
        self.current_analysis = self.current_analyzer.analyze_model()

        return self.current_analysis

    def _detect_framework(self, model_path: str) -&gt; str:
        &quot;&quot;&quot;Detect ML framework from model file&quot;&quot;&quot;
        path = Path(model_path)

        # Check file extensions and patterns
        if path.suffix == '.h5' or 'keras' in str(path) or 'tensorflow' in str(path):
            return &quot;tensorflow&quot;
        elif path.suffix == '.pt' or path.suffix == '.pth' or 'pytorch' in str(path):
            return &quot;pytorch&quot;
        elif path.suffix == '.pkl' or path.suffix == '.joblib':
            return &quot;sklearn&quot;
        else:
            # Default to tensorflow if unsure
            return &quot;tensorflow&quot;

    def explain_architecture(self, detail_level: str = &quot;comprehensive&quot;) -&gt; str:
        &quot;&quot;&quot;Generate explanation of model architecture&quot;&quot;&quot;
        if not self.current_analysis:
            raise ValueError(&quot;No model analysis available. Load a model first.&quot;)

        # Prepare architecture details
        architecture = self.current_analysis[&quot;architecture&quot;]
        layer_details = self._format_layer_details(architecture[&quot;layers&quot;])

        prompt = self.prompt_templates.format_template(
            &quot;model_architecture_analysis&quot;,
            framework=self.current_analysis[&quot;framework&quot;],
            model_type=self.current_analysis[&quot;model_type&quot;],
            architecture_summary=json.dumps(architecture, indent=2),
            layer_details=layer_details
        )

        response = self._send_request(prompt)
        return response

    def explain_hyperparameters(self) -&gt; str:
        &quot;&quot;&quot;Generate explanation of model hyperparameters&quot;&quot;&quot;
        if not self.current_analysis:
            raise ValueError(&quot;No model analysis available. Load a model first.&quot;)

        hyperparams = self.current_analysis[&quot;hyperparameters&quot;]

        # Format training configuration
        training_config = {
            &quot;total_parameters&quot;: hyperparams.get(&quot;total_parameters&quot;, &quot;Unknown&quot;),
            &quot;trainable_parameters&quot;: hyperparams.get(&quot;trainable_parameters&quot;, &quot;Unknown&quot;),
            &quot;optimizer&quot;: hyperparams.get(&quot;optimizer&quot;, {}),
            &quot;loss_function&quot;: hyperparams.get(&quot;loss_function&quot;, &quot;Unknown&quot;)
        }

        prompt = self.prompt_templates.format_template(
            &quot;hyperparameter_explanation&quot;,
            model_type=self.current_analysis[&quot;model_type&quot;],
            hyperparameters=json.dumps(hyperparams, indent=2),
            training_config=json.dumps(training_config, indent=2)
        )

        response = self._send_request(prompt)
        return response

    def explain_prediction(self, sample_input: Any, include_feature_importance: bool = False) -&gt; str:
        &quot;&quot;&quot;Explain a specific prediction&quot;&quot;&quot;
        if not self.current_analyzer:
            raise ValueError(&quot;No model loaded. Load a model first.&quot;)

        # Get prediction details
        prediction_info = self.current_analyzer.predict_sample(sample_input)

        # Format input features (simplified)
        input_features = f&quot;Input shape: {prediction_info['input_shape']}&quot;

        # Feature importance placeholder (would need additional implementation)
        feature_importance = &quot;Feature importance analysis not yet implemented&quot;
        if include_feature_importance:
            # This would require additional analysis like SHAP, LIME, etc.
            pass

        prompt = self.prompt_templates.format_template(
            &quot;prediction_explanation&quot;,
            model_type=self.current_analysis[&quot;model_type&quot;],
            task_type=self._infer_task_type(),
            input_features=input_features,
            prediction=json.dumps(prediction_info[&quot;prediction&quot;]),
            confidence=prediction_info.get(&quot;confidence&quot;, &quot;N/A&quot;),
            feature_importance=feature_importance
        )

        response = self._send_request(prompt)
        return response

    def generate_deployment_guidance(self, target_environment: str = &quot;production&quot;) -&gt; str:
        &quot;&quot;&quot;Generate deployment guidance for the model&quot;&quot;&quot;
        if not self.current_analysis:
            raise ValueError(&quot;No model analysis available. Load a model first.&quot;)

        # Prepare performance summary
        complexity = self.current_analysis.get(&quot;complexity&quot;, {})
        performance_summary = {
            &quot;parameter_count&quot;: complexity.get(&quot;total_parameters&quot;, &quot;Unknown&quot;),
            &quot;memory_usage&quot;: complexity.get(&quot;estimated_memory_mb&quot;, &quot;Unknown&quot;),
            &quot;complexity_category&quot;: complexity.get(&quot;complexity_category&quot;, &quot;Unknown&quot;)
        }

        # Prepare requirements and constraints
        requirements = {
            &quot;framework&quot;: self.current_analysis[&quot;framework&quot;],
            &quot;model_type&quot;: self.current_analysis[&quot;model_type&quot;],
            &quot;memory_requirements&quot;: f&quot;{complexity.get('estimated_memory_mb', 'Unknown')} MB&quot;
        }

        constraints = {
            &quot;environment&quot;: target_environment,
            &quot;scalability_needs&quot;: &quot;To be determined&quot;,
            &quot;latency_requirements&quot;: &quot;To be determined&quot;
        }

        prompt = self.prompt_templates.format_template(
            &quot;model_deployment_guidance&quot;,
            model_type=self.current_analysis[&quot;model_type&quot;],
            performance_summary=json.dumps(performance_summary, indent=2),
            requirements=json.dumps(requirements, indent=2),
            constraints=json.dumps(constraints, indent=2)
        )

        response = self._send_request(prompt)
        return response

    def generate_comprehensive_report(self) -&gt; Dict[str, str]:
        &quot;&quot;&quot;Generate a comprehensive explanation report&quot;&quot;&quot;
        if not self.current_analysis:
            raise ValueError(&quot;No model analysis available. Load a model first.&quot;)

        report = {
            &quot;architecture_explanation&quot;: self.explain_architecture(),
            &quot;hyperparameter_explanation&quot;: self.explain_hyperparameters(),
            &quot;deployment_guidance&quot;: self.generate_deployment_guidance(),
            &quot;model_summary&quot;: self.current_analysis[&quot;summary&quot;],
            &quot;technical_details&quot;: json.dumps(self.current_analysis, indent=2)
        }

        return report

    def _format_layer_details(self, layers: List[Dict]) -&gt; str:
        &quot;&quot;&quot;Format layer details for prompt&quot;&quot;&quot;
        details = []
        for layer in layers:
            layer_str = f&quot;Layer {layer['index']}: {layer['name']} ({layer['class']})&quot;
            if layer.get('units'):
                layer_str += f&quot; - Units: {layer['units']}&quot;
            if layer.get('filters'):
                layer_str += f&quot; - Filters: {layer['filters']}&quot;
            if layer.get('kernel_size'):
                layer_str += f&quot; - Kernel Size: {layer['kernel_size']}&quot;
            if layer.get('trainable_params'):
                layer_str += f&quot; - Parameters: {layer['trainable_params']}&quot;
            details.append(layer_str)

        return '\n'.join(details)

    def _infer_task_type(self) -&gt; str:
        &quot;&quot;&quot;Infer the type of ML task based on model architecture&quot;&quot;&quot;
        if not self.current_analysis:
            return &quot;Unknown&quot;

        architecture = self.current_analysis[&quot;architecture&quot;]
        output_shape = architecture.get(&quot;output_shape&quot;)

        if output_shape:
            if isinstance(output_shape, (list, tuple)) and len(output_shape) &gt; 1:
                output_size = output_shape[-1] if output_shape[-1] is not None else 1
                if output_size == 1:
                    return &quot;Binary Classification or Regression&quot;
                elif output_size &gt; 1:
                    return &quot;Multi-class Classification&quot;

        return &quot;Unknown Task Type&quot;

    def _send_request(self, prompt: str, temperature: float = None) -&gt; str:
        &quot;&quot;&quot;Send request to OpenAI API&quot;&quot;&quot;
        temperature = temperature if temperature is not None else config.TEMPERATURE

        try:
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
                temperature=temperature,
                max_tokens=config.MAX_TOKENS
            )
            return response.choices[0].message.content
        except Exception as e:
            raise Exception(f&quot;Error calling OpenAI API: {str(e)}&quot;)
</code></pre>

<h3 id="835-report-generator">8.3.5 Report Generator</h3>
<p>Create <code>explainer/report_generator.py</code>:</p>
<pre class="codehilite"><code class="language-python">from jinja2 import Template
from pathlib import Path
import json
from datetime import datetime
from typing import Dict, Any, Optional

class ReportGenerator:
    &quot;&quot;&quot;Generate formatted reports from model explanations&quot;&quot;&quot;

    def __init__(self, template_dir: Optional[str] = None):
        self.template_dir = Path(template_dir) if template_dir else Path(__file__).parent / &quot;templates&quot;
        self.template_dir.mkdir(exist_ok=True)
        self._create_default_templates()

    def _create_default_templates(self):
        &quot;&quot;&quot;Create default report templates&quot;&quot;&quot;
        # HTML Report Template
        html_template = &quot;&quot;&quot;
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;ML Model Explanation Report&lt;/title&gt;
    &lt;style&gt;
        body { font-family: Arial, sans-serif; max-width: 1200px; margin: 0 auto; padding: 20px; }
        h1, h2, h3 { color: #2c3e50; }
        .section { margin-bottom: 30px; padding: 20px; border-left: 4px solid #3498db; background-color: #f8f9fa; }
        .technical-details { background-color: #f1f2f6; padding: 15px; border-radius: 5px; font-family: monospace; }
        .metadata { color: #7f8c8d; font-size: 0.9em; }
        table { width: 100%; border-collapse: collapse; margin: 10px 0; }
        th, td { padding: 10px; text-align: left; border-bottom: 1px solid #ddd; }
        th { background-color: #34495e; color: white; }
    &lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;h1&gt;ML Model Explanation Report&lt;/h1&gt;

    &lt;div class=&quot;metadata&quot;&gt;
        &lt;p&gt;Generated on: {{ timestamp }}&lt;/p&gt;
        &lt;p&gt;Model Path: {{ model_path }}&lt;/p&gt;
        &lt;p&gt;Framework: {{ framework }}&lt;/p&gt;
    &lt;/div&gt;

    &lt;div class=&quot;section&quot;&gt;
        &lt;h2&gt;Model Architecture&lt;/h2&gt;
        {{ architecture_explanation }}
    &lt;/div&gt;

    &lt;div class=&quot;section&quot;&gt;
        &lt;h2&gt;Hyperparameters&lt;/h2&gt;
        {{ hyperparameter_explanation }}
    &lt;/div&gt;

    &lt;div class=&quot;section&quot;&gt;
        &lt;h2&gt;Deployment Guidance&lt;/h2&gt;
        {{ deployment_guidance }}
    &lt;/div&gt;

    &lt;div class=&quot;section&quot;&gt;
        &lt;h2&gt;Technical Summary&lt;/h2&gt;
        &lt;div class=&quot;technical-details&quot;&gt;
            &lt;pre&gt;{{ model_summary }}&lt;/pre&gt;
        &lt;/div&gt;
    &lt;/div&gt;

    {% if technical_details %}
    &lt;div class=&quot;section&quot;&gt;
        &lt;h2&gt;Raw Analysis Data&lt;/h2&gt;
        &lt;details&gt;
            &lt;summary&gt;Click to expand technical details&lt;/summary&gt;
            &lt;div class=&quot;technical-details&quot;&gt;
                &lt;pre&gt;{{ technical_details }}&lt;/pre&gt;
            &lt;/div&gt;
        &lt;/details&gt;
    &lt;/div&gt;
    {% endif %}
&lt;/body&gt;
&lt;/html&gt;
        &quot;&quot;&quot;

        # Markdown Report Template
        markdown_template = &quot;&quot;&quot;
# ML Model Explanation Report

**Generated on:** {{ timestamp }}  
**Model Path:** {{ model_path }}  
**Framework:** {{ framework }}

## Model Architecture

{{ architecture_explanation }}

## Hyperparameters

{{ hyperparameter_explanation }}

## Deployment Guidance

{{ deployment_guidance }}

## Technical Summary
</code></pre>

<p>{{ model_summary }}</p>
<pre class="codehilite"><code>{% if technical_details %}
## Raw Analysis Data

&lt;details&gt;
&lt;summary&gt;Technical Details&lt;/summary&gt;

```json
{{ technical_details }}
</code></pre>

<p></details>
{% endif %}
        """</p>
<pre class="codehilite"><code>    # Save templates
    with open(self.template_dir / &quot;report.html&quot;, &quot;w&quot;) as f:
        f.write(html_template.strip())

    with open(self.template_dir / &quot;report.md&quot;, &quot;w&quot;) as f:
        f.write(markdown_template.strip())

def generate_html_report(self, explanations: Dict[str, Any], analysis: Dict[str, Any], output_path: str):
    &quot;&quot;&quot;Generate HTML report&quot;&quot;&quot;
    template_path = self.template_dir / &quot;report.html&quot;
    with open(template_path, &quot;r&quot;) as f:
        template = Template(f.read())

    report_data = {
        &quot;timestamp&quot;: datetime.now().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;),
        &quot;model_path&quot;: analysis.get(&quot;metadata&quot;, {}).get(&quot;model_path&quot;, &quot;Unknown&quot;),
        &quot;framework&quot;: analysis.get(&quot;framework&quot;, &quot;Unknown&quot;),
        &quot;architecture_explanation&quot;: self._format_for_html(explanations.get(&quot;architecture_explanation&quot;, &quot;&quot;)),
        &quot;hyperparameter_explanation&quot;: self._format_for_html(explanations.get(&quot;hyperparameter_explanation&quot;, &quot;&quot;)),
        &quot;deployment_guidance&quot;: self._format_for_html(explanations.get(&quot;deployment_guidance&quot;, &quot;&quot;)),
        &quot;model_summary&quot;: explanations.get(&quot;model_summary&quot;, &quot;&quot;),
        &quot;technical_details&quot;: json.dumps(analysis, indent=2)
    }

    html_content = template.render(**report_data)

    with open(output_path, &quot;w&quot;) as f:
        f.write(html_content)

    return output_path

def generate_markdown_report(self, explanations: Dict[str, Any], analysis: Dict[str, Any], output_path: str):
    &quot;&quot;&quot;Generate Markdown report&quot;&quot;&quot;
    template_path = self.template_dir / &quot;report.md&quot;
    with open(template_path, &quot;r&quot;) as f:
        template = Template(f.read())

    report_data = {
        &quot;timestamp&quot;: datetime.now().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;),
        &quot;model_path&quot;: analysis.get(&quot;metadata&quot;, {}).get(&quot;model_path&quot;, &quot;Unknown&quot;),
        &quot;framework&quot;: analysis.get(&quot;framework&quot;, &quot;Unknown&quot;),
        &quot;architecture_explanation&quot;: explanations.get(&quot;architecture_explanation&quot;, &quot;&quot;),
        &quot;hyperparameter_explanation&quot;: explanations.get(&quot;hyperparameter_explanation&quot;, &quot;&quot;),
        &quot;deployment_guidance&quot;: explanations.get(&quot;deployment_guidance&quot;, &quot;&quot;),
        &quot;model_summary&quot;: explanations.get(&quot;model_summary&quot;, &quot;&quot;),
        &quot;technical_details&quot;: json.dumps(analysis, indent=2)
    }

    markdown_content = template.render(**report_data)

    with open(output_path, &quot;w&quot;) as f:
        f.write(markdown_content)

    return output_path

def _format_for_html(self, text: str) -&gt; str:
    &quot;&quot;&quot;Format text for HTML display&quot;&quot;&quot;
    # Convert line breaks to HTML line breaks
    text = text.replace('\n', '&lt;br&gt;\n')

    # Convert **bold** to &lt;strong&gt;
    import re
    text = re.sub(r'\*\*(.*?)\*\*', r'&lt;strong&gt;\1&lt;/strong&gt;', text)

    # Convert *italic* to &lt;em&gt;
    text = re.sub(r'\*(.*?)\*', r'&lt;em&gt;\1&lt;/em&gt;', text)

    return text
</code></pre>

<pre class="codehilite"><code>## 8.4 Building the Command-Line Interface

Create `main.py`:

```python
import typer
from pathlib import Path
from typing import Optional
from rich.console import Console
from rich.panel import Panel
from rich.markdown import Markdown

import config
from explainer.core import MLModelExplainer
from explainer.report_generator import ReportGenerator

app = typer.Typer(help=&quot;ML Model Explainer - AI-powered model interpretation&quot;)
console = Console()

@app.command(&quot;explain&quot;)
def explain_model(
    model_path: Path = typer.Argument(..., help=&quot;Path to the model file&quot;),
    framework: Optional[str] = typer.Option(None, &quot;--framework&quot;, &quot;-f&quot;, help=&quot;ML framework (tensorflow, pytorch, sklearn)&quot;),
    output_dir: Optional[Path] = typer.Option(None, &quot;--output&quot;, &quot;-o&quot;, help=&quot;Output directory for reports&quot;),
    format: str = typer.Option(&quot;markdown&quot;, &quot;--format&quot;, help=&quot;Report format (html, markdown, json)&quot;),
    sections: str = typer.Option(&quot;all&quot;, &quot;--sections&quot;, help=&quot;Sections to include (all, architecture, hyperparams, deployment)&quot;)
):
    &quot;&quot;&quot;Explain a machine learning model comprehensively&quot;&quot;&quot;

    console.print(f&quot;[bold blue]Loading model:[/bold blue] {model_path}&quot;)

    try:
        # Initialize explainer
        explainer = MLModelExplainer()

        # Load and analyze model
        analysis = explainer.load_model(str(model_path), framework)

        console.print(f&quot;[green]✓ Model loaded successfully[/green]&quot;)
        console.print(f&quot;Framework: {analysis['framework']}&quot;)
        console.print(f&quot;Model Type: {analysis['model_type']}&quot;)
        console.print(f&quot;Total Parameters: {analysis.get('complexity', {}).get('total_parameters', 'Unknown')}&quot;)

        # Generate explanations based on requested sections
        explanations = {}

        if sections == &quot;all&quot; or &quot;architecture&quot; in sections:
            console.print(&quot;\n[bold blue]Generating architecture explanation...[/bold blue]&quot;)
            explanations[&quot;architecture_explanation&quot;] = explainer.explain_architecture()

        if sections == &quot;all&quot; or &quot;hyperparams&quot; in sections:
            console.print(&quot;[bold blue]Generating hyperparameter explanation...[/bold blue]&quot;)
            explanations[&quot;hyperparameter_explanation&quot;] = explainer.explain_hyperparameters()

        if sections == &quot;all&quot; or &quot;deployment&quot; in sections:
            console.print(&quot;[bold blue]Generating deployment guidance...[/bold blue]&quot;)
            explanations[&quot;deployment_guidance&quot;] = explainer.generate_deployment_guidance()

        # Add model summary
        explanations[&quot;model_summary&quot;] = analysis[&quot;summary&quot;]

        # Display explanations
        console.print(&quot;\n&quot; + &quot;=&quot;*80)

        if &quot;architecture_explanation&quot; in explanations:
            console.print(Panel(Markdown(explanations[&quot;architecture_explanation&quot;]), 
                              title=&quot;[bold]Architecture Explanation[/bold]&quot;, border_style=&quot;blue&quot;))

        if &quot;hyperparameter_explanation&quot; in explanations:
            console.print(Panel(Markdown(explanations[&quot;hyperparameter_explanation&quot;]), 
                              title=&quot;[bold]Hyperparameter Explanation[/bold]&quot;, border_style=&quot;green&quot;))

        if &quot;deployment_guidance&quot; in explanations:
            console.print(Panel(Markdown(explanations[&quot;deployment_guidance&quot;]), 
                              title=&quot;[bold]Deployment Guidance[/bold]&quot;, border_style=&quot;yellow&quot;))

        # Generate report if output directory specified
        if output_dir:
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)

            report_generator = ReportGenerator()
            model_name = model_path.stem

            if format == &quot;html&quot;:
                report_path = output_dir / f&quot;{model_name}_explanation.html&quot;
                report_generator.generate_html_report(explanations, analysis, str(report_path))
                console.print(f&quot;[green]HTML report saved to:[/green] {report_path}&quot;)

            elif format == &quot;markdown&quot;:
                report_path = output_dir / f&quot;{model_name}_explanation.md&quot;
                report_generator.generate_markdown_report(explanations, analysis, str(report_path))
                console.print(f&quot;[green]Markdown report saved to:[/green] {report_path}&quot;)

            elif format == &quot;json&quot;:
                import json
                report_path = output_dir / f&quot;{model_name}_analysis.json&quot;
                full_report = {
                    &quot;analysis&quot;: analysis,
                    &quot;explanations&quot;: explanations
                }
                with open(report_path, 'w') as f:
                    json.dump(full_report, f, indent=2, default=str)
                console.print(f&quot;[green]JSON analysis saved to:[/green] {report_path}&quot;)

    except Exception as e:
        console.print(f&quot;[bold red]Error:[/bold red] {str(e)}&quot;)
        raise typer.Exit(code=1)

@app.command(&quot;predict&quot;)
def explain_prediction(
    model_path: Path = typer.Argument(..., help=&quot;Path to the model file&quot;),
    input_data: str = typer.Argument(..., help=&quot;Input data (JSON format or file path)&quot;),
    framework: Optional[str] = typer.Option(None, &quot;--framework&quot;, &quot;-f&quot;, help=&quot;ML framework&quot;),
    output_file: Optional[Path] = typer.Option(None, &quot;--output&quot;, &quot;-o&quot;, help=&quot;Output file for explanation&quot;)
):
    &quot;&quot;&quot;Explain a specific model prediction&quot;&quot;&quot;

    console.print(f&quot;[bold blue]Loading model for prediction explanation:[/bold blue] {model_path}&quot;)

    try:
        # Initialize explainer
        explainer = MLModelExplainer()

        # Load model
        explainer.load_model(str(model_path), framework)

        # Parse input data
        import json
        import numpy as np

        if Path(input_data).exists():
            # Load from file
            with open(input_data, 'r') as f:
                if input_data.endswith('.json'):
                    sample_input = json.load(f)
                else:
                    # Assume it's a text file with comma-separated values
                    sample_input = [float(x.strip()) for x in f.read().split(',')]
        else:
            # Parse as JSON string
            sample_input = json.loads(input_data)

        # Convert to numpy array
        sample_input = np.array(sample_input)

        console.print(f&quot;[green]✓ Input data loaded, shape: {sample_input.shape}[/green]&quot;)

        # Generate prediction explanation
        console.print(&quot;[bold blue]Generating prediction explanation...[/bold blue]&quot;)
        explanation = explainer.explain_prediction(sample_input)

        # Display explanation
        console.print(Panel(Markdown(explanation), 
                          title=&quot;[bold]Prediction Explanation[/bold]&quot;, border_style=&quot;cyan&quot;))

        # Save to file if requested
        if output_file:
            with open(output_file, 'w') as f:
                f.write(explanation)
            console.print(f&quot;[green]Explanation saved to:[/green] {output_file}&quot;)

    except Exception as e:
        console.print(f&quot;[bold red]Error:[/bold red] {str(e)}&quot;)
        raise typer.Exit(code=1)

@app.command(&quot;compare&quot;)
def compare_models(
    model_paths: str = typer.Argument(..., help=&quot;Comma-separated list of model paths&quot;),
    output_dir: Optional[Path] = typer.Option(None, &quot;--output&quot;, &quot;-o&quot;, help=&quot;Output directory for comparison report&quot;)
):
    &quot;&quot;&quot;Compare multiple models&quot;&quot;&quot;

    paths = [p.strip() for p in model_paths.split(',')]
    console.print(f&quot;[bold blue]Comparing {len(paths)} models...[/bold blue]&quot;)

    try:
        model_analyses = []

        for i, model_path in enumerate(paths):
            console.print(f&quot;[blue]Analyzing model {i+1}:[/blue] {model_path}&quot;)

            explainer = MLModelExplainer()
            analysis = explainer.load_model(model_path)
            model_analyses.append({
                &quot;path&quot;: model_path,
                &quot;analysis&quot;: analysis,
                &quot;explainer&quot;: explainer
            })

        # Generate comparison (simplified for this example)
        console.print(&quot;\n[bold green]Model Comparison Summary:[/bold green]&quot;)

        for i, model_data in enumerate(model_analyses):
            analysis = model_data[&quot;analysis&quot;]
            console.print(f&quot;\n[bold]Model {i+1}:[/bold] {Path(model_data['path']).name}&quot;)
            console.print(f&quot;  Framework: {analysis['framework']}&quot;)
            console.print(f&quot;  Type: {analysis['model_type']}&quot;)
            console.print(f&quot;  Parameters: {analysis.get('complexity', {}).get('total_parameters', 'Unknown')}&quot;)
            console.print(f&quot;  Complexity: {analysis.get('complexity', {}).get('complexity_category', 'Unknown')}&quot;)

        if output_dir:
            # Generate detailed comparison report
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)

            comparison_data = {
                &quot;models&quot;: model_analyses,
                &quot;timestamp&quot;: datetime.now().isoformat()
            }

            with open(output_dir / &quot;model_comparison.json&quot;, 'w') as f:
                json.dump(comparison_data, f, indent=2, default=str)

            console.print(f&quot;[green]Comparison data saved to:[/green] {output_dir / 'model_comparison.json'}&quot;)

    except Exception as e:
        console.print(f&quot;[bold red]Error:[/bold red] {str(e)}&quot;)
        raise typer.Exit(code=1)

@app.command(&quot;info&quot;)
def show_info():
    &quot;&quot;&quot;Show information about the ML Model Explainer&quot;&quot;&quot;
    console.print(Panel.fit(&quot;&quot;&quot;
[bold blue]ML Model Explainer[/bold blue]

AI-powered tool for understanding and explaining machine learning models.

[bold green]Supported Frameworks:[/bold green]
• TensorFlow/Keras
• PyTorch  
• Scikit-learn

[bold green]Available Commands:[/bold green]
• explain    - Comprehensive model explanation
• predict    - Explain specific predictions
• compare    - Compare multiple models
• info       - Show this information

[bold green]Configuration:[/bold green]
• Model: {model}
• Supported formats: .h5, .pt, .pth, .pkl, .joblib
    &quot;&quot;&quot;.format(model=config.DEFAULT_MODEL), 
    title=&quot;ML Model Explainer&quot;, border_style=&quot;blue&quot;))

if __name__ == &quot;__main__&quot;:
    app()
</code></pre>

<h2 id="chapter8_section4">8.5 Usage Examples and Practical Applications</h2>
<h3 id="851-explaining-a-keras-model">8.5.1 Explaining a Keras Model</h3>
<pre class="codehilite"><code class="language-bash"># Explain a complete Keras model
python main.py explain ./models/image_classifier.h5 --framework tensorflow --output ./reports --format html

# Explain only architecture and hyperparameters
python main.py explain ./models/text_classifier.h5 --sections architecture,hyperparams --format markdown
</code></pre>

<h3 id="852-understanding-model-predictions">8.5.2 Understanding Model Predictions</h3>
<pre class="codehilite"><code class="language-bash"># Explain a prediction with JSON input
python main.py predict ./models/sentiment_model.h5 '[0.1, 0.5, 0.3, 0.8]' --output prediction_explanation.md

# Explain prediction with input from file
python main.py predict ./models/price_predictor.pkl ./data/sample_input.json --framework sklearn
</code></pre>

<h3 id="853-comparing-different-models">8.5.3 Comparing Different Models</h3>
<pre class="codehilite"><code class="language-bash"># Compare multiple models
python main.py compare &quot;./models/model_v1.h5,./models/model_v2.h5,./models/model_v3.h5&quot; --output ./comparison_reports
</code></pre>

<h2 id="chapter8_section5">8.6 Best Practices and Limitations</h2>
<h3 id="861-best-practices">8.6.1 Best Practices</h3>
<ol>
<li>
<p><strong>Model Documentation</strong>: Always maintain detailed documentation about your model's training process, data preprocessing, and intended use cases.</p>
</li>
<li>
<p><strong>Explanation Validation</strong>: Cross-check LLM explanations with your domain knowledge and established ML principles.</p>
</li>
<li>
<p><strong>Context Awareness</strong>: Provide business context when generating explanations for stakeholders.</p>
</li>
<li>
<p><strong>Regular Updates</strong>: Keep explanations current as models are retrained or updated.</p>
</li>
<li>
<p><strong>Security Considerations</strong>: Be cautious about exposing sensitive model details in explanations.</p>
</li>
</ol>
<h3 id="862-current-limitations">8.6.2 Current Limitations</h3>
<ol>
<li>
<p><strong>Framework Coverage</strong>: Our current implementation has basic support for major frameworks but may need extension for specialized architectures.</p>
</li>
<li>
<p><strong>Feature Importance</strong>: True feature importance analysis requires additional tools like SHAP or LIME integration.</p>
</li>
<li>
<p><strong>Explanation Accuracy</strong>: LLM explanations should be validated by domain experts.</p>
</li>
<li>
<p><strong>Complex Architectures</strong>: Very complex or custom architectures may require specialized analysis approaches.</p>
</li>
</ol>
<h2 id="chapter8_section6">8.7 Future Enhancements</h2>
<p>Potential improvements for our ML Model Explainer include:</p>
<ol>
<li><strong>Advanced Interpretability</strong>: Integration with SHAP, LIME, and other interpretability tools</li>
<li><strong>Visualization Generation</strong>: Automatic creation of architecture diagrams and performance plots</li>
<li><strong>Interactive Dashboards</strong>: Web-based interface for exploring model explanations</li>
<li><strong>Custom Model Support</strong>: Extensible architecture for specialized model types</li>
<li><strong>Deployment Monitoring</strong>: Integration with model monitoring and drift detection tools</li>
</ol>
<h2 id="chapter8_section7">8.8 Conclusion</h2>
<p>In this chapter, we've built a sophisticated ML Model Explainer that demonstrates how LLMs can be used to make complex machine learning models more interpretable and accessible. By combining traditional model analysis techniques with the natural language generation capabilities of LLMs, we've created a tool that can bridge the gap between technical complexity and human understanding.</p>
<p>The key insights from this project include:</p>
<ol>
<li><strong>Prompt Specialization</strong>: Domain-specific prompts yield much better results than generic explanations</li>
<li><strong>Structured Analysis</strong>: Breaking down model analysis into clear components (architecture, hyperparameters, etc.) enables more focused explanations</li>
<li><strong>Multi-Format Output</strong>: Different stakeholders need different types of explanations and reports</li>
<li><strong>Framework Abstraction</strong>: Using a base analyzer class allows for easy extension to new ML frameworks</li>
</ol>
<p>In the next chapter, we'll build on these concepts to create an ML Training Debugger and Optimizer that can help identify and resolve common training issues.</p></div><div class="navigation"><a href="#chapter7" class="prev-chapter">Previous Chapter</a><a href="#chapter9" class="next-chapter">Next Chapter</a></div><div class="chapter" id="chapter-content-9"><h1 id="chapter9">Chapter 9: Hands-on Project 3: ML Training Debugger and Optimizer</h1>
<p>Building on our ML Model Explainer from Chapter 8, we now tackle one of the most frustrating aspects of machine learning: debugging failed or suboptimal training runs. This chapter focuses on creating an intelligent system that can analyze training logs, identify issues, and provide actionable optimization recommendations.</p>
<h2 id="chapter9_section1">9.1 The Training Debug Challenge</h2>
<h3 id="911-common-training-problems">9.1.1 Common Training Problems</h3>
<p>Machine learning training often fails in subtle ways:
- <strong>Convergence Issues</strong>: Models that won't converge or converge too slowly
- <strong>Overfitting/Underfitting</strong>: Poor generalization or insufficient learning
- <strong>Hyperparameter Problems</strong>: Learning rates too high/low, poor batch sizes
- <strong>Technical Issues</strong>: Memory problems, gradient explosions, vanishing gradients</p>
<h3 id="912-why-llms-help">9.1.2 Why LLMs Help</h3>
<p>Traditional debugging relies on manual interpretation of metrics and charts. LLMs can:
- Recognize patterns across thousands of training runs
- Correlate multiple metrics simultaneously
- Provide contextual explanations in natural language
- Suggest specific, actionable fixes</p>
<h2 id="chapter9_section2">9.2 Project Architecture</h2>
<p>Our ML Training Debugger will have three core components:
1. <strong>Log Parser</strong>: Extract metrics from TensorFlow, PyTorch, or custom logs
2. <strong>Pattern Analyzer</strong>: Detect common training issues automatically
3. <strong>LLM Explainer</strong>: Generate insights and optimization recommendations</p>
<h2 id="chapter9_section3">9.3 Core Implementation</h2>
<h3 id="931-training-log-parser">9.3.1 Training Log Parser</h3>
<pre class="codehilite"><code class="language-python"># training_debugger.py
import pandas as pd
import re
import json
from pathlib import Path

class TrainingLogParser:
    def __init__(self, log_path):
        self.log_path = Path(log_path)
        self.data = None

    def parse_logs(self):
        &quot;&quot;&quot;Parse training logs from various formats&quot;&quot;&quot;
        if self.log_path.suffix == '.csv':
            return self._parse_csv()
        elif self.log_path.suffix == '.json':
            return self._parse_json()
        else:
            return self._parse_text_logs()

    def _parse_csv(self):
        &quot;&quot;&quot;Parse CSV training history&quot;&quot;&quot;
        self.data = pd.read_csv(self.log_path)
        self.data.columns = [col.lower().replace(' ', '_') for col in self.data.columns]
        return self.data

    def _parse_text_logs(self):
        &quot;&quot;&quot;Extract metrics from text logs using regex&quot;&quot;&quot;
        with open(self.log_path, 'r') as f:
            content = f.read()

        # Common patterns for training logs
        patterns = {
            'epoch': r'Epoch (\d+)',
            'loss': r'loss:\s*([0-9.]+)',
            'accuracy': r'accuracy:\s*([0-9.]+)',
            'val_loss': r'val_loss:\s*([0-9.]+)',
            'val_accuracy': r'val_accuracy:\s*([0-9.]+)',
            'lr': r'lr:\s*([0-9.e-]+)'
        }

        data_dict = {key: [] for key in patterns.keys()}

        lines = content.split('\n')
        for line in lines:
            epoch_match = re.search(patterns['epoch'], line)
            if epoch_match:
                current_epoch = int(epoch_match.group(1))

                # Extract all metrics for this epoch
                epoch_data = {'epoch': current_epoch}
                for metric, pattern in patterns.items():
                    if metric == 'epoch':
                        continue
                    match = re.search(pattern, line)
                    if match:
                        epoch_data[metric] = float(match.group(1))

                # Add to data_dict
                for key in data_dict.keys():
                    data_dict[key].append(epoch_data.get(key, None))

        self.data = pd.DataFrame(data_dict).dropna()
        return self.data
</code></pre>

<h3 id="932-issue-detection-system">9.3.2 Issue Detection System</h3>
<pre class="codehilite"><code class="language-python">class TrainingIssueDetector:
    def __init__(self, data):
        self.data = data
        self.issues = []

    def detect_all_issues(self):
        &quot;&quot;&quot;Run all issue detection methods&quot;&quot;&quot;
        self.detect_overfitting()
        self.detect_learning_rate_issues()
        self.detect_convergence_problems()
        self.detect_loss_explosions()
        return self.issues

    def detect_overfitting(self):
        &quot;&quot;&quot;Detect train/validation performance gaps&quot;&quot;&quot;
        if 'loss' in self.data.columns and 'val_loss' in self.data.columns:
            train_loss = self.data['loss'].dropna()
            val_loss = self.data['val_loss'].dropna()

            if len(train_loss) &gt; 10 and len(val_loss) &gt; 10:
                # Check recent performance gap
                recent_train = train_loss.tail(5).mean()
                recent_val = val_loss.tail(5).mean()
                gap = (recent_val - recent_train) / recent_train

                if gap &gt; 0.15:  # 15% gap threshold
                    self.issues.append({
                        'type': 'overfitting',
                        'severity': 'high' if gap &gt; 0.3 else 'medium',
                        'description': f'Validation loss {gap:.1%} higher than training loss',
                        'metrics': {
                            'train_loss': recent_train,
                            'val_loss': recent_val,
                            'gap': gap
                        }
                    })

    def detect_learning_rate_issues(self):
        &quot;&quot;&quot;Detect learning rate problems&quot;&quot;&quot;
        if 'lr' in self.data.columns:
            current_lr = self.data['lr'].iloc[-1]

            if current_lr &gt; 0.1:
                self.issues.append({
                    'type': 'learning_rate_too_high',
                    'severity': 'high',
                    'description': f'Learning rate {current_lr} may be too high',
                    'metrics': {'current_lr': current_lr}
                })
            elif current_lr &lt; 1e-6:
                self.issues.append({
                    'type': 'learning_rate_too_low',
                    'severity': 'medium',
                    'description': f'Learning rate {current_lr} may be too low',
                    'metrics': {'current_lr': current_lr}
                })

    def detect_convergence_problems(self):
        &quot;&quot;&quot;Detect poor convergence&quot;&quot;&quot;
        if 'loss' in self.data.columns:
            loss_series = self.data['loss'].dropna()

            if len(loss_series) &gt; 20:
                # Check if loss is still decreasing in recent epochs
                recent_improvement = (loss_series.iloc[-10:].iloc[0] - loss_series.iloc[-1]) / loss_series.iloc[-10:].iloc[0]

                if recent_improvement &lt; 0.01:  # Less than 1% improvement
                    self.issues.append({
                        'type': 'poor_convergence',
                        'severity': 'medium',
                        'description': 'Loss not improving in recent epochs',
                        'metrics': {'recent_improvement': recent_improvement}
                    })

    def detect_loss_explosions(self):
        &quot;&quot;&quot;Detect sudden loss increases&quot;&quot;&quot;
        if 'loss' in self.data.columns:
            loss_series = self.data['loss'].dropna()

            # Check for sudden jumps (&gt;5x increase)
            pct_changes = loss_series.pct_change().fillna(0)
            explosions = pct_changes &gt; 5.0

            if explosions.any():
                explosion_idx = explosions.idxmax()
                self.issues.append({
                    'type': 'loss_explosion',
                    'severity': 'critical',
                    'description': 'Sudden loss explosion detected',
                    'metrics': {
                        'explosion_epoch': explosion_idx,
                        'change_factor': pct_changes.loc[explosion_idx]
                    }
                })
</code></pre>

<h3 id="933-llm-powered-analysis-engine">9.3.3 LLM-Powered Analysis Engine</h3>
<pre class="codehilite"><code class="language-python">import openai
import json

class TrainingAnalyzer:
    def __init__(self, api_key):
        openai.api_key = api_key

    def analyze_training_run(self, data, issues, hyperparams=None):
        &quot;&quot;&quot;Generate comprehensive training analysis&quot;&quot;&quot;

        # Prepare training summary
        summary = self._create_training_summary(data, issues)

        prompt = f&quot;&quot;&quot;You are an expert ML engineer analyzing a training run. 

Training Summary:
{summary}

Detected Issues:
{json.dumps(issues, indent=2)}

Current Hyperparameters:
{json.dumps(hyperparams or {}, indent=2)}

Please provide:
1. **Overall Assessment**: How is this training run performing?
2. **Root Cause Analysis**: What's causing the main issues?
3. **Specific Recommendations**: What should be changed next?
4. **Priority Actions**: What to fix first?

Be specific and actionable in your recommendations.&quot;&quot;&quot;

        response = openai.ChatCompletion.create(
            model=&quot;gpt-4&quot;,
            messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
            temperature=0.3,
            max_tokens=2000
        )

        return response.choices[0].message.content

    def suggest_hyperparameter_optimization(self, current_params, issues, performance_data):
        &quot;&quot;&quot;Suggest specific hyperparameter changes&quot;&quot;&quot;

        prompt = f&quot;&quot;&quot;You are a hyperparameter optimization expert. Based on the training issues and performance, suggest specific parameter adjustments.

Current Hyperparameters:
{json.dumps(current_params, indent=2)}

Training Issues Detected:
{json.dumps([issue['type'] for issue in issues])}

Performance Data:
- Final Loss: {performance_data.get('final_loss', 'Unknown')}
- Best Validation: {performance_data.get('best_val', 'Unknown')}
- Training Epochs: {performance_data.get('epochs', 'Unknown')}

Provide specific recommendations for:
1. **Learning Rate**: Exact values to try
2. **Batch Size**: Optimal batch size
3. **Architecture Changes**: If needed
4. **Regularization**: Dropout, weight decay adjustments
5. **Training Schedule**: Learning rate schedules, early stopping

Focus on the top 2-3 most impactful changes.&quot;&quot;&quot;

        response = openai.ChatCompletion.create(
            model=&quot;gpt-4&quot;, 
            messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
            temperature=0.2,
            max_tokens=1500
        )

        return response.choices[0].message.content

    def _create_training_summary(self, data, issues):
        &quot;&quot;&quot;Create a concise training summary&quot;&quot;&quot;
        summary = []

        if 'epoch' in data.columns:
            summary.append(f&quot;Total epochs: {data['epoch'].max()}&quot;)

        if 'loss' in data.columns:
            loss_series = data['loss'].dropna()
            summary.append(f&quot;Final loss: {loss_series.iloc[-1]:.4f}&quot;)
            summary.append(f&quot;Best loss: {loss_series.min():.4f}&quot;)

        if 'val_loss' in data.columns:
            val_loss = data['val_loss'].dropna()
            summary.append(f&quot;Final val_loss: {val_loss.iloc[-1]:.4f}&quot;)
            summary.append(f&quot;Best val_loss: {val_loss.min():.4f}&quot;)

        if 'accuracy' in data.columns:
            acc = data['accuracy'].dropna()
            summary.append(f&quot;Final accuracy: {acc.iloc[-1]:.3f}&quot;)

        summary.append(f&quot;Issues detected: {len(issues)}&quot;)

        return &quot;\n&quot;.join(summary)
</code></pre>

<h3 id="934-main-training-debugger-interface">9.3.4 Main Training Debugger Interface</h3>
<pre class="codehilite"><code class="language-python">class MLTrainingDebugger:
    def __init__(self, api_key):
        self.analyzer = TrainingAnalyzer(api_key)

    def debug_training_run(self, log_path, hyperparams=None):
        &quot;&quot;&quot;Complete debugging workflow&quot;&quot;&quot;

        # 1. Parse logs
        print(&quot;📊 Parsing training logs...&quot;)
        parser = TrainingLogParser(log_path)
        data = parser.parse_logs()
        print(f&quot;✅ Loaded {len(data)} training records&quot;)

        # 2. Detect issues
        print(&quot;🔍 Detecting training issues...&quot;)
        detector = TrainingIssueDetector(data)
        issues = detector.detect_all_issues()
        print(f&quot;⚠️  Found {len(issues)} potential issues&quot;)

        # 3. Generate analysis
        print(&quot;🤖 Generating AI analysis...&quot;)
        analysis = self.analyzer.analyze_training_run(data, issues, hyperparams)

        # 4. Get optimization suggestions
        performance_data = {
            'final_loss': data['loss'].iloc[-1] if 'loss' in data.columns else None,
            'best_val': data['val_loss'].min() if 'val_loss' in data.columns else None,
            'epochs': data['epoch'].max() if 'epoch' in data.columns else len(data)
        }

        optimization = self.analyzer.suggest_hyperparameter_optimization(
            hyperparams or {}, issues, performance_data
        )

        return {
            'data': data,
            'issues': issues,
            'analysis': analysis,
            'optimization_suggestions': optimization
        }
</code></pre>

<h2 id="chapter9_section4">9.4 Usage Examples</h2>
<h3 id="941-basic-usage">9.4.1 Basic Usage</h3>
<pre class="codehilite"><code class="language-python"># Initialize debugger
debugger = MLTrainingDebugger(api_key=&quot;your-openai-key&quot;)

# Analyze a training run
result = debugger.debug_training_run(
    log_path=&quot;training_history.csv&quot;,
    hyperparams={
        &quot;learning_rate&quot;: 0.001,
        &quot;batch_size&quot;: 32,
        &quot;optimizer&quot;: &quot;adam&quot;
    }
)

# Print analysis
print(&quot;=== TRAINING ANALYSIS ===&quot;)
print(result['analysis'])
print(&quot;\n=== OPTIMIZATION SUGGESTIONS ===&quot;)  
print(result['optimization_suggestions'])
</code></pre>

<h3 id="942-cli-interface">9.4.2 CLI Interface</h3>
<pre class="codehilite"><code class="language-python">import typer
from rich.console import Console

app = typer.Typer()
console = Console()

@app.command()
def debug(
    log_file: str = typer.Argument(..., help=&quot;Path to training log file&quot;),
    api_key: str = typer.Option(..., envvar=&quot;OPENAI_API_KEY&quot;, help=&quot;OpenAI API key&quot;),
    output: str = typer.Option(None, help=&quot;Save report to file&quot;)
):
    &quot;&quot;&quot;Debug a machine learning training run&quot;&quot;&quot;

    console.print(f&quot;[blue]Analyzing training log:[/blue] {log_file}&quot;)

    try:
        debugger = MLTrainingDebugger(api_key)
        result = debugger.debug_training_run(log_file)

        # Display results
        console.print(&quot;\n[bold green]🎯 Training Analysis[/bold green]&quot;)
        console.print(result['analysis'])

        console.print(&quot;\n[bold blue]⚡ Optimization Suggestions[/bold blue]&quot;)
        console.print(result['optimization_suggestions'])

        # Save if requested
        if output:
            with open(output, 'w') as f:
                f.write(f&quot;# Training Debug Report\n\n&quot;)
                f.write(f&quot;## Analysis\n{result['analysis']}\n\n&quot;)
                f.write(f&quot;## Optimization\n{result['optimization_suggestions']}&quot;)
            console.print(f&quot;[green]Report saved to {output}[/green]&quot;)

    except Exception as e:
        console.print(f&quot;[red]Error: {e}[/red]&quot;)

if __name__ == &quot;__main__&quot;:
    app()
</code></pre>

<h2 id="chapter9_section5">9.5 Advanced Features</h2>
<h3 id="951-experiment-comparison">9.5.1 Experiment Comparison</h3>
<pre class="codehilite"><code class="language-python">def compare_experiments(self, experiment_logs):
    &quot;&quot;&quot;Compare multiple training experiments&quot;&quot;&quot;

    experiments = []
    for log_path in experiment_logs:
        parser = TrainingLogParser(log_path)
        data = parser.parse_logs()
        detector = TrainingIssueDetector(data)
        issues = detector.detect_all_issues()

        experiments.append({
            'name': Path(log_path).stem,
            'final_loss': data['loss'].iloc[-1] if 'loss' in data.columns else None,
            'best_val': data['val_loss'].min() if 'val_loss' in data.columns else None,
            'issues': len(issues),
            'converged': len([i for i in issues if i['type'] == 'poor_convergence']) == 0
        })

    # Generate comparison analysis
    comparison_prompt = f&quot;&quot;&quot;Compare these ML experiments and identify the best performing approach:

{json.dumps(experiments, indent=2)}

Provide:
1. **Best Experiment**: Which performed best and why?
2. **Key Patterns**: What patterns lead to success?
3. **Recommendations**: What to try next?&quot;&quot;&quot;

    # ...rest of comparison logic
</code></pre>

<h3 id="952-visualization-integration">9.5.2 Visualization Integration</h3>
<pre class="codehilite"><code class="language-python">import matplotlib.pyplot as plt

def generate_training_plots(data, output_dir=&quot;plots&quot;):
    &quot;&quot;&quot;Generate training visualization plots&quot;&quot;&quot;

    fig, axes = plt.subplots(2, 2, figsize=(12, 8))

    # Loss curves
    if 'loss' in data.columns:
        axes[0,0].plot(data['epoch'], data['loss'], label='Training Loss')
        if 'val_loss' in data.columns:
            axes[0,0].plot(data['epoch'], data['val_loss'], label='Validation Loss')
        axes[0,0].set_title('Loss Curves')
        axes[0,0].legend()

    # Accuracy curves  
    if 'accuracy' in data.columns:
        axes[0,1].plot(data['epoch'], data['accuracy'], label='Training Accuracy')
        if 'val_accuracy' in data.columns:
            axes[0,1].plot(data['epoch'], data['val_accuracy'], label='Validation Accuracy')
        axes[0,1].set_title('Accuracy Curves')
        axes[0,1].legend()

    # Learning rate schedule
    if 'lr' in data.columns:
        axes[1,0].plot(data['epoch'], data['lr'])
        axes[1,0].set_title('Learning Rate Schedule')
        axes[1,0].set_yscale('log')

    plt.tight_layout()
    plt.savefig(f&quot;{output_dir}/training_analysis.png&quot;, dpi=300, bbox_inches='tight')
    plt.close()
</code></pre>

<h2 id="chapter9_section6">9.6 Best Practices and Limitations</h2>
<h3 id="961-best-practices">9.6.1 Best Practices</h3>
<ol>
<li><strong>Log Everything</strong>: Include learning rates, gradient norms, and custom metrics</li>
<li><strong>Consistent Formatting</strong>: Use standard logging formats for easier parsing</li>
<li><strong>Domain Context</strong>: Provide model architecture and dataset information</li>
<li><strong>Multiple Runs</strong>: Compare across experiments for better insights</li>
<li><strong>Human Validation</strong>: Always verify LLM suggestions with domain knowledge</li>
</ol>
<h3 id="962-current-limitations">9.6.2 Current Limitations</h3>
<ol>
<li><strong>Pattern Recognition</strong>: LLMs may miss domain-specific issues</li>
<li><strong>Causation vs Correlation</strong>: May suggest fixes that don't address root causes</li>
<li><strong>Framework Specifics</strong>: Different frameworks have unique debugging needs</li>
<li><strong>Resource Costs</strong>: Extensive analysis can be expensive with API calls</li>
</ol>
<h2 id="chapter9_section7">9.7 Conclusion</h2>
<p>The ML Training Debugger demonstrates how LLMs can transform the traditionally manual and expertise-heavy process of training diagnosis. By combining automatic issue detection with intelligent analysis, we can:</p>
<ul>
<li><strong>Reduce Debug Time</strong>: Quickly identify common training problems</li>
<li><strong>Improve Training Success</strong>: Get specific, actionable recommendations  </li>
<li><strong>Learn Faster</strong>: Understand why certain approaches work or fail</li>
<li><strong>Scale Expertise</strong>: Make advanced debugging accessible to more developers</li>
</ul>
<p>The key insight is that LLMs excel at pattern recognition across training metrics when provided with the right context and structured prompts. This approach can significantly accelerate the iterative process of ML model development.</p>
<p>In Part 2 of this book, we'll shift focus to architectural considerations for deploying LLM-powered systems at enterprise scale.</p></div><div class="navigation"><a href="#chapter8" class="prev-chapter">Previous Chapter</a><div></div></div>
</div><div class="index-section">
    <h1>Index</h1>
    
    <div class="index-container">
        <div class="index-entry">
            <span class="main-entry">API Keys</span> <span class="page-numbers">12, 18</span>
        </div>
        
        <div class="index-entry">
            <span class="main-entry">Bias in AI</span> <span class="page-numbers">5, 110</span>
        </div>
        
        <div class="index-entry">
            <span class="main-entry">Chain-of-Thought (CoT) Prompting</span> <span class="page-numbers">72-75</span>
            <div class="sub-entry">example implementation <span class="page-numbers">76</span></div>
            <div class="sub-entry">use cases <span class="page-numbers">73</span></div>
        </div>
        
        <div class="index-entry">
            <span class="main-entry">Code Generation</span> <span class="page-numbers">45-50</span>
            <div class="sub-entry">functions <span class="page-numbers">46</span></div>
            <div class="sub-entry">classes <span class="page-numbers">48</span></div>
        </div>
        
        <div class="index-entry">
            <span class="main-entry">Cost Optimization</span> <span class="page-numbers">32, 94-96</span>
            <div class="sub-entry">token counting <span class="page-numbers">94</span></div>
            <div class="sub-entry">caching <span class="page-numbers">95</span></div>
        </div>
        
        <div class="index-entry">
            <span class="main-entry">Debugging</span> <span class="page-numbers">52, 88</span>
            <div class="sub-entry">LLM applications <span class="page-numbers">88</span></div>
            <div class="sub-entry">ML models <span class="page-numbers">145-150</span></div>
        </div>
        
        <div class="index-entry">
            <span class="main-entry">Documentation Generation</span> <span class="page-numbers">51</span>
        </div>
        
        <div class="index-entry">
            <span class="main-entry">Error Handling</span> <span class="page-numbers">28, 78</span>
        </div>
        
        <div class="index-entry">
            <span class="main-entry">Few-shot Prompting</span> <span class="page-numbers">38</span>
        </div>
        
        <div class="index-entry">
            <span class="main-entry">Hallucinations</span> <span class="page-numbers">24, 80</span>
            <div class="sub-entry">mitigation strategies <span class="page-numbers">81</span></div>
        </div>
        
        <div class="index-entry">
            <span class="main-entry">LLM APIs</span> <span class="page-numbers">22-30</span>
            <div class="sub-entry">OpenAI <span class="page-numbers">23</span></div>
            <div class="sub-entry">Google Gemini <span class="page-numbers">25</span></div>
            <div class="sub-entry">Anthropic Claude <span class="page-numbers">26</span></div>
        </div>
        
        <div class="index-entry">
            <span class="main-entry">Model Explainability</span> <span class="page-numbers">120-135</span>
        </div>
        
        <div class="index-entry">
            <span class="main-entry">Performance Profiling</span> <span class="page-numbers">90-92</span>
        </div>
        
        <div class="index-entry">
            <span class="main-entry">Persona-Based Prompting</span> <span class="page-numbers">77</span>
        </div>
        
        <div class="index-entry">
            <span class="main-entry">Prompt</span> <span class="page-numbers">1-5</span>
            <div class="sub-entry">anatomy of <span class="page-numbers">34</span></div>
            <div class="sub-entry">chaining <span class="page-numbers">78</span></div>
            <div class="sub-entry">libraries <span class="page-numbers">87</span></div>
            <div class="sub-entry">patterns <span class="page-numbers">45-60</span></div>
            <div class="sub-entry">version control <span class="page-numbers">16</span></div>
        </div>
        
        <div class="index-entry">
            <span class="main-entry">Self-Correction</span> <span class="page-numbers">74</span>
        </div>
        
        <div class="index-entry">
            <span class="main-entry">Temperature</span> <span class="page-numbers">76</span>
        </div>
        
        <div class="index-entry">
            <span class="main-entry">Testing</span> <span class="page-numbers">40, 92-93</span>
            <div class="sub-entry">prompt effectiveness <span class="page-numbers">40</span></div>
            <div class="sub-entry">LLM applications <span class="page-numbers">92</span></div>
        </div>
        
        <div class="index-entry">
            <span class="main-entry">Token Management</span> <span class="page-numbers">30-32</span>
        </div>
        
        <div class="index-entry">
            <span class="main-entry">Zero-shot Prompting</span> <span class="page-numbers">37</span>
        </div>
    </div>
</div></body>
</html>
